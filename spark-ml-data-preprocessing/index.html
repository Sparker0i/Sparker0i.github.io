<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. It is more important when it comes to big data."><title>Cool Spark ML - Part 2: Preprocessing of Data</title><link rel=canonical href=https://blog.sparker0i.me/spark-ml-data-preprocessing/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="Cool Spark ML - Part 2: Preprocessing of Data"><meta property='og:description' content="Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. It is more important when it comes to big data."><meta property='og:url' content='https://blog.sparker0i.me/spark-ml-data-preprocessing/'><meta property='og:site_name' content="Sparker0i's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Spark'><meta property='article:tag' content='Machine Learning'><meta property='article:tag' content='Big Data'><meta property='article:published_time' content='2020-06-06T08:02:00+00:00'><meta property='article:modified_time' content='2024-04-14T17:20:05+00:00'><meta property='og:image' content='https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47.png'><meta name=twitter:title content="Cool Spark ML - Part 2: Preprocessing of Data"><meta name=twitter:description content="Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. It is more important when it comes to big data."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47.png'><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-B20G7JM0X8"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B20G7JM0X8")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/sparker0i_hu_693dc16bb266ace6.jpg width=300 height=301 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ðŸ˜Ž</span></figure><div class=site-meta><h1 class=site-name><a href=/>Sparker0i's Blog</a></h1><h2 class=site-description>I blog about coding, tech, AI, cricket and other personal hobbies too.</h2></div></header><ol class=menu-social><li><a href=https://github.com/Sparker0i target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/sparker0i target=_blank title=LinkedIn rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-linkedin"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 10-4 0"/><path d="M3 7a4 4 0 014-4h10a4 4 0 014 4v10a4 4 0 01-4 4H7a4 4 0 01-4-4z"/></svg></a></li><li><a href=https://steamcommunity.com/id/Sparker0i target=_blank title=Steam rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-steam"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16.5 5a4.5 4.5.0 11-.653 8.953L11.5 16.962V17a3 3 0 01-2.824 3H8.5a3 3 0 01-2.94-2.402L3 16.5V13l3.51 1.755a2.989 2.989.0 012.834-.635l2.727-3.818A4.5 4.5.0 0116.5 5z"/><circle cx="16.5" cy="9.5" r="1" fill="currentColor"/></svg></a></li><li><a href=https://x.com/Sparker0i target=_blank title=Twitter rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-x"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4l11.733 16H20L8.267 4z"/><path d="M4 20l6.768-6.768m2.46-2.46L20 4"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#types-of-preprocessing-in-spark>Types of Preprocessing in Spark</a></li><li><a href=#numeric-data>Numeric Data</a><ol><li><a href=#normalize>Normalize</a></li><li><a href=#standardize>Standardize</a></li><li><a href=#bucketize>Bucketize</a></li></ol></li><li><a href=#text>Text</a><ol><li><a href=#tokenize>Tokenize</a></li><li><a href=#term-frequency-inverse-document-frequency-tf-idf>Term Frequency-Inverse Document Frequency (TF-IDF)</a></li></ol></li><li><a href=#conclusion>CONCLUSION</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/spark-ml-data-preprocessing/><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_hu_780771378215d952.png srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_hu_780771378215d952.png 800w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_hu_b646334932f54184.png 1600w" width=800 height=420 loading=lazy alt="Featured image of post Cool Spark ML - Part 2: Preprocessing of Data"></a></div><div class=article-details><header class=article-category><a href=/categories/spark/ style=background-color:#e78b34;color:#000>Spark</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/spark-ml-data-preprocessing/>Cool Spark ML - Part 2: Preprocessing of Data</a></h2><h3 class=article-subtitle>Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. It is more important when it comes to big data.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 06, 2020</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>11 minute read</time></div></footer></div></header><section class=article-content><p><em>Note: This is the second article of the series: Cool Spark ML. The other parts can be found below:</em></p><ul><li><a class=link href=https://blog.sparker0i.me/spark-machine-learning-knn/ target=_blank rel=noopener>Part 1: K Nearest Neighbours</a></li><li>Part 2: Preprocessing of Data (current)</li></ul><p>People who have been performing Machine Learning for quite a long time know that Data Preprocessing is a key step before running any algorithms on the data. In a majority of datasets, you might always find null, or incomplete values. The data would also be inconsistent across columns, which directly affects algorithms using distance measures.</p><p>This is where Data Preprocessing comes in. It is a crucial step which involves cleaning and organizing the data to make it suitable for building models. In other words, if you don&rsquo;t perform Preprocessing, your models may not be accurate.</p><p>While there are quite a lot of articles online about Data Preprocessing in Python, there aren&rsquo;t a lot of them in Spark, or even Scala. In this post, I will be dealing with the ways you can perform Data Preprocessing in Spark on Scala.</p><p><em>PS. You might be asking why I&rsquo;m dealing with this now when I have actually written KNN in Spark before. The truth is, KNN isn&rsquo;t officially supported inside Spark ML module. What I wrote in the previous article was a top-to-bottom version of KNN performed using Spark. You can also say that I&rsquo;m doing a complete reset of this series ðŸ˜…</em></p><p>Just like always, the codes for all posts in this series will be available on <a class=link href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost" target=_blank rel=noopener>my GitHub repo</a>.</p><h2 id=types-of-preprocessing-in-spark>Types of Preprocessing in Spark</h2><p>There are two types of preprocessing:</p><ul><li>Numeric Data</li><li>Text Data</li></ul><h2 id=numeric-data>Numeric Data</h2><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e.png width=744 height=291 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e_hu_a661649faac3f5a4.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e_hu_12db0115b3c706bb.png 1024w" loading=lazy class=gallery-image data-flex-grow=255 data-flex-basis=613px></p><p>There are three ways you can preprocess numeric data in Spark:</p><ul><li>Normalize</li><li>Standardize</li><li>Bucketize</li></ul><p>To illustrate Normalize and Standardize, I&rsquo;ll be using some Scala magic which will generate my points as a Vector. Each vector represents a point in a 3-Dimensional Space.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>points</span> <span class=k>=</span> <span class=k>for</span> <span class=o>(</span><span class=n>i</span> <span class=k>&lt;-</span> <span class=mi>1</span> <span class=n>to</span> <span class=mi>1000</span><span class=o>)</span> <span class=k>yield</span> <span class=o>(</span><span class=n>i</span><span class=o>,</span> <span class=nc>Vectors</span><span class=o>.</span><span class=n>dense</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=nc>Array</span><span class=o>(</span>
</span></span><span class=line><span class=cl>        <span class=o>(</span><span class=n>math</span><span class=o>.</span><span class=n>random</span> <span class=o>*</span> <span class=o>(</span><span class=mi>10</span> <span class=o>-</span> <span class=mi>1</span><span class=o>))</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=mf>1.0</span><span class=o>,</span>
</span></span><span class=line><span class=cl>        <span class=o>(</span><span class=n>math</span><span class=o>.</span><span class=n>random</span> <span class=o>*</span> <span class=o>(</span><span class=mi>10000</span> <span class=o>-</span> <span class=mi>1000</span><span class=o>))</span> <span class=o>+</span> <span class=mf>1000.0</span><span class=o>,</span>
</span></span><span class=line><span class=cl>        <span class=n>math</span><span class=o>.</span><span class=n>random</span> <span class=o>*</span> <span class=n>i</span>
</span></span><span class=line><span class=cl>    <span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>featuresDf</span> <span class=k>=</span> <span class=n>points</span><span class=o>.</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;id&#34;</span><span class=o>,</span> <span class=s>&#34;features&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Doing the above results in the following <code>DataFrame</code>:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3.png width=577 height=508 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3_hu_e44d68fae859d626.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3_hu_6122e23b16f7d93.png 1024w" loading=lazy class=gallery-image data-flex-grow=113 data-flex-basis=272px></p><p>Each element inside Features column represents a point in a 3-D space.</p><h3 id=normalize>Normalize</h3><p>Normalization is the process of mapping numeric data from their original range into a range of 0 to 1. The lowest value of the original range gets value of 0, and the highest gets the value 1. All the other values in the original range will fall between these two.</p><p>This is important because there may be multiple attributes with different ranges. <em>E.g. Salary values may range between 3 and 8+ digit numbers, years in company will be between 1- and 2-digit numbers.</em> The reason we want to normalize those attributes in a <code>[0,1]</code> range is so that when algorithms that use distance as a measure, they don&rsquo;t weigh some attributes like salary more heavily than others.</p><p>The formula to convert values in an un-normalized column to a normalized form is given by:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c.png width=220 height=88 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c_hu_5c8f5f3c3406b51f.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c_hu_bf30bc0d48a5c834.png 1024w" loading=lazy class=gallery-image data-flex-grow=250 data-flex-basis=600px></p><p>Normalization Formula</p><p>Where:</p><ul><li><code>x</code> is the value inside a column to be normalized,</li><li><code>x(new)</code> is the normalized value,</li><li><code>x(min)</code> is the minimum value of that column, and</li><li><code>x(max)</code> is the maximum value of that column</li></ul><p>Working on the <code>featuresDf</code> created above, we will import <code>MinMaxScaler</code> from the <code>org.apache.spark.ml.feature</code> package. We now have to create an instance of the <code>MinMaxScaler</code>. It will take two parameters: Input column name, and an Output Column name. This object will transform the contents of the input column vectors into a scaled version, and save it into the output column.</p><p>In our case, we will be using our <code>features</code> column inside <code>featuresDf</code> as the input column, and our output column will be named <code>sFeatures</code>. We create the instance in this manner:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>featureScaler</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>MinMaxScaler</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;features&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;sfeatures&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Next, we have to <code>fit</code> the data present in our <code>featuresDf</code> inside this <code>featureScaler</code> and later <code>transform</code> to create the scaled data. This is done using the code below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>scaledDf</span> <span class=k>=</span> <span class=n>featureScaler</span><span class=o>.</span><span class=n>fit</span><span class=o>(</span><span class=n>featuresDf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>featuresDf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Transforming original values into normalized ones</p><p>Now, if we have a look at our transformed data:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03.png width=1170 height=499 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03_hu_47ff0effba0c7922.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03_hu_f07a2e91d95b1737.png 1024w" loading=lazy class=gallery-image data-flex-grow=234 data-flex-basis=562px></p><p>Normalized <code>DataFrame</code></p><p>You can then use this new <code>sFeatures</code> to calculate distances among points.</p><h3 id=standardize>Standardize</h3><p>Now, we may have data whose values can be mapped to a bell-shaped curve, or normally distributed but maybe not exactly. With standardization, we map our data and transform it, which has a variance of 1 and/or a mean value of 0. This is done because some machine learning algorithms, like SVM, work better this way.</p><p>Thus, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. The formula to convert values in a non-standardized column to a standardized form is given by:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea.png width=214 height=87 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea_hu_e9428b7044f5744d.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea_hu_76b46cd750961242.png 1024w" loading=lazy class=gallery-image data-flex-grow=245 data-flex-basis=590px></p><p>Standardization Formula</p><p>Where:</p><ul><li><code>x</code> is the value to be standardized</li><li><code>x(new)</code> is the standardized value</li><li><code>Î¼</code> is the mean of the column</li><li><code>Ïƒ</code> is the standard deviation of the column.</li></ul><p>Again, we will be using the <code>featuresDf</code> created above. We will import <code>StandardScaler</code> from the <code>org.apache.spark.ml.feature</code> package. Just like <code>MinMaxScaler</code>, an instance of <code>StandardScaler</code> will require an input column and an output column. In our case, we will still continue with <code>features</code> and <code>sFeatures</code>. We will then <code>fit</code> the data inside the scaler and later <code>transform</code> the data. I&rsquo;ve combined both these steps into a single code snippet:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>featureStandardScaler</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>StandardScaler</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;features&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;sfeatures&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setWithStd</span><span class=o>(</span><span class=kc>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setWithMean</span><span class=o>(</span><span class=kc>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>standardizedDf</span> <span class=k>=</span> <span class=n>featureStandardScaler</span><span class=o>.</span><span class=n>fit</span><span class=o>(</span><span class=n>featuresDf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>featuresDf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Now if we have a look at our transformed data:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff.png width=1168 height=495 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff_hu_4df59158823a7545.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff_hu_86937542ea1a8ac7.png 1024w" loading=lazy class=gallery-image data-flex-grow=235 data-flex-basis=566px></p><p>Standardized Numeric Data</p><p>Wait, weren&rsquo;t the values supposed to be scaled within the range of <code>[-1, 1]</code>? Well, that&rsquo;s the surprise associated with the <code>StandardScaler</code>. It uses the unbiased sample standard deviation instead of the population standard deviation.</p><p>In other words, while the standard deviation will be 1 (or very close to 1), the mean may not be necessarily 0. To scale your data in a way that the range of numbers is between <code>[-1,1]</code> and the standard deviation is 1 and mean 0, you will have to follow <a class=link href="https://stackoverflow.com/a/51755387/2451763?ref=localhost" target=_blank rel=noopener>this accepted StackOverflow answer</a>. Even otherwise with this process, the data has been standardized.</p><h3 id=bucketize>Bucketize</h3><p>Bucketization is done when we have to organize continuous ranges of data into different buckets. <code>Bucketizer</code> allows us to group data based on boundaries, so a list of boundaries has to be provided. I will call it <code>splits</code> with the domain of all buckets when added looks like: <code>{(-âˆž, -500.0) â‹ƒ [-500.0, -100.0) â‹ƒ [-100.0, -10.0) â‹ƒ [-10.0, 0.0) â‹ƒ [0.0, 10.0) â‹ƒ [10.0, 100.0) â‹ƒ [100.0, 500.0) â‹ƒ [500.0, âˆž)}</code>.</p><p>Then I&rsquo;ll generate 1000 random points that fall in the range of <code>[-10000.0, 10000.0]</code> and save it in a <code>DataFrame</code> with column name as <code>features</code>. This is done using the below code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>splits</span> <span class=k>=</span> <span class=nc>Array</span><span class=o>(</span><span class=nc>Float</span><span class=o>.</span><span class=nc>NegativeInfinity</span><span class=o>,</span> <span class=o>-</span><span class=mf>500.0</span><span class=o>,</span> <span class=o>-</span><span class=mf>100.0</span><span class=o>,</span> <span class=o>-</span><span class=mf>10.0</span><span class=o>,</span> <span class=mf>0.0</span><span class=o>,</span> <span class=mf>10.0</span><span class=o>,</span> <span class=mf>100.0</span><span class=o>,</span> <span class=mf>500.0</span><span class=o>,</span> <span class=nc>Float</span><span class=o>.</span><span class=nc>PositiveInfinity</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>bucketData</span> <span class=k>=</span> <span class=o>(</span><span class=k>for</span> <span class=o>(</span><span class=n>i</span> <span class=k>&lt;-</span> <span class=mi>0</span> <span class=n>to</span> <span class=mi>10000</span><span class=o>)</span> <span class=k>yield</span> <span class=n>math</span><span class=o>.</span><span class=n>random</span> <span class=o>*</span> <span class=mf>10000.0</span> <span class=o>*</span> <span class=o>(</span><span class=k>if</span> <span class=o>(</span><span class=n>math</span><span class=o>.</span><span class=n>random</span> <span class=o>&lt;</span> <span class=mf>0.5</span><span class=o>)</span> <span class=o>-</span><span class=mi>1</span> <span class=k>else</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>bucketDf</span> <span class=k>=</span> <span class=n>bucketData</span><span class=o>.</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;features&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Now, our <code>Bucketizer</code> needs three inputs: the splits, input column name, and output column name. Then I&rsquo;ll <code>transform</code> that data which would then give me the element and which bucket it belongs to:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>bucketizer</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>Bucketizer</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setSplits</span><span class=o>(</span><span class=n>splits</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;features&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;bfeatures&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>bucketedDf</span> <span class=k>=</span> <span class=n>bucketizer</span><span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>bucketDf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Notice that I didn&rsquo;t have to do a <code>fit</code> operation before doing a <code>transform</code>. This is because Bucketizing is fairly simple and you only need to find which bucket a number belongs to. Thus, there are no operations like scaling which happened in the other 2 sections, and hence you don&rsquo;t need to <code>fit</code> your data. Now if we have a look at the created <code>DataFrame</code>:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f.png width=298 height=510 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f_hu_f998aa47506f8847.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f_hu_796a64ef7381ef30.png 1024w" loading=lazy class=gallery-image data-flex-grow=58 data-flex-basis=140px></p><p>Bucketized DataFrame</p><p>Now you might also want to know how many numbers are there in a particular bucket. So, I will do a <code>groupBy</code> on <code>bFeatures</code> column and retrieve the count of occurrences. The following code does that and displays my generated data:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60.png width=160 height=231 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60_hu_ec9c2eab740d4bf2.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60_hu_7949b11a7e5f43a8.png 1024w" loading=lazy class=gallery-image data-flex-grow=69 data-flex-basis=166px></p><p>Fairly easy, isn&rsquo;t it?</p><h2 id=text>Text</h2><p>There are two ways in which you can preprocess text-based data in Spark:</p><ul><li>Tokenize</li><li>TF-IDF</li></ul><p>To illustrate both of them, I will be using <code>sentencesDf</code> created using this code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>sentencesDf</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=o>(</span><span class=mi>1</span><span class=o>,</span> <span class=s>&#34;This is an introduction to Spark ML&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>    <span class=o>(</span><span class=mi>2</span><span class=o>,</span> <span class=s>&#34;MLLib includes libraries for classification and regression&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>    <span class=o>(</span><span class=mi>3</span><span class=o>,</span> <span class=s>&#34;It also contains supporting tools for pipelines&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;id&#34;</span><span class=o>,</span> <span class=s>&#34;sentence&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=tokenize>Tokenize</h3><p>In tokenization, you map your string containing a sentence into a set of tokens, or words. As an Example, the sentence <em>&ldquo;This is an introduction to Spark ML&rdquo;</em> can be mapped into a list of 7 words - <code>{This, is, an, introduction, to, Spark, ML}</code>.</p><p>We will first import <code>Tokenizer</code> from the <code>org.apache.spark.ml.feature</code> package. Now an instance of this will need two parameters - input column and output column. Our input will be <code>sentence</code> and the output will be <code>words</code>, because that is what the <code>Tokenizer</code> will produce. Then we will apply <code>transform</code> on the sentences above.</p><p>Now, just like bucketing, we are not <code>fit</code>ting any data here. <code>Tokenizer</code> already knows its job - Split strings into the separate words. The above process is illustrated in the code below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>sentenceToken</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>Tokenizer</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;sentence&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;words&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>sentenceTokenizedDf</span> <span class=k>=</span> <span class=n>sentenceToken</span><span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>sentencesDf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Now, if we have a look at our data:</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671.png width=1195 height=164 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671_hu_92a327a34d5cae14.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671_hu_8584960d546cd83a.png 1024w" loading=lazy class=gallery-image data-flex-grow=728 data-flex-basis=1748px></p><p>The <code>words</code> column contains lists of words that have been broken up in the ways you would expect a regular expression pattern matching to break up a sentence into words - based on white space, punctuation, etc.</p><p>Easy, isn&rsquo;t it?</p><h3 id=term-frequency-inverse-document-frequency-tf-idf>Term Frequency-Inverse Document Frequency (TF-IDF)</h3><p>Here we map text from a single, typically long string, to a vector, indicating the frequency of each word in a text relative to a group of texts such as a corpus. This transformation is widely used in text classification.</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804.png width=820 height=433 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804_hu_23ab18036869a4d7.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804_hu_e2319ff2c75b6f56.png 1024w" loading=lazy class=gallery-image data-flex-grow=189 data-flex-basis=454px></p><p>TF-IDF captures the intuition that infrequently used words are more useful for distinguishing categories of text than frequently used words. Considering the above figure as an example, <em>Normalizing</em> appears only once, <em>to</em> appears twice and so on. Like this, we go through all the documents in our corpus, which is nothing but a collection of documents. Then we count up how often a term appears across all of the documents. In this example <em>normalizing</em> is a very rare word. Whereas other words like <em>maps, data</em> and <em>to</em> show up more frequently. We use these two sets of counts and feed those two into the term frequency-inverse document frequency calculation. And that gives us our TF-IDF measures.</p><p>I will use the same <code>sentenceTokenizedDf</code> created above for this exercise as well. Just like other processes mentioned above, we will need to import a few things from <code>org.apache.spark.ml.feature</code> package - <code>HashingTF</code> (for hashing Term Frequency), <code>IDF</code> (for Inverse Document Frequency), <code>Tokenizer</code>.</p><p>First, I will create a <code>HashingTF</code> instance - which takes an input column (<code>words</code>), an output column (<code>rawFeatures</code>) and the number of features to keep track of (<code>20</code>) as the parameters. Now we apply our <code>transform</code>ation on this and get a new <code>DataFrame</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>hashingTF</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>HashingTF</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;words&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;rawFeatures&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setNumFeatures</span><span class=o>(</span><span class=mi>20</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>sentenceHashingFunctionTermFrequencyDf</span> <span class=k>=</span> <span class=n>hashingTF</span><span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>sentenceTokenizedDf</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>sentenceHashingFunctionTermFrequencyDf</span><span class=o>.</span><span class=n>show</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p>Now if we have a look at our data, it has added an extra column which is of <code>Vector</code> type. It has mapped each word to an index, so for example, <em>this</em> maps to 1, <em>is</em> maps to 4, <em>an</em> -> 5, and so on.</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8.png width=1688 height=154 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8_hu_dc939f94e76812db.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8_hu_c57048eb454f7550.png 1024w" loading=lazy class=gallery-image data-flex-grow=1096 data-flex-basis=2630px></p><p>Now we&rsquo;re going to scale the <code>rawFeatures</code> vector values and we&rsquo;re going to scale them based on how often the words appear in the entire collection of sentences. To do this we&rsquo;re going to create an <code>IDF</code> instance. Again, we have to specify an input column (<code>rawFeatures</code>) and an output column (<code>idfFeatures</code>) as parameters.</p><p>Let&rsquo;s use the term frequency data we just calculated to <code>fit</code> the inverse document frequency model. And to do that I&rsquo;m going to create an <code>idfModel</code>, and we&rsquo;re going to call the <code>idf</code> object I just created, and I&rsquo;m going to fit it using our term frequency data. Then we apply the IDF <code>transform</code>ation to create a new <code>DataFrame</code> that has both the term frequency and the inverse document frequency transformations applied.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>idf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>IDF</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setInputCol</span><span class=o>(</span><span class=s>&#34;rawFeatures&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>setOutputCol</span><span class=o>(</span><span class=s>&#34;idfFeatures&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>idfModel</span> <span class=k>=</span> <span class=n>idf</span><span class=o>.</span><span class=n>fit</span><span class=o>(</span><span class=n>sentenceHashingFunctionTermFrequencyDf</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>tfIdfDf</span> <span class=k>=</span> <span class=n>idfModel</span><span class=o>.</span><span class=n>transform</span><span class=o>(</span><span class=n>sentenceHashingFunctionTermFrequencyDf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Now if we have a look at our data (I&rsquo;m selecting only the <code>rawFeatures</code> and <code>idfFeatures</code> columns to fit in the screen):</p><p><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a.png width=1871 height=159 srcset="/spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a_hu_ab78ff15b2aad54.png 480w, /spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a_hu_b8b1bede67a8fffe.png 1024w" loading=lazy class=gallery-image data-flex-grow=1176 data-flex-basis=2824px></p><p>Now we have a new column which contains the inverse document frequency features. These are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences.</p><h2 id=conclusion>CONCLUSION</h2><p>Preprocessing is indeed a tough challenge where you will have to know what kinds of data you might get and what kinds of processing you want to apply on your data. If not done properly, your machine learning models might not be of much use.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/spark/>Spark</a>
<a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/big-data/>Big Data</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Apr 14, 2024 17:20 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/spark-machine-learning-knn/><div class=article-image><img src=/spark-machine-learning-knn/661c0843dbf837e5981954cc.b42c5cf4806d76e548c7b451de5a66a5_hu_e0a020b163962c1b.png width=250 height=150 loading=lazy alt="Featured image of post Cool Spark ML: K Nearest Neighbors" data-key=spark-machine-learning-knn data-hash="md5-tCxc9IBtduVIx7RR3lpmpQ=="></div><div class=article-details><h2 class=article-title>Cool Spark ML: K Nearest Neighbors</h2></div></a></article><article class=has-image><a href=/spark-3-native-gpu-integration/><div class=article-image><img src=/spark-3-native-gpu-integration/661c08375610fb416dbc1887.6528655ab75cec0475df3bd3338927c9_hu_eac0a47f3dce3e98.png width=250 height=150 loading=lazy alt="Featured image of post Spark 3.0 adds native GPU integration: Why that matters?" data-key=spark-3-native-gpu-integration data-hash="md5-ZShlWrdc7AR13zvTM4knyQ=="></div><div class=article-details><h2 class=article-title>Spark 3.0 adds native GPU integration: Why that matters?</h2></div></a></article><article class=has-image><a href=/run-spark-3-applications-on-gpu/><div class=article-image><img src=/run-spark-3-applications-on-gpu/661c08139bb70e9dac2bfee1.16684aaa4058fbc4677edb3921d175ce_hu_e75e4405205a36d0.png width=250 height=150 loading=lazy alt="Featured image of post How to run Spark 3.0 applications on your GPU" data-key=run-spark-3-applications-on-gpu data-hash="md5-FmhKqkBY+8Rnfts5IdF1zg=="></div><div class=article-details><h2 class=article-title>How to run Spark 3.0 applications on your GPU</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Sparker0i/Sparker0i.github.io data-repo-id=R_kgDOPDIt_w data-category=Announcements data-category-id=DIC_kwDOPDIt_84CsLyO data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"preferred_color_scheme":"preferred_color_scheme")}})()</script><footer class=site-footer><section class=copyright>&copy;
2017 -
2025 Sparker0i's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>