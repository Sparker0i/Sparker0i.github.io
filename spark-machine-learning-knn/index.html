<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Learn how you can write a KNN algorithm from scratch and modify it for use with larger datasets in Spark"><title>Cool Spark ML: K Nearest Neighbors</title><link rel=canonical href=https://blog.sparker0i.me/spark-machine-learning-knn/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="Cool Spark ML: K Nearest Neighbors"><meta property='og:description' content="Learn how you can write a KNN algorithm from scratch and modify it for use with larger datasets in Spark"><meta property='og:url' content='https://blog.sparker0i.me/spark-machine-learning-knn/'><meta property='og:site_name' content="Sparker0i's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Big Data'><meta property='article:tag' content='Machine Learning'><meta property='article:tag' content='Scala'><meta property='article:tag' content='Spark'><meta property='article:published_time' content='2020-04-19T05:41:03+00:00'><meta property='article:modified_time' content='2024-04-14T17:43:28+00:00'><meta property='og:image' content='https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc.png'><meta name=twitter:title content="Cool Spark ML: K Nearest Neighbors"><meta name=twitter:description content="Learn how you can write a KNN algorithm from scratch and modify it for use with larger datasets in Spark"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc.png'><link rel="shortcut icon" href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-B20G7JM0X8"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B20G7JM0X8")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/sparker0i_hu_693dc16bb266ace6.jpg width=300 height=301 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ðŸ˜Ž</span></figure><div class=site-meta><h1 class=site-name><a href=/>Sparker0i's Blog</a></h1><h2 class=site-description>I blog about coding, tech, AI, cricket and other personal hobbies too.</h2></div></header><ol class=menu-social><li><a href=https://github.com/Sparker0i target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/sparker0i target=_blank title=LinkedIn rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-linkedin"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 10-4 0"/><path d="M3 7a4 4 0 014-4h10a4 4 0 014 4v10a4 4 0 01-4 4H7a4 4 0 01-4-4z"/></svg></a></li><li><a href=https://steamcommunity.com/id/Sparker0i target=_blank title=Steam rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-steam"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16.5 5a4.5 4.5.0 11-.653 8.953L11.5 16.962V17a3 3 0 01-2.824 3H8.5a3 3 0 01-2.94-2.402L3 16.5V13l3.51 1.755a2.989 2.989.0 012.834-.635l2.727-3.818A4.5 4.5.0 0116.5 5z"/><circle cx="16.5" cy="9.5" r="1" fill="currentColor"/></svg></a></li><li><a href=https://x.com/Sparker0i target=_blank title=Twitter rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-x"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4l11.733 16H20L8.267 4z"/><path d="M4 20l6.768-6.768m2.46-2.46L20 4"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#intro-to-spark-ml>Intro to Spark ML</a></li><li><a href=#why-spark-ml>Why Spark ML?</a></li><li><a href=#knn-k-nearest-neighbors>KNN: K-Nearest Neighbors</a></li><li><a href=#problem-definition>Problem Definition</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#knn-steps>KNN Steps</a></li><li><a href=#step-1-calculate-euclidean-distance>Step 1: Calculate Euclidean Distance</a></li><li><a href=#step-2-get-nearest-neighbors>Step 2: Get Nearest Neighbors</a></li><li><a href=#step-3-make-predictions>Step 3: Make Predictions</a></li><li><a href=#apply-the-above-concepts-to-iris-dataset>Apply the above concepts to Iris Dataset</a></li><li><a href=#conclusion>CONCLUSION</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/spark-machine-learning-knn/><img src=/spark-machine-learning-knn/661c0843dbf837e5981954cc_hu_1d5d6bf15cfe45f1.png srcset="/spark-machine-learning-knn/661c0843dbf837e5981954cc_hu_1d5d6bf15cfe45f1.png 800w, /spark-machine-learning-knn/661c0843dbf837e5981954cc_hu_685e38c1ec4447d4.png 1600w" width=800 height=420 loading=lazy alt="Featured image of post Cool Spark ML: K Nearest Neighbors"></a></div><div class=article-details><header class=article-category><a href=/categories/spark/ style=background-color:#e78b34;color:#000>Spark</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/spark-machine-learning-knn/>Cool Spark ML: K Nearest Neighbors</a></h2><h3 class=article-subtitle>Learn how you can write a KNN algorithm from scratch and modify it for use with larger datasets in Spark</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Apr 19, 2020</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div></footer></div></header><section class=article-content><p><em>Note: This article is the first of the Series: Cool Spark ML. Other parts are coming soon.</em></p><p>I had taken up a few machine learning courses in my college throughout 2018. Most of the problems there were solved using Python and the necessary libraries - NumPy, Pandas, Scikit-Learn and Matplotlib. With my daily work at IBM now requiring me to use Scala and Spark, I decided to use my free time during the lockdown to try out Spark ML.</p><p><em>Note: All the codes in the Cool Spark ML Series will be available on</em> <a class=link href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost" target=_blank rel=noopener><em>my GitHub repo</em></a></p><h3 id=intro-to-spark-ml>Intro to Spark ML</h3><p>As the name suggests, Spark ML is the Machine Learning library consisting of common Machine learning algorithms - classification, regression, clustering etc.</p><h3 id=why-spark-ml>Why Spark ML?</h3><p>Pandas - a Python library - wonâ€™t work every time. It is a single machine tool, so it&rsquo;s constrained by the machine&rsquo;s limits. Moreover, pandas doesnâ€™t have any parallelism built in, which means it uses only one CPU core. You may hit a dead-end on datasets of the size of a few gigabytes. Pandas won&rsquo;t help if you want to work on very big datasets.</p><p>We are now in the Big Data era, where gigabytes of data are generated every few seconds. Such datasets will require powerful systems to run even the basic machine learning algorithms. The cost of getting such a powerful system will be huge, as well as the costs to scale them up. With distributed computers, such calculations can be sent to multiple low-end machines, which prevents the cost of getting a single high-end machine.</p><p>This is where Spark kicks in. Spark has the concept of <code>DataFrame</code> (now deprecated in favor of Datasets), which behaves very similar to how a Pandas <code>DataFrame</code> would do, including having very similar APIs too. The advantage of using Spark <code>DataFrame</code> is that it was designed from ground-up to support Big Data. Spark can also distribute such <code>DataFrame</code>s across multiple machines and collect the calculated results.</p><h3 id=knn-k-nearest-neighbors>KNN: K-Nearest Neighbors</h3><p>The process in KNN is pretty simple. You load your entire dataset first, each of which will have input columns and one output column. This is then split into a training set and a testing set. You then use your training set to train your model, and then use the testing set to predict the output column value by testing it against the model. You then compare the actual and the predicted target values and calculate the accuracy of your model.</p><h3 id=problem-definition>Problem Definition</h3><p>We are going to train a model to predict the famous <a class=link href="http://archive.ics.uci.edu/ml/datasets/iris?ref=localhost" target=_blank rel=noopener>Iris dataset</a>. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.</p><p>It is a multiclass classification problem. The number of observations for each class is the same. The dataset is small in size with only 150 rows with 4 input variables and 1 output variable.</p><p>The 4 features are described as follows:</p><ol><li>Sepal-Length, in cm</li><li>Sepal-Width, in cm</li><li>Petal-Length, in cm</li><li>Petal-Width, in cm</li></ol><h3 id=prerequisites>Prerequisites</h3><ol><li>Create a Scala project in IntelliJ IDEA based on SBT</li><li>Select Scala version 2.11.12</li><li>Include <code>spark-core</code>, <code>spark-sql</code> and <code>spark-ml</code> 2.4.5 as library dependencies in your <code>build.sbt</code></li></ol><h3 id=knn-steps>KNN Steps</h3><p>In this blog post, I will be developing KNN algorithm from scratch. The process to perform KNN can be broken down into 3 easy steps:</p><ol><li>Calculate Euclidean Distance</li><li>Get Nearest Neighbors</li><li>Make Predictions</li></ol><h3 id=step-1-calculate-euclidean-distance>Step 1: Calculate Euclidean Distance</h3><p>The first step will be to calculate the distance between two rows in a Dataset. Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line.</p><p>Euclidean Distance is calculated as the square root of the sum of the squared differences between the two vectors, as given in the image below:</p><p><a class=link href="https://www.codecogs.com/eqnedit.php?latex=%5Cinline&amp;space%3B%5Cbg_white=&amp;space%3B%7B%5Ccolor%7BRed%7D=&amp;space%3B%24%24dist_%7Bx_1%2Cx_2%7D=&amp;space%3B=&amp;space%3B%5Csqrt%7B%5Csum_%7Bi=0%7D%5E%7BN%7D&amp;space%3B%28%7Bx_1_i=&amp;space%3B-=&amp;space%3Bx_2_i%7D%29%5E2%7D.%24%24%7D=&amp;ref=localhost" target=_blank rel=noopener><img src="https://latex.codecogs.com/gif.latex?%5Cinline&amp;space;%5Cbg_white&amp;space;%7B%5Ccolor%7BRed%7D&amp;space;$$dist_%7Bx_1,x_2%7D&amp;space;=&amp;space;%5Csqrt%7B%5Csum_%7Bi=0%7D%5E%7BN%7D&amp;space;%28%7Bx_1_i&amp;space;-&amp;space;x_2_i%7D%29%5E2%7D.$$%7D" loading=lazy></a></p><p>Where <code>x1</code> is the first row of data, <code>x2</code> is the second row of data, and <code>i</code> is a specific index for a column as we sum across all columns. Smaller the value, more similar will be the two rows.</p><p>Since we will be reading our data and transforming it using Spark, to compute distances between two <code>Row</code>s in a <code>DataFrame</code>, we write the function below in Scala:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=k>def</span> <span class=nf>computeEuclideanDistance</span><span class=p>(</span><span class=n>row1</span><span class=p>:</span> <span class=n>Row</span><span class=p>,</span> <span class=n>row2</span><span class=p>:</span> <span class=n>Row</span><span class=p>):</span> <span class=n>Double</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>var</span> <span class=n>distance</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;-</span> <span class=mi>0</span> <span class=n>until</span> <span class=n>row1</span><span class=o>.</span><span class=n>length</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>distance</span> <span class=o>+=</span> <span class=n>math</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>row1</span><span class=o>.</span><span class=n>getDouble</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=o>-</span> <span class=n>row2</span><span class=o>.</span><span class=n>getDouble</span><span class=p>(</span><span class=n>i</span><span class=p>),</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>distance</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>You can see that the function assumes that the last column in each row is an output value which is ignored from the distance calculation.</p><h3 id=step-2-get-nearest-neighbors>Step 2: Get Nearest Neighbors</h3><p>Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our distance measure. To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above.</p><p>We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance, and then retrieve the neighbors. The below function does this job in Scala:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=k>def</span> <span class=nf>getNeighbours</span><span class=p>(</span><span class=n>trainSet</span><span class=p>:</span> <span class=ne>Array</span><span class=p>[</span><span class=n>Row</span><span class=p>],</span> <span class=n>testRow</span><span class=p>:</span> <span class=n>Row</span><span class=p>,</span> <span class=n>k</span><span class=p>:</span> <span class=n>Int</span><span class=p>):</span> <span class=n>List</span><span class=p>[</span><span class=n>Row</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>var</span> <span class=n>distances</span> <span class=o>=</span> <span class=n>mutable</span><span class=o>.</span><span class=n>MutableList</span><span class=p>[(</span><span class=n>Row</span><span class=p>,</span> <span class=n>Double</span><span class=p>)]()</span>
</span></span><span class=line><span class=cl>    <span class=n>trainSet</span><span class=o>.</span><span class=n>foreach</span><span class=p>{</span><span class=n>trainRow</span> <span class=o>=&gt;</span>
</span></span><span class=line><span class=cl>        <span class=n>val</span> <span class=n>dist</span> <span class=o>=</span> <span class=n>computeEuclideanDistance</span><span class=p>(</span><span class=n>trainRow</span><span class=p>,</span> <span class=n>testRow</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>val</span> <span class=n>x</span> <span class=o>=</span> <span class=p>(</span><span class=n>trainRow</span><span class=p>,</span> <span class=n>dist</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>distances</span> <span class=o>+=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>distances</span> <span class=o>=</span> <span class=n>distances</span><span class=o>.</span><span class=n>sortBy</span><span class=p>(</span><span class=n>_</span><span class=o>.</span><span class=n>_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>var</span> <span class=n>neighbours</span> <span class=o>=</span> <span class=n>mutable</span><span class=o>.</span><span class=n>MutableList</span><span class=p>[</span><span class=n>Row</span><span class=p>]()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;-</span> <span class=mi>1</span> <span class=n>to</span> <span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>neighbours</span> <span class=o>+=</span> <span class=n>distances</span><span class=p>(</span><span class=n>i</span><span class=p>)</span><span class=o>.</span><span class=n>_1</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>neighbours</span><span class=o>.</span><span class=n>toList</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=step-3-make-predictions>Step 3: Make Predictions</h3><p>The most similar neighbors collected from the training dataset can be used to make predictions. In the case of classification, we can return the most represented output value (Class) among the neighbors.</p><p>We would first map the class values to the number of times it appears among the neighbors, then sort the counts in descending order and get the most appeared class value. The below function does exactly that in Scala:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def predictClassification(trainSet: Array[Row], testRow: Row, k: Int): String =
</span></span><span class=line><span class=cl>{
</span></span><span class=line><span class=cl>    val neighbours = getNeighbours(trainSet, testRow, k)
</span></span><span class=line><span class=cl>    val outputValues = for (row &lt;- neighbours) yield row.getString(trainSet(0).length - 1)
</span></span><span class=line><span class=cl>    outputValues.groupBy(identity)
</span></span><span class=line><span class=cl>        .mapValues(_.size)
</span></span><span class=line><span class=cl>        .toSeq
</span></span><span class=line><span class=cl>        .sortWith(_._2 &gt; _._2)
</span></span><span class=line><span class=cl>        .head._1
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><h3 id=apply-the-above-concepts-to-iris-dataset>Apply the above concepts to Iris Dataset</h3><p>We will now apply the concepts above to perform KNN on the Iris Dataset.</p><p>First, we have to load the dataset into the program. This is done using the <code>readCsv</code> function I&rsquo;ve written below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=k>def</span> <span class=nf>readCsv</span><span class=p>(</span><span class=n>fileName</span><span class=p>:</span> <span class=ne>String</span><span class=p>,</span> <span class=n>header</span><span class=p>:</span> <span class=n>Boolean</span><span class=p>):</span> <span class=n>DataFrame</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>read</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;csv&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=n>header</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;inferSchema&#34;</span><span class=p>,</span> <span class=n>header</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>fileName</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=o>$</span><span class=s2>&#34;Class&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>We also have to normalize the data we have. This is because KNN is based on distance between records. Unless data is normalized distance will be incorrectly calculated, because different attributes will not contribute to the distance in a uniform way. Attributes having a larger value range will have an unduly large influence on the distance, because they make greater contribution to the distance. If the dataset requires that some columns be given a greater preference over others, then normalization isn&rsquo;t recommended, but this is not true in the case of the Iris dataset.</p><p>We use the Z Score Normalization technique. With this, we subtract the mean of the respective column from each cell, and divide that with the standard deviation of that column. <a class=link href="https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0?ref=localhost" target=_blank rel=noopener>This</a> article describes Data Normalization in good detail.</p><p>The following function does our job:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def normalizeData(): Unit = {
</span></span><span class=line><span class=cl>    df.columns.filterNot(e =&gt; e == &#34;Class&#34;).foreach{col =&gt;
</span></span><span class=line><span class=cl>        val (mean_col, stddev_col) = df.select(mean(col), stddev(col))
</span></span><span class=line><span class=cl>            .as[(Double, Double)]
</span></span><span class=line><span class=cl>            .first()
</span></span><span class=line><span class=cl>        df = df.withColumn(s&#34;$col.norm&#34;, ($&#34;$col&#34; - mean_col) / stddev_col)
</span></span><span class=line><span class=cl>            .drop(col)
</span></span><span class=line><span class=cl>            .withColumnRenamed(s&#34;$col.norm&#34;, col)
</span></span><span class=line><span class=cl>    }
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><p>As you can see above, we are filtering out the class value, because we will not be using this value to compute the distance. There&rsquo;s one problem with our approach though, our KNN functions written above assume that the class value will be the last column. In the way we&rsquo;ve normalized the data, we are dropping the original column, and adding the normalized column in place. This will push the <code>Class</code> column to the beginning. So, I&rsquo;ve written another function which will move the column back to where it should actually be:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def moveClassToEnd(): Unit = {
</span></span><span class=line><span class=cl>    val cols = df.columns.filterNot(_ == &#34;Class&#34;) ++ Array(&#34;Class&#34;)
</span></span><span class=line><span class=cl>    df = df.select(cols.head, cols.tail: _*)
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><p>We will evaluate our algorithm using K-fold cross-validation with 5 folds. This means that we will have 150/5 = 30 rows per fold. We will use helper functions <code>evaluateAlgorithm()</code> and <code>accuracyMetric()</code> to evaluate the algorithm for cross-validation and calculate the accuracy of our predictions respectively.</p><p>Since Spark does not allow any of its operations inside a Spark transformation, we will have to perform a <code>collect()</code> on the Train set and Test set <code>DataFrame</code>s every time before passing it to any function. A sample run with <code>k = 3</code> produces the following output:</p><p><img src=/spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420.png width=383 height=64 srcset="/spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420_hu_85b7fcfa19312aff.png 480w, /spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420_hu_1740d1c92b4e1874.png 1024w" loading=lazy class=gallery-image data-flex-grow=598 data-flex-basis=1436px></p><p>Let&rsquo;s go one step further and run our program over different values of <code>k</code>. I&rsquo;m running it for <code>k</code> from <code>1 to 10</code>, and here are some results (this may not be the same everytime):</p><p><img src=/spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4.png width=682 height=585 srcset="/spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4_hu_158f5b034d8b9663.png 480w, /spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4_hu_c5a645ae67b8192d.png 1024w" loading=lazy class=gallery-image data-flex-grow=116 data-flex-basis=279px></p><p>KNN accuracy for a variety of k values</p><p>You can find the entire code below:</p><h3 id=conclusion>CONCLUSION</h3><p>While Spark ideally shouldn&rsquo;t be used smaller datasets like this, you could apply the same thought process and transform this code to use for some larger datasets, and there you will see the magic of Spark over Pandas.</p><p>Inspired heavily from <a class=link href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/?ref=localhost" target=_blank rel=noopener>this</a> great article.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/big-data/>Big Data</a>
<a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/scala/>Scala</a>
<a href=/tags/spark/>Spark</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Apr 14, 2024 17:43 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/spark-ml-data-preprocessing/><div class=article-image><img src=/spark-ml-data-preprocessing/661c082397510b44fe969e47.31a65b17b08b845b4b4da4e8b65f8e21_hu_9092776f6194c295.png width=250 height=150 loading=lazy alt="Featured image of post Cool Spark ML - Part 2: Preprocessing of Data" data-key=spark-ml-data-preprocessing data-hash="md5-MaZbF7CLhFtLTaTotl+OIQ=="></div><div class=article-details><h2 class=article-title>Cool Spark ML - Part 2: Preprocessing of Data</h2></div></a></article><article class=has-image><a href=/run-spark-3-applications-on-gpu/><div class=article-image><img src=/run-spark-3-applications-on-gpu/661c08139bb70e9dac2bfee1.16684aaa4058fbc4677edb3921d175ce_hu_e75e4405205a36d0.png width=250 height=150 loading=lazy alt="Featured image of post How to run Spark 3.0 applications on your GPU" data-key=run-spark-3-applications-on-gpu data-hash="md5-FmhKqkBY+8Rnfts5IdF1zg=="></div><div class=article-details><h2 class=article-title>How to run Spark 3.0 applications on your GPU</h2></div></a></article><article class=has-image><a href=/spark-3-native-gpu-integration/><div class=article-image><img src=/spark-3-native-gpu-integration/661c08375610fb416dbc1887.6528655ab75cec0475df3bd3338927c9_hu_eac0a47f3dce3e98.png width=250 height=150 loading=lazy alt="Featured image of post Spark 3.0 adds native GPU integration: Why that matters?" data-key=spark-3-native-gpu-integration data-hash="md5-ZShlWrdc7AR13zvTM4knyQ=="></div><div class=article-details><h2 class=article-title>Spark 3.0 adds native GPU integration: Why that matters?</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Sparker0i/Sparker0i.github.io data-repo-id=R_kgDOPDIt_w data-category=Announcements data-category-id=DIC_kwDOPDIt_84CsLyO data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"preferred_color_scheme":"preferred_color_scheme")}})()</script><footer class=site-footer><section class=copyright>&copy;
2017 -
2025 Sparker0i's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>