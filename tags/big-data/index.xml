<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Big Data on Sparker0i's Blog</title><link>https://blog.sparker0i.me/tags/big-data/</link><description>Recent content in Big Data on Sparker0i's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 14 Apr 2024 17:43:28 +0000</lastBuildDate><atom:link href="https://blog.sparker0i.me/tags/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Cool Spark ML - Part 2: Preprocessing of Data</title><link>https://blog.sparker0i.me/spark-ml-data-preprocessing/</link><pubDate>Sat, 06 Jun 2020 08:02:00 +0000</pubDate><guid>https://blog.sparker0i.me/spark-ml-data-preprocessing/</guid><description>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47.png" alt="Featured image of post Cool Spark ML - Part 2: Preprocessing of Data" />&lt;p>&lt;em>Note: This is the second article of the series: Cool Spark ML. The other parts can be found below:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://blog.sparker0i.me/spark-machine-learning-knn/" target="_blank" rel="noopener"
>Part 1: K Nearest Neighbours&lt;/a>&lt;/li>
&lt;li>Part 2: Preprocessing of Data (current)&lt;/li>
&lt;/ul>
&lt;p>People who have been performing Machine Learning for quite a long time know that Data Preprocessing is a key step before running any algorithms on the data. In a majority of datasets, you might always find null, or incomplete values. The data would also be inconsistent across columns, which directly affects algorithms using distance measures.&lt;/p>
&lt;p>This is where Data Preprocessing comes in. It is a crucial step which involves cleaning and organizing the data to make it suitable for building models. In other words, if you don&amp;rsquo;t perform Preprocessing, your models may not be accurate.&lt;/p>
&lt;p>While there are quite a lot of articles online about Data Preprocessing in Python, there aren&amp;rsquo;t a lot of them in Spark, or even Scala. In this post, I will be dealing with the ways you can perform Data Preprocessing in Spark on Scala.&lt;/p>
&lt;p>&lt;em>PS. You might be asking why I&amp;rsquo;m dealing with this now when I have actually written KNN in Spark before. The truth is, KNN isn&amp;rsquo;t officially supported inside Spark ML module. What I wrote in the previous article was a top-to-bottom version of KNN performed using Spark. You can also say that I&amp;rsquo;m doing a complete reset of this series ðŸ˜…&lt;/em>&lt;/p>
&lt;p>Just like always, the codes for all posts in this series will be available on &lt;a class="link" href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost" target="_blank" rel="noopener"
>my GitHub repo&lt;/a>.&lt;/p>
&lt;h2 id="types-of-preprocessing-in-spark">Types of Preprocessing in Spark
&lt;/h2>&lt;p>There are two types of preprocessing:&lt;/p>
&lt;ul>
&lt;li>Numeric Data&lt;/li>
&lt;li>Text Data&lt;/li>
&lt;/ul>
&lt;h2 id="numeric-data">Numeric Data
&lt;/h2>&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e.png"
width="744"
height="291"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e_hu_a661649faac3f5a4.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_013c380e-319f-4422-a230-92cd5dce8c9e_hu_12db0115b3c706bb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="613px"
>&lt;/p>
&lt;p>There are three ways you can preprocess numeric data in Spark:&lt;/p>
&lt;ul>
&lt;li>Normalize&lt;/li>
&lt;li>Standardize&lt;/li>
&lt;li>Bucketize&lt;/li>
&lt;/ul>
&lt;p>To illustrate Normalize and Standardize, I&amp;rsquo;ll be using some Scala magic which will generate my points as a Vector. Each vector represents a point in a 3-Dimensional Space.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">points&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="k">&amp;lt;-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="k">yield&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="nc">Vectors&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dense&lt;/span>&lt;span class="o">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nc">Array&lt;/span>&lt;span class="o">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="mi">10&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="mi">10000&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mf">1000.0&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">featuresDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">points&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">toDF&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;id&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;features&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Doing the above results in the following &lt;code>DataFrame&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3.png"
width="577"
height="508"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3_hu_e44d68fae859d626.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_cef59b8a-1ff3-44fe-ab23-93a073b574e3_hu_6122e23b16f7d93.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
>&lt;/p>
&lt;p>Each element inside Features column represents a point in a 3-D space.&lt;/p>
&lt;h3 id="normalize">Normalize
&lt;/h3>&lt;p>Normalization is the process of mapping numeric data from their original range into a range of 0 to 1. The lowest value of the original range gets value of 0, and the highest gets the value 1. All the other values in the original range will fall between these two.&lt;/p>
&lt;p>This is important because there may be multiple attributes with different ranges. &lt;em>E.g. Salary values may range between 3 and 8+ digit numbers, years in company will be between 1- and 2-digit numbers.&lt;/em> The reason we want to normalize those attributes in a &lt;code>[0,1]&lt;/code> range is so that when algorithms that use distance as a measure, they don&amp;rsquo;t weigh some attributes like salary more heavily than others.&lt;/p>
&lt;p>The formula to convert values in an un-normalized column to a normalized form is given by:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c.png"
width="220"
height="88"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c_hu_5c8f5f3c3406b51f.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_03103cb8-0d5d-4960-b635-33c1a62ec52c_hu_bf30bc0d48a5c834.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>Normalization Formula&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>x&lt;/code> is the value inside a column to be normalized,&lt;/li>
&lt;li>&lt;code>x(new)&lt;/code> is the normalized value,&lt;/li>
&lt;li>&lt;code>x(min)&lt;/code> is the minimum value of that column, and&lt;/li>
&lt;li>&lt;code>x(max)&lt;/code> is the maximum value of that column&lt;/li>
&lt;/ul>
&lt;p>Working on the &lt;code>featuresDf&lt;/code> created above, we will import &lt;code>MinMaxScaler&lt;/code> from the &lt;code>org.apache.spark.ml.feature&lt;/code> package. We now have to create an instance of the &lt;code>MinMaxScaler&lt;/code>. It will take two parameters: Input column name, and an Output Column name. This object will transform the contents of the input column vectors into a scaled version, and save it into the output column.&lt;/p>
&lt;p>In our case, we will be using our &lt;code>features&lt;/code> column inside &lt;code>featuresDf&lt;/code> as the input column, and our output column will be named &lt;code>sFeatures&lt;/code>. We create the instance in this manner:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">featureScaler&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">MinMaxScaler&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;features&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sfeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Next, we have to &lt;code>fit&lt;/code> the data present in our &lt;code>featuresDf&lt;/code> inside this &lt;code>featureScaler&lt;/code> and later &lt;code>transform&lt;/code> to create the scaled data. This is done using the code below:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">scaledDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">featureScaler&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">featuresDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">featuresDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Transforming original values into normalized ones&lt;/p>
&lt;p>Now, if we have a look at our transformed data:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03.png"
width="1170"
height="499"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03_hu_47ff0effba0c7922.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_58d6eebe-732e-499e-8cb9-08961bd34c03_hu_f07a2e91d95b1737.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="562px"
>&lt;/p>
&lt;p>Normalized &lt;code>DataFrame&lt;/code>&lt;/p>
&lt;p>You can then use this new &lt;code>sFeatures&lt;/code> to calculate distances among points.&lt;/p>
&lt;h3 id="standardize">Standardize
&lt;/h3>&lt;p>Now, we may have data whose values can be mapped to a bell-shaped curve, or normally distributed but maybe not exactly. With standardization, we map our data and transform it, which has a variance of 1 and/or a mean value of 0. This is done because some machine learning algorithms, like SVM, work better this way.&lt;/p>
&lt;p>Thus, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. The formula to convert values in a non-standardized column to a standardized form is given by:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea.png"
width="214"
height="87"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea_hu_e9428b7044f5744d.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_af201ed8-65e8-4341-8861-fde84255b7ea_hu_76b46cd750961242.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="245"
data-flex-basis="590px"
>&lt;/p>
&lt;p>Standardization Formula&lt;/p>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>x&lt;/code> is the value to be standardized&lt;/li>
&lt;li>&lt;code>x(new)&lt;/code> is the standardized value&lt;/li>
&lt;li>&lt;code>Î¼&lt;/code> is the mean of the column&lt;/li>
&lt;li>&lt;code>Ïƒ&lt;/code> is the standard deviation of the column.&lt;/li>
&lt;/ul>
&lt;p>Again, we will be using the &lt;code>featuresDf&lt;/code> created above. We will import &lt;code>StandardScaler&lt;/code> from the &lt;code>org.apache.spark.ml.feature&lt;/code> package. Just like &lt;code>MinMaxScaler&lt;/code>, an instance of &lt;code>StandardScaler&lt;/code> will require an input column and an output column. In our case, we will still continue with &lt;code>features&lt;/code> and &lt;code>sFeatures&lt;/code>. We will then &lt;code>fit&lt;/code> the data inside the scaler and later &lt;code>transform&lt;/code> the data. I&amp;rsquo;ve combined both these steps into a single code snippet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">featureStandardScaler&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">StandardScaler&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;features&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sfeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setWithStd&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setWithMean&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">standardizedDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">featureStandardScaler&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">featuresDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">featuresDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now if we have a look at our transformed data:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff.png"
width="1168"
height="495"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff_hu_4df59158823a7545.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_069f19b3-5a7d-4b8a-bb0f-6875947e17ff_hu_86937542ea1a8ac7.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="566px"
>&lt;/p>
&lt;p>Standardized Numeric Data&lt;/p>
&lt;p>Wait, weren&amp;rsquo;t the values supposed to be scaled within the range of &lt;code>[-1, 1]&lt;/code>? Well, that&amp;rsquo;s the surprise associated with the &lt;code>StandardScaler&lt;/code>. It uses the unbiased sample standard deviation instead of the population standard deviation.&lt;/p>
&lt;p>In other words, while the standard deviation will be 1 (or very close to 1), the mean may not be necessarily 0. To scale your data in a way that the range of numbers is between &lt;code>[-1,1]&lt;/code> and the standard deviation is 1 and mean 0, you will have to follow &lt;a class="link" href="https://stackoverflow.com/a/51755387/2451763?ref=localhost" target="_blank" rel="noopener"
>this accepted StackOverflow answer&lt;/a>. Even otherwise with this process, the data has been standardized.&lt;/p>
&lt;h3 id="bucketize">Bucketize
&lt;/h3>&lt;p>Bucketization is done when we have to organize continuous ranges of data into different buckets. &lt;code>Bucketizer&lt;/code> allows us to group data based on boundaries, so a list of boundaries has to be provided. I will call it &lt;code>splits&lt;/code> with the domain of all buckets when added looks like: &lt;code>{(-âˆž, -500.0) â‹ƒ [-500.0, -100.0) â‹ƒ [-100.0, -10.0) â‹ƒ [-10.0, 0.0) â‹ƒ [0.0, 10.0) â‹ƒ [10.0, 100.0) â‹ƒ [100.0, 500.0) â‹ƒ [500.0, âˆž)}&lt;/code>.&lt;/p>
&lt;p>Then I&amp;rsquo;ll generate 1000 random points that fall in the range of &lt;code>[-10000.0, 10000.0]&lt;/code> and save it in a &lt;code>DataFrame&lt;/code> with column name as &lt;code>features&lt;/code>. This is done using the below code:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">splits&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="nc">Array&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="nc">Float&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="nc">NegativeInfinity&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">500.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">100.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">10.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="mf">10.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="mf">100.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="mf">500.0&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="nc">Float&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="nc">PositiveInfinity&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">bucketData&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="k">&amp;lt;-&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="mi">10000&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="k">yield&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mf">10000.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mf">0.5&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">bucketDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">bucketData&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">toDF&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;features&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now, our &lt;code>Bucketizer&lt;/code> needs three inputs: the splits, input column name, and output column name. Then I&amp;rsquo;ll &lt;code>transform&lt;/code> that data which would then give me the element and which bucket it belongs to:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">bucketizer&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">Bucketizer&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setSplits&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">splits&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;features&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;bfeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">bucketedDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">bucketizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bucketDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Notice that I didn&amp;rsquo;t have to do a &lt;code>fit&lt;/code> operation before doing a &lt;code>transform&lt;/code>. This is because Bucketizing is fairly simple and you only need to find which bucket a number belongs to. Thus, there are no operations like scaling which happened in the other 2 sections, and hence you don&amp;rsquo;t need to &lt;code>fit&lt;/code> your data. Now if we have a look at the created &lt;code>DataFrame&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f.png"
width="298"
height="510"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f_hu_f998aa47506f8847.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_0fb28aab-611f-48ed-8abe-1c8794d3c20f_hu_796a64ef7381ef30.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="58"
data-flex-basis="140px"
>&lt;/p>
&lt;p>Bucketized DataFrame&lt;/p>
&lt;p>Now you might also want to know how many numbers are there in a particular bucket. So, I will do a &lt;code>groupBy&lt;/code> on &lt;code>bFeatures&lt;/code> column and retrieve the count of occurrences. The following code does that and displays my generated data:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60.png"
width="160"
height="231"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60_hu_ec9c2eab740d4bf2.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5ae7705a-2089-45b7-8f55-be6804bdda60_hu_7949b11a7e5f43a8.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="69"
data-flex-basis="166px"
>&lt;/p>
&lt;p>Fairly easy, isn&amp;rsquo;t it?&lt;/p>
&lt;h2 id="text">Text
&lt;/h2>&lt;p>There are two ways in which you can preprocess text-based data in Spark:&lt;/p>
&lt;ul>
&lt;li>Tokenize&lt;/li>
&lt;li>TF-IDF&lt;/li>
&lt;/ul>
&lt;p>To illustrate both of them, I will be using &lt;code>sentencesDf&lt;/code> created using this code:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">sentencesDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="nc">Seq&lt;/span>&lt;span class="o">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;This is an introduction to Spark ML&amp;#34;&lt;/span>&lt;span class="o">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;MLLib includes libraries for classification and regression&amp;#34;&lt;/span>&lt;span class="o">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;It also contains supporting tools for pipelines&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">).&lt;/span>&lt;span class="n">toDF&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;id&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;sentence&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="tokenize">Tokenize
&lt;/h3>&lt;p>In tokenization, you map your string containing a sentence into a set of tokens, or words. As an Example, the sentence &lt;em>&amp;ldquo;This is an introduction to Spark ML&amp;rdquo;&lt;/em> can be mapped into a list of 7 words - &lt;code>{This, is, an, introduction, to, Spark, ML}&lt;/code>.&lt;/p>
&lt;p>We will first import &lt;code>Tokenizer&lt;/code> from the &lt;code>org.apache.spark.ml.feature&lt;/code> package. Now an instance of this will need two parameters - input column and output column. Our input will be &lt;code>sentence&lt;/code> and the output will be &lt;code>words&lt;/code>, because that is what the &lt;code>Tokenizer&lt;/code> will produce. Then we will apply &lt;code>transform&lt;/code> on the sentences above.&lt;/p>
&lt;p>Now, just like bucketing, we are not &lt;code>fit&lt;/code>ting any data here. &lt;code>Tokenizer&lt;/code> already knows its job - Split strings into the separate words. The above process is illustrated in the code below:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">sentenceToken&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">Tokenizer&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sentence&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;words&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">sentenceTokenizedDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">sentenceToken&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">sentencesDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now, if we have a look at our data:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671.png"
width="1195"
height="164"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671_hu_92a327a34d5cae14.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_34e46d62-61a0-4814-aa63-461ded227671_hu_8584960d546cd83a.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="728"
data-flex-basis="1748px"
>&lt;/p>
&lt;p>The &lt;code>words&lt;/code> column contains lists of words that have been broken up in the ways you would expect a regular expression pattern matching to break up a sentence into words - based on white space, punctuation, etc.&lt;/p>
&lt;p>Easy, isn&amp;rsquo;t it?&lt;/p>
&lt;h3 id="term-frequency-inverse-document-frequency-tf-idf">Term Frequency-Inverse Document Frequency (TF-IDF)
&lt;/h3>&lt;p>Here we map text from a single, typically long string, to a vector, indicating the frequency of each word in a text relative to a group of texts such as a corpus. This transformation is widely used in text classification.&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804.png"
width="820"
height="433"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804_hu_23ab18036869a4d7.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_932f49cd-4e0a-403b-918d-968c1151c804_hu_e2319ff2c75b6f56.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>TF-IDF captures the intuition that infrequently used words are more useful for distinguishing categories of text than frequently used words. Considering the above figure as an example, &lt;em>Normalizing&lt;/em> appears only once, &lt;em>to&lt;/em> appears twice and so on. Like this, we go through all the documents in our corpus, which is nothing but a collection of documents. Then we count up how often a term appears across all of the documents. In this example &lt;em>normalizing&lt;/em> is a very rare word. Whereas other words like &lt;em>maps, data&lt;/em> and &lt;em>to&lt;/em> show up more frequently. We use these two sets of counts and feed those two into the term frequency-inverse document frequency calculation. And that gives us our TF-IDF measures.&lt;/p>
&lt;p>I will use the same &lt;code>sentenceTokenizedDf&lt;/code> created above for this exercise as well. Just like other processes mentioned above, we will need to import a few things from &lt;code>org.apache.spark.ml.feature&lt;/code> package - &lt;code>HashingTF&lt;/code> (for hashing Term Frequency), &lt;code>IDF&lt;/code> (for Inverse Document Frequency), &lt;code>Tokenizer&lt;/code>.&lt;/p>
&lt;p>First, I will create a &lt;code>HashingTF&lt;/code> instance - which takes an input column (&lt;code>words&lt;/code>), an output column (&lt;code>rawFeatures&lt;/code>) and the number of features to keep track of (&lt;code>20&lt;/code>) as the parameters. Now we apply our &lt;code>transform&lt;/code>ation on this and get a new &lt;code>DataFrame&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">hashingTF&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">HashingTF&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;words&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;rawFeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setNumFeatures&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="mi">20&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">sentenceHashingFunctionTermFrequencyDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">hashingTF&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">sentenceTokenizedDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sentenceHashingFunctionTermFrequencyDf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now if we have a look at our data, it has added an extra column which is of &lt;code>Vector&lt;/code> type. It has mapped each word to an index, so for example, &lt;em>this&lt;/em> maps to 1, &lt;em>is&lt;/em> maps to 4, &lt;em>an&lt;/em> -&amp;gt; 5, and so on.&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8.png"
width="1688"
height="154"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8_hu_dc939f94e76812db.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_5a42bed0-8330-4ea2-9c07-50730eacf3d8_hu_c57048eb454f7550.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="1096"
data-flex-basis="2630px"
>&lt;/p>
&lt;p>Now we&amp;rsquo;re going to scale the &lt;code>rawFeatures&lt;/code> vector values and we&amp;rsquo;re going to scale them based on how often the words appear in the entire collection of sentences. To do this we&amp;rsquo;re going to create an &lt;code>IDF&lt;/code> instance. Again, we have to specify an input column (&lt;code>rawFeatures&lt;/code>) and an output column (&lt;code>idfFeatures&lt;/code>) as parameters.&lt;/p>
&lt;p>Let&amp;rsquo;s use the term frequency data we just calculated to &lt;code>fit&lt;/code> the inverse document frequency model. And to do that I&amp;rsquo;m going to create an &lt;code>idfModel&lt;/code>, and we&amp;rsquo;re going to call the &lt;code>idf&lt;/code> object I just created, and I&amp;rsquo;m going to fit it using our term frequency data. Then we apply the IDF &lt;code>transform&lt;/code>ation to create a new &lt;code>DataFrame&lt;/code> that has both the term frequency and the inverse document frequency transformations applied.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-scala" data-lang="scala">&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">idf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nc">IDF&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setInputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;rawFeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">setOutputCol&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;idfFeatures&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">idfModel&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">idf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">sentenceHashingFunctionTermFrequencyDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">val&lt;/span> &lt;span class="n">tfIdfDf&lt;/span> &lt;span class="k">=&lt;/span> &lt;span class="n">idfModel&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">sentenceHashingFunctionTermFrequencyDf&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now if we have a look at our data (I&amp;rsquo;m selecting only the &lt;code>rawFeatures&lt;/code> and &lt;code>idfFeatures&lt;/code> columns to fit in the screen):&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a.png"
width="1871"
height="159"
srcset="https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a_hu_ab78ff15b2aad54.png 480w, https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_14c44800-1c47-461b-84cf-084d26a3606a_hu_b8b1bede67a8fffe.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="1176"
data-flex-basis="2824px"
>&lt;/p>
&lt;p>Now we have a new column which contains the inverse document frequency features. These are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences.&lt;/p>
&lt;h2 id="conclusion">CONCLUSION
&lt;/h2>&lt;p>Preprocessing is indeed a tough challenge where you will have to know what kinds of data you might get and what kinds of processing you want to apply on your data. If not done properly, your machine learning models might not be of much use.&lt;/p></description></item><item><title>Spark 3.0 adds native GPU integration: Why that matters?</title><link>https://blog.sparker0i.me/spark-3-native-gpu-integration/</link><pubDate>Sat, 16 May 2020 18:01:34 +0000</pubDate><guid>https://blog.sparker0i.me/spark-3-native-gpu-integration/</guid><description>&lt;img src="https://blog.sparker0i.me/spark-3-native-gpu-integration/661c08375610fb416dbc1887.png" alt="Featured image of post Spark 3.0 adds native GPU integration: Why that matters?" />&lt;p>You can soon run your Apache Spark programs natively on your GPU. This became possible thanks to collaboration between Nvidia and Databricks. At the GPU Technology Conference, both the companies have presented a solution that brings GPU Acceleration to Spark 3.0 without major code changes.&lt;/p>
&lt;h2 id="how-things-were-before">How things were before?
&lt;/h2>&lt;p>GPU based solutions have existed for Spark for a long time, so what has changed?&lt;/p>
&lt;p>Such GPU integrations into Spark were provided by either third party libraries in Java/Scala, or you had to depend on Cloud Providers which would provide such an infrastructure to run Spark on GPU. Also, programs would usually be restricted to applications based on Spark ML, thus they generally couldn&amp;rsquo;t be applied to other Big Data uses on Scale.&lt;/p>
&lt;p>When it comes to Spark/Python, you had to use custom tools like Horovod, which would also end up using popular Python based libraries like Numpy and Tensorflow. Thus, this approach severely limits the performance of the Spark Programs due to the nature of Python, where programs are dynamically interpreted.&lt;/p>
&lt;p>Don&amp;rsquo;t get me wrong, Python has its very own unique use-cases which Scala doesn&amp;rsquo;t provide (yet), but because Spark was built to do Big Data operations effectively, Python severely restricts the performance.&lt;/p>
&lt;h2 id="what-happened-now">What happened now?
&lt;/h2>&lt;p>With the release of Spark 3.0, native GPU based acceleration will be provided within Spark. This acceleration is based on the open source &lt;a class="link" href="https://www.anrdoezrs.net/links/9041660/type/dlg/sid/zd-ad14a02e5d404bd4822065953dda157b--%7Cxid:fr1589641152241fef/https://developer.nvidia.com/rapids?ref=localhost" target="_blank" rel="noopener"
>RAPIDS&lt;/a> suite of software libraries, Nvidia built on &lt;a class="link" href="https://www.anrdoezrs.net/links/9041660/type/dlg/sid/zd-ad14a02e5d404bd4822065953dda157b--%7Cxid:fr1589641152241dce/https://developer.nvidia.com/machine-learning?ref=localhost" target="_blank" rel="noopener"
>CUDA-X AI&lt;/a>. This will allow developers to run Spark code without any modifications on GPUs - thereby alleviating load off the CPU.&lt;/p>
&lt;p>This also benefits Spark SQL and &lt;code>DataFrame&lt;/code> operations, thereby making the GPU acceleration benefits available for non-Machine Learning workloads as well. This will also bring capabilities where we don&amp;rsquo;t have to provision a dedicated Spark Cluster for AI and ML based jobs.&lt;/p>
&lt;p>In an advanced briefing for members of the press, NVidia CEO Jensen Huang explained that users of Spark clusters on &lt;a class="link" href="https://click.linksynergy.com/deeplink?id=IokOf8qagZo&amp;amp;mid=24542&amp;amp;u1=zd-ad14a02e5d404bd4822065953dda157b--%7Cxid%3Afr1589641152241ghb&amp;amp;murl=https%3A%2F%2Fazure.microsoft.com%2Fservices%2Fmachine-learning%2F&amp;amp;ref=localhost" target="_blank" rel="noopener"
>Azure Machine Learning&lt;/a> or &lt;a class="link" href="https://aws.amazon.com/sagemaker/?ref=localhost" target="_blank" rel="noopener"
>Amazon SageMaker&lt;/a> can benefit from the GPU acceleration as well. This means that the infrastructure is already in place, it is now upon other cloud providers to provide the necessary infrastructure, and upon developers to adopt and build their workloads to the new changes.&lt;/p>
&lt;h2 id="adobe--spark-gpu-acceleration">Adobe + Spark GPU Acceleration
&lt;/h2>&lt;p>Adobe and Nvidia had signed a &lt;a class="link" href="https://news.adobe.com/news/news-details/2018/Adobe-and-NVIDIA-Announce-Partnership-to-Deliver-New-AI-Services-for-Creativity-and-Digital-Experiences/default.aspx?ref=localhost" target="_blank" rel="noopener"
>deal&lt;/a> in 2018 where they will utilize Nvidia&amp;rsquo;s AI capabilities for their solutions. Building upon this deal, Adobe has been an early adopter for this new GPU Acceleration on Spark, and they have shown a 7x improvement in performance of their workloads, while saving up to 90% of the costs.&lt;/p>
&lt;p>These are serious numbers. Imagine, if a company as huge as Adobe is able to bring down costs while improving performance, other companies too can follow suit and we could see Profits and Performance for everyone. Period.&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>Imagine how game changing this can prove to be for the Big Data community overall. No longer will we have to wait for operations to complete when we can utilize the GPU, we have on our local Gaming PCs and laptops. We will also be able to utilize GPU servers on Cloud for Spark without doing major changes.&lt;/p>
&lt;p>This can also encourage many people to start using Scala for AI and Machine Learning instead of Python. While I do realize that there are no major visualization libraries supporting Spark available in Scala, an encouragement to do machine learning with Spark shall bring more enthusiasm for Scala, due to the disadvantages I mentioned for Python above. This in turn will lead to a growth in the Scala community, which will further result in availability of more and more libraries.&lt;/p>
&lt;p>For now, there is a Scala visualization library that supports Spark, in active development, which when released to MVN Repository could be a game changer. Head over to &lt;a class="link" href="https://github.com/MarkCLewis/SwiftVis2?ref=localhost" target="_blank" rel="noopener"
>SwiftViz2&amp;rsquo;s GitHub repo&lt;/a> for more info. You can place safe bets on this one :)&lt;/p>
&lt;p>In short, this is a win-win situation for everyone involved in this ecosystem.&lt;/p>
&lt;p>Until another blog post, Ciao.&lt;/p>
&lt;h2 id="sources">SOURCES
&lt;/h2>&lt;p>&lt;a class="link" href="https://www.zdnet.com/article/nvidia-and-databricks-announce-gpu-acceleration-for-spark-3-0/?ref=localhost" target="_blank" rel="noopener"
>ZdNet&lt;/a>, &lt;a class="link" href="https://nvidianews.nvidia.com/news/nvidia-accelerates-apache-spark-worlds-leading-data-analytics-platform?ref=localhost" target="_blank" rel="noopener"
>Nvidia Newsroom&lt;/a>&lt;/p></description></item><item><title>Cool Spark ML: K Nearest Neighbors</title><link>https://blog.sparker0i.me/spark-machine-learning-knn/</link><pubDate>Sun, 19 Apr 2020 05:41:03 +0000</pubDate><guid>https://blog.sparker0i.me/spark-machine-learning-knn/</guid><description>&lt;img src="https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc.png" alt="Featured image of post Cool Spark ML: K Nearest Neighbors" />&lt;p>&lt;em>Note: This article is the first of the Series: Cool Spark ML. Other parts are coming soon.&lt;/em>&lt;/p>
&lt;p>I had taken up a few machine learning courses in my college throughout 2018. Most of the problems there were solved using Python and the necessary libraries - NumPy, Pandas, Scikit-Learn and Matplotlib. With my daily work at IBM now requiring me to use Scala and Spark, I decided to use my free time during the lockdown to try out Spark ML.&lt;/p>
&lt;p>&lt;em>Note: All the codes in the Cool Spark ML Series will be available on&lt;/em> &lt;a class="link" href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost" target="_blank" rel="noopener"
>&lt;em>my GitHub repo&lt;/em>&lt;/a>&lt;/p>
&lt;h3 id="intro-to-spark-ml">Intro to Spark ML
&lt;/h3>&lt;p>As the name suggests, Spark ML is the Machine Learning library consisting of common Machine learning algorithms - classification, regression, clustering etc.&lt;/p>
&lt;h3 id="why-spark-ml">Why Spark ML?
&lt;/h3>&lt;p>Pandas - a Python library - wonâ€™t work every time. It is a single machine tool, so it&amp;rsquo;s constrained by the machine&amp;rsquo;s limits. Moreover, pandas doesnâ€™t have any parallelism built in, which means it uses only one CPU core. You may hit a dead-end on datasets of the size of a few gigabytes. Pandas won&amp;rsquo;t help if you want to work on very big datasets.&lt;/p>
&lt;p>We are now in the Big Data era, where gigabytes of data are generated every few seconds. Such datasets will require powerful systems to run even the basic machine learning algorithms. The cost of getting such a powerful system will be huge, as well as the costs to scale them up. With distributed computers, such calculations can be sent to multiple low-end machines, which prevents the cost of getting a single high-end machine.&lt;/p>
&lt;p>This is where Spark kicks in. Spark has the concept of &lt;code>DataFrame&lt;/code> (now deprecated in favor of Datasets), which behaves very similar to how a Pandas &lt;code>DataFrame&lt;/code> would do, including having very similar APIs too. The advantage of using Spark &lt;code>DataFrame&lt;/code> is that it was designed from ground-up to support Big Data. Spark can also distribute such &lt;code>DataFrame&lt;/code>s across multiple machines and collect the calculated results.&lt;/p>
&lt;h3 id="knn-k-nearest-neighbors">KNN: K-Nearest Neighbors
&lt;/h3>&lt;p>The process in KNN is pretty simple. You load your entire dataset first, each of which will have input columns and one output column. This is then split into a training set and a testing set. You then use your training set to train your model, and then use the testing set to predict the output column value by testing it against the model. You then compare the actual and the predicted target values and calculate the accuracy of your model.&lt;/p>
&lt;h3 id="problem-definition">Problem Definition
&lt;/h3>&lt;p>We are going to train a model to predict the famous &lt;a class="link" href="http://archive.ics.uci.edu/ml/datasets/iris?ref=localhost" target="_blank" rel="noopener"
>Iris dataset&lt;/a>. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.&lt;/p>
&lt;p>It is a multiclass classification problem. The number of observations for each class is the same. The dataset is small in size with only 150 rows with 4 input variables and 1 output variable.&lt;/p>
&lt;p>The 4 features are described as follows:&lt;/p>
&lt;ol>
&lt;li>Sepal-Length, in cm&lt;/li>
&lt;li>Sepal-Width, in cm&lt;/li>
&lt;li>Petal-Length, in cm&lt;/li>
&lt;li>Petal-Width, in cm&lt;/li>
&lt;/ol>
&lt;h3 id="prerequisites">Prerequisites
&lt;/h3>&lt;ol>
&lt;li>Create a Scala project in IntelliJ IDEA based on SBT&lt;/li>
&lt;li>Select Scala version 2.11.12&lt;/li>
&lt;li>Include &lt;code>spark-core&lt;/code>, &lt;code>spark-sql&lt;/code> and &lt;code>spark-ml&lt;/code> 2.4.5 as library dependencies in your &lt;code>build.sbt&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="knn-steps">KNN Steps
&lt;/h3>&lt;p>In this blog post, I will be developing KNN algorithm from scratch. The process to perform KNN can be broken down into 3 easy steps:&lt;/p>
&lt;ol>
&lt;li>Calculate Euclidean Distance&lt;/li>
&lt;li>Get Nearest Neighbors&lt;/li>
&lt;li>Make Predictions&lt;/li>
&lt;/ol>
&lt;h3 id="step-1-calculate-euclidean-distance">Step 1: Calculate Euclidean Distance
&lt;/h3>&lt;p>The first step will be to calculate the distance between two rows in a Dataset. Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line.&lt;/p>
&lt;p>Euclidean Distance is calculated as the square root of the sum of the squared differences between the two vectors, as given in the image below:&lt;/p>
&lt;p>&lt;a class="link" href="https://www.codecogs.com/eqnedit.php?latex=%5Cinline&amp;amp;space%3B%5Cbg_white=&amp;amp;space%3B%7B%5Ccolor%7BRed%7D=&amp;amp;space%3B%24%24dist_%7Bx_1%2Cx_2%7D=&amp;amp;space%3B=&amp;amp;space%3B%5Csqrt%7B%5Csum_%7Bi=0%7D%5E%7BN%7D&amp;amp;space%3B%28%7Bx_1_i=&amp;amp;space%3B-=&amp;amp;space%3Bx_2_i%7D%29%5E2%7D.%24%24%7D=&amp;amp;ref=localhost" target="_blank" rel="noopener"
>&lt;img src="https://latex.codecogs.com/gif.latex?%5Cinline&amp;amp;space;%5Cbg_white&amp;amp;space;%7B%5Ccolor%7BRed%7D&amp;amp;space;$$dist_%7Bx_1,x_2%7D&amp;amp;space;=&amp;amp;space;%5Csqrt%7B%5Csum_%7Bi=0%7D%5E%7BN%7D&amp;amp;space;%28%7Bx_1_i&amp;amp;space;-&amp;amp;space;x_2_i%7D%29%5E2%7D.$$%7D"
loading="lazy"
>&lt;/a>&lt;/p>
&lt;p>Where &lt;code>x1&lt;/code> is the first row of data, &lt;code>x2&lt;/code> is the second row of data, and &lt;code>i&lt;/code> is a specific index for a column as we sum across all columns. Smaller the value, more similar will be the two rows.&lt;/p>
&lt;p>Since we will be reading our data and transforming it using Spark, to compute distances between two &lt;code>Row&lt;/code>s in a &lt;code>DataFrame&lt;/code>, we write the function below in Scala:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">computeEuclideanDistance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">row1&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">row2&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Row&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="n">Double&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">var&lt;/span> &lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="n">until&lt;/span> &lt;span class="n">row1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">length&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">distance&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">row1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getDouble&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">row2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">getDouble&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can see that the function assumes that the last column in each row is an output value which is ignored from the distance calculation.&lt;/p>
&lt;h3 id="step-2-get-nearest-neighbors">Step 2: Get Nearest Neighbors
&lt;/h3>&lt;p>Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our distance measure. To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above.&lt;/p>
&lt;p>We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance, and then retrieve the neighbors. The below function does this job in Scala:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">getNeighbours&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trainSet&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="ne">Array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">testRow&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Int&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">var&lt;/span> &lt;span class="n">distances&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mutable&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MutableList&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Double&lt;/span>&lt;span class="p">)]()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">trainSet&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">foreach&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">trainRow&lt;/span> &lt;span class="o">=&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">val&lt;/span> &lt;span class="n">dist&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">computeEuclideanDistance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trainRow&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">testRow&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">val&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">trainRow&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">distances&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">x&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">distances&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">distances&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sortBy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">_&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">var&lt;/span> &lt;span class="n">neighbours&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mutable&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MutableList&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="p">]()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">neighbours&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">distances&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">neighbours&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">toList&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="step-3-make-predictions">Step 3: Make Predictions
&lt;/h3>&lt;p>The most similar neighbors collected from the training dataset can be used to make predictions. In the case of classification, we can return the most represented output value (Class) among the neighbors.&lt;/p>
&lt;p>We would first map the class values to the number of times it appears among the neighbors, then sort the counts in descending order and get the most appeared class value. The below function does exactly that in Scala:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">def predictClassification(trainSet: Array[Row], testRow: Row, k: Int): String =
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> val neighbours = getNeighbours(trainSet, testRow, k)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> val outputValues = for (row &amp;lt;- neighbours) yield row.getString(trainSet(0).length - 1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> outputValues.groupBy(identity)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .mapValues(_.size)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .toSeq
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .sortWith(_._2 &amp;gt; _._2)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .head._1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="apply-the-above-concepts-to-iris-dataset">Apply the above concepts to Iris Dataset
&lt;/h3>&lt;p>We will now apply the concepts above to perform KNN on the Iris Dataset.&lt;/p>
&lt;p>First, we have to load the dataset into the program. This is done using the &lt;code>readCsv&lt;/code> function I&amp;rsquo;ve written below:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">readCsv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fileName&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="ne">String&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Boolean&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="n">DataFrame&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">spark&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">format&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;csv&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">option&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;header&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">option&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;inferSchema&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fileName&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="n">repartition&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="s2">&amp;#34;Class&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We also have to normalize the data we have. This is because KNN is based on distance between records. Unless data is normalized distance will be incorrectly calculated, because different attributes will not contribute to the distance in a uniform way. Attributes having a larger value range will have an unduly large influence on the distance, because they make greater contribution to the distance. If the dataset requires that some columns be given a greater preference over others, then normalization isn&amp;rsquo;t recommended, but this is not true in the case of the Iris dataset.&lt;/p>
&lt;p>We use the Z Score Normalization technique. With this, we subtract the mean of the respective column from each cell, and divide that with the standard deviation of that column. &lt;a class="link" href="https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0?ref=localhost" target="_blank" rel="noopener"
>This&lt;/a> article describes Data Normalization in good detail.&lt;/p>
&lt;p>The following function does our job:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">def normalizeData(): Unit = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> df.columns.filterNot(e =&amp;gt; e == &amp;#34;Class&amp;#34;).foreach{col =&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> val (mean_col, stddev_col) = df.select(mean(col), stddev(col))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .as[(Double, Double)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .first()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> df = df.withColumn(s&amp;#34;$col.norm&amp;#34;, ($&amp;#34;$col&amp;#34; - mean_col) / stddev_col)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .drop(col)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .withColumnRenamed(s&amp;#34;$col.norm&amp;#34;, col)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>As you can see above, we are filtering out the class value, because we will not be using this value to compute the distance. There&amp;rsquo;s one problem with our approach though, our KNN functions written above assume that the class value will be the last column. In the way we&amp;rsquo;ve normalized the data, we are dropping the original column, and adding the normalized column in place. This will push the &lt;code>Class&lt;/code> column to the beginning. So, I&amp;rsquo;ve written another function which will move the column back to where it should actually be:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">def moveClassToEnd(): Unit = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> val cols = df.columns.filterNot(_ == &amp;#34;Class&amp;#34;) ++ Array(&amp;#34;Class&amp;#34;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> df = df.select(cols.head, cols.tail: _*)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We will evaluate our algorithm using K-fold cross-validation with 5 folds. This means that we will have 150/5 = 30 rows per fold. We will use helper functions &lt;code>evaluateAlgorithm()&lt;/code> and &lt;code>accuracyMetric()&lt;/code> to evaluate the algorithm for cross-validation and calculate the accuracy of our predictions respectively.&lt;/p>
&lt;p>Since Spark does not allow any of its operations inside a Spark transformation, we will have to perform a &lt;code>collect()&lt;/code> on the Train set and Test set &lt;code>DataFrame&lt;/code>s every time before passing it to any function. A sample run with &lt;code>k = 3&lt;/code> produces the following output:&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420.png"
width="383"
height="64"
srcset="https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420_hu_85b7fcfa19312aff.png 480w, https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_1316a779-2e73-40a0-aef1-ae450ca01420_hu_1740d1c92b4e1874.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="598"
data-flex-basis="1436px"
>&lt;/p>
&lt;p>Let&amp;rsquo;s go one step further and run our program over different values of &lt;code>k&lt;/code>. I&amp;rsquo;m running it for &lt;code>k&lt;/code> from &lt;code>1 to 10&lt;/code>, and here are some results (this may not be the same everytime):&lt;/p>
&lt;p>&lt;img src="https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4.png"
width="682"
height="585"
srcset="https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4_hu_158f5b034d8b9663.png 480w, https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_cb17fce4-6f7f-4070-8fc0-ff27ece000e4_hu_c5a645ae67b8192d.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="116"
data-flex-basis="279px"
>&lt;/p>
&lt;p>KNN accuracy for a variety of k values&lt;/p>
&lt;p>You can find the entire code below:&lt;/p>
&lt;h3 id="conclusion">CONCLUSION
&lt;/h3>&lt;p>While Spark ideally shouldn&amp;rsquo;t be used smaller datasets like this, you could apply the same thought process and transform this code to use for some larger datasets, and there you will see the magic of Spark over Pandas.&lt;/p>
&lt;p>Inspired heavily from &lt;a class="link" href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/?ref=localhost" target="_blank" rel="noopener"
>this&lt;/a> great article.&lt;/p></description></item></channel></rss>