[{"content":"Cloud photo services keep getting more expensive, and there\u0026rsquo;s something unsettling about handing years of memories to companies that might change their terms, get acquired, or simply shut down. We\u0026rsquo;ve seen that happen with Google Photos when they offered their service for free for the first few years to make people get used to their product, and then flipped the switch on them to make them.\nImmich offers a compelling alternative - an open-source photo management platform that rivals Google Photos and iCloud, without the subscription fees or storage limits.\nI\u0026rsquo;ve been running Fedora on my gaming PC for a while - which has since been repurposed to a homelab while not gaming - and Podman has become my go-to container runtime. It\u0026rsquo;s daemon-free, runs rootless by default, and integrates cleanly with systemd. But adding GPU acceleration to the mix - necessary for Immich\u0026rsquo;s AI features like facial recognition and smart search - turns a straightforward setup into a complex puzzle involving user namespaces, device permissions, SELinux policies, and network configuration.\nThis guide walks through my complete setup of Immich v2 with full CUDA GPU acceleration using Podman Quadlet. The journey involved navigating several poorly-documented incompatibilities between rootless containers and GPU access. The result is a self-hosted photo solution that actually uses your own hardware.\nSystem Environment:\nOS: Fedora Linux with SELinux enforcing Container Runtime: Podman 5.x (rootless mode) GPU: NVIDIA GeForce RTX 5080 Driver: NVIDIA 580.119.02, CUDA 13.0 Immich: v2 Why Not Docker Compose or Podman Compose? Before diving into the setup, it\u0026rsquo;s worth explaining why I chose Podman Quadlet over the more familiar Docker Compose or even Podman Compose.\nThe Case Against Docker Docker requires a daemon running as root, which means:\nRoot privileges: The Docker daemon runs with full root access, and any user in the docker group effectively has root on the host (they can mount any directory and modify system files) Daemon dependency: If the Docker daemon crashes or hangs, all containers stop Background service: Docker constantly consumes resources even when no containers are running Podman, by contrast, is daemonless and runs rootless by default. Each container runs under your own user, making it inherently more secure for personal homelab use.\nPodman Compose vs Quadlet Podman Compose exists as a drop-in replacement for Docker Compose, but it has limitations:\nExternal tool: It\u0026rsquo;s a wrapper around Podman that needs to be installed separately and run manually No systemd integration: Starting containers on boot requires manual setup with systemd service files Command-driven: You still run podman-compose up -d, which feels like a workaround rather than native integration Why Quadlet? Quadlet lets you define containers as systemd unit files (.container, .pod, .volume, etc.) in ~/.config/containers/systemd/. The benefits are significant:\nNative systemd integration: Containers are managed as systemd services - start, stop, enable, and check status using familiar systemctl commands Automatic startup: Enable services to start on boot with systemctl --user enable \u0026lt;service\u0026gt; Service dependencies: Define explicit dependencies (e.g., database must start before server) using systemd\u0026rsquo;s Requires= and After= directives Unified management: All system services - containers or not - are managed the same way. Your Immich service appears alongside other systemd services Declarative configuration: No imperative up/down commands. The configuration files define the desired state, and systemd maintains it For a homelab server where I want everything to start automatically after a reboot and recover from failures, Quadlet is the cleaner solution. It\u0026rsquo;s the \u0026ldquo;Fedora way\u0026rdquo; of running containers - embracing systemd rather than fighting it.\nThat said, this approach has trade-offs. The main one you\u0026rsquo;ll see in this guide: Quadlet\u0026rsquo;s rootless pods with user namespace remapping create complications with GPU device access that are easier to work around in Docker. But the security and integration benefits are worth the extra setup effort.\nInitial Setup: Converting Docker Compose to Quadlet Start by obtaining Immich\u0026rsquo;s docker-compose.yml. You would then need to adapt it for Podman by making the following changes in the compose file:\nSet DB_HOSTNAME and REDIS_HOSTNAME to 127.0.0.1 for microservice communication within the same pod Used absolute paths for volumes. I\u0026rsquo;m currently using /mnt/server/immich-app Specified environment file location 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 name: immich services: immich-server: image: ghcr.io/immich-app/immich-server:v2 volumes: - /mnt/server/immich-app/server/upload:/data - /etc/localtime:/etc/localtime:ro ports: - \u0026#34;2283:2283\u0026#34; depends_on: - redis - database environment: DB_HOSTNAME: 127.0.0.1 REDIS_HOSTNAME: 127.0.0.1 env_file: - /mnt/server/immich-app/.env restart: always machine-learning: image: ghcr.io/immich-app/immich-machine-learning:v2-cuda volumes: - /mnt/server/immich-app/model-cache:/cache env_file: - /mnt/server/immich-app/.env restart: always immich-redis: image: docker.io/valkey/valkey:9 volumes: - /mnt/server/immich-app/redis:/data restart: always immich-database: image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0 volumes: - /mnt/server/immich-app/postgres:/var/lib/postgresql/data env_file: - /mnt/server/immich-app/.env restart: always Converting with podlet Use podlet to convert the compose file to Quadlet unit files:\n1 2 3 4 5 # Install podlet if not already installed dnf install podlet # Convert to Quadlet files podlet compose -i -a --pod The -i flag adds [Install] sections, -a converts relative paths to absolute paths, and --pod creates a pod for all services.\nThis generates .container and .pod files that belong in ~/.config/containers/systemd/. However, achieving GPU acceleration requires working through several complications that aren\u0026rsquo;t covered in the standard documentation.\nThe Journey: Issues Encountered and Solutions 1. Database Mount Directory Ownership Issues Problem Manifestation:\nThe PostgreSQL data directory at /mnt/server/immich-app/postgres was owned by systemd-oom user from a previous installation attempt, causing permission denied errors when the database tried to access files.\nSolution:\nChanged ownership to the current user:\n1 sudo chown -R $USER:$USER /mnt/server/immich-app/postgres Then added UserNS=keep-id to the pod configuration to maintain consistent UID/GID between host and containers:\n1 2 3 4 5 6 7 8 # immich.pod [Pod] PodName=systemd-immich PublishPort=2283:2283 UserNS=keep-id [Install] WantedBy=default.target 2. SELinux Volume Mount Errors Problem Manifestation:\nServer container failed to start with SELinux errors when using the :Z flag on existing volume mounts.\nSolution:\nRemoved the :Z flag and added --security-opt label=disable to disable SELinux labeling for this container:\n1 2 3 4 5 6 # server.container [Container] ... PodmanArgs=--security-opt label=disable Volume=/mnt/server/immich-app/server/upload:/data Volume=/etc/localtime:/etc/localtime:ro 3. Redis Permission Errors After Running Problem Manifestation:\nAfter solving the database ownership issue, the Immich server started crashing repeatedly with:\n1 2 3 ERROR [Microservices:MetadataService] Unable to initialize reverse geocoding: ReplyError: MISCONF Valkey is configured to save RDB snapshots, but it\u0026#39;s currently unable to persist to disk. Valkey logs revealed the root cause:\n1 2 Failed opening the temp RDB file temp-1.rdb (in server root dir /data) for saving: Permission denied Solution:\nFound in nite07\u0026rsquo;s Podman guide: add User=root and Group=root to the Redis container configuration:\n1 2 3 4 5 6 # immich-redis.container [Container] ... User=root Group=root ... After this change, reload systemd:\n1 2 systemctl --user daemon-reload systemctl --user restart immich-pod 4. Machine Learning Service Unreachable Problem Manifestation:\nServer logs showed ML service became unhealthy, unable to connect to the machine learning container.\nSolution:\nThe server was trying to reach the ML service using container hostname. Added explicit environment variable to use localhost:\n1 2 3 4 5 # immich-server.container [Container] ... Environment=DB_HOSTNAME=127.0.0.1 REDIS_HOSTNAME=127.0.0.1 IMMICH_MACHINE_LEARNING_URL=http://127.0.0.1:3003 ... Note: This was later changed to http://host.containers.internal:3003 after moving ML outside the pod.\n5. The GPU Challenge: CUDA Driver Version Mismatch Problem Manifestation:\nThe ML container initially used implicit GPU assignment:\n1 PodmanArgs=--gpus all ... When starting, it failed with:\n1 CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version Despite having NVIDIA Driver 580.119.02 and CUDA 13.0 installed on the host, the container couldn\u0026rsquo;t access the GPU properly.\nRoot Cause:\nAfter extensive troubleshooting, I discovered that rootless Podman pods with UserNS=keep-id are incompatible with CDI (Container Device Interface) GPU device access. This isn\u0026rsquo;t well-documented and required significant debugging to identify.\nUnderstanding the Conflict:\nUserNS=keep-id preserves your host user\u0026rsquo;s UID/GID inside containers. Normally in rootless mode, Podman maps container UIDs to a range of host UIDs (e.g., container root UID 0 → host UID 100000). With UserNS=keep-id, your UID stays the same (e.g., UID 1000 stays UID 1000), making file ownership manageable.\nCDI (Container Device Interface) is NVIDIA\u0026rsquo;s modern approach to exposing GPUs to containers. Instead of manually mounting device nodes like /dev/nvidia0, CDI uses a specification file (nvidia.yaml) that describes GPU devices, their dependencies, and required hooks. When you use --device nvidia.com/gpu=all, Podman reads this spec and sets up the GPU environment.\nThe Incompatibility:\nCDI relies on device node ownership and permissions that are set up outside the user namespace. When UserNS=keep-id is active in a pod:\nThe pod creates a user namespace with UID mapping CDI tries to bind GPU device nodes into this namespace The device nodes\u0026rsquo; permissions become inconsistent due to UID remapping The container runtime (crun) fails with \u0026ldquo;File exists\u0026rdquo; or permission errors GPU access works perfectly in standalone containers (no user namespace conflicts) but fails inside pods with UserNS=keep-id.\nSolution Path:\nStep 1: Set Up CDI (Container Device Interface) First, make the NVIDIA CDI specification accessible to rootless Podman (Replace sparker0i with your username):\n1 2 3 4 5 6 7 8 9 10 # Copy CDI specification to user directory mkdir -p ~/.config/cdi sudo cp /var/run/cdi/nvidia.yaml ~/.config/cdi/ sudo chown sparker0i:sparker0i ~/.config/cdi/nvidia.yaml # Configure Podman to use CDI cat \u0026gt; ~/.config/containers/containers.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [engine] cdi_spec_dirs = [\u0026#34;/var/run/cdi\u0026#34;, \u0026#34;/home/sparker0i/.config/cdi\u0026#34;] EOF Verify CDI devices are available:\n1 cat /var/run/cdi/nvidia.yaml | head -50 Step 2: Test CDI GPU Access Testing confirmed the incompatibility:\n1 2 3 4 5 6 7 8 9 # Test FAILED in pod with UserNS=keep-id podman run --rm --pod systemd-immich --device nvidia.com/gpu=all \\ ghcr.io/immich-app/immich-machine-learning:v2-cuda echo \u0026#34;Test\u0026#34; # Error: crun: read status from sync socket: File exists # Test SUCCEEDED outside pod podman run --rm --device nvidia.com/gpu=all \\ ghcr.io/immich-app/immich-machine-learning:v2-cuda echo \u0026#34;Test\u0026#34; # Output: Test This confirmed that CDI GPU devices work perfectly in standalone containers but fail when combined with pod + UserNS=keep-id.\nStep 3: Move ML Container Outside Pod The solution was to run the ML container outside the pod with host networking:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # immich-machine-learning.container [Container] EnvironmentFile=/mnt/server/immich-app/.env Image=ghcr.io/immich-app/immich-machine-learning:v2-cuda Network=host Volume=/mnt/server/immich-app/model-cache:/cache PodmanArgs=--device nvidia.com/gpu=all --security-opt label=disable [Service] Restart=always [Install] WantedBy=default.target 6. Network Connectivity Between Pod and Host Problem Manifestation:\nAfter moving the ML container to host network, the server (still in the pod) couldn\u0026rsquo;t reach it at 127.0.0.1:3003. The containers were now in different network namespaces.\nSolution:\nPodman provides a special hostname host.containers.internal that automatically resolves to the host gateway IP from within pod containers. This hostname remains stable across reboots, unlike manually using gateway IPs like 169.254.1.2.\nTesting from inside the server container:\n1 2 3 4 5 6 7 # Check resolution podman exec systemd-server getent hosts host.containers.internal # Output: 169.254.1.2 host.containers.internal host.docker.internal # Test connectivity podman exec systemd-server curl -s http://host.containers.internal:3003/ping # Output: pong Updated the server configuration:\n1 2 3 4 # server.container [Container] Environment=DB_HOSTNAME=127.0.0.1 REDIS_HOSTNAME=127.0.0.1 IMMICH_MACHINE_LEARNING_URL=http://host.containers.internal:3003 ... After making this change:\n1 2 systemctl --user daemon-reload systemctl --user restart server.service Final Working Configuration After working through all these issues, here\u0026rsquo;s the complete working configuration. All files belong in ~/.config/containers/systemd/.\nPod Definition (immich.pod) 1 2 3 4 5 6 7 [Pod] PodName=systemd-immich PublishPort=2283:2283 UserNS=keep-id [Install] WantedBy=default.target Database Container (immich-database.container) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] After=immich.pod [Container] EnvironmentFile=/mnt/server/immich-app/.env Image=docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0 Pod=immich.pod Volume=/mnt/server/immich-app/postgres:/var/lib/postgresql/data [Service] Restart=always [Install] WantedBy=default.target Redis Container (immich-redis.container) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [Unit] After=immich.pod [Container] Image=docker.io/valkey/valkey:9 Pod=immich.pod User=root Group=root Volume=/mnt/server/immich-app/redis:/data [Service] Restart=always [Install] WantedBy=default.target Server Container (immich-server.container) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [Unit] Requires=redis.service database.service After=redis.service database.service [Container] Environment=DB_HOSTNAME=127.0.0.1 REDIS_HOSTNAME=127.0.0.1 IMMICH_MACHINE_LEARNING_URL=http://host.containers.internal:3003 EnvironmentFile=/mnt/server/immich-app/.env Image=ghcr.io/immich-app/immich-server:v2 Pod=immich.pod PodmanArgs=--security-opt label=disable Volume=/mnt/server/immich-app/server/upload:/data Volume=/etc/localtime:/etc/localtime:ro [Service] Restart=always [Install] WantedBy=default.target Machine Learning Container (immich-machine-learning.container) 1 2 3 4 5 6 7 8 9 10 11 12 [Container] EnvironmentFile=/mnt/server/immich-app/.env Image=ghcr.io/immich-app/immich-machine-learning:v2-cuda Network=host Volume=/mnt/server/immich-app/model-cache:/cache PodmanArgs=--device nvidia.com/gpu=all --security-opt label=disable [Service] Restart=always [Install] WantedBy=default.target Deployment Steps Save unit files: Place all the above files in ~/.config/containers/systemd/ directory.\nCreate data directories:\n1 mkdir -p /mnt/server/immich-app/{postgres,redis,server/upload,model-cache} Reload systemd:\n1 systemctl --user daemon-reload Start services:\n1 2 3 4 5 6 7 8 # Start the pod first systemctl --user start immich.pod # Start individual services (they\u0026#39;ll auto-start with dependencies) systemctl --user start database.service systemctl --user start redis.service systemctl --user start server.service systemctl --user start immich-machine-learning.service Enable auto-start on boot (optional):\n1 2 systemctl --user enable immich.pod database.service redis.service \\ server.service immich-machine-learning.service Check status:\n1 2 systemctl --user status database.service redis.service \\ server.service immich-machine-learning.service Verification and Testing Check All Services are Running 1 2 systemctl --user status database.service redis.service \\ server.service immich-machine-learning.service --no-pager | grep -E \u0026#34;^●|Active:\u0026#34; Expected output showing all services active (running).\nVerify GPU Acceleration is Active Check ONNX Runtime providers:\n1 2 podman exec systemd-immich-machine-learning python -c \\ \u0026#34;import onnxruntime as ort; print(\u0026#39;Available providers:\u0026#39;, ort.get_available_providers())\u0026#34; Expected output:\n1 Available providers: [\u0026#39;TensorrtExecutionProvider\u0026#39;, \u0026#39;CUDAExecutionProvider\u0026#39;, \u0026#39;CPUExecutionProvider\u0026#39;] Check GPU memory usage:\n1 nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv You should see the Python process from the ML container using GPU memory.\nCheck container logs for CUDA provider:\n1 podman logs systemd-immich-machine-learning 2\u0026gt;\u0026amp;1 | grep -i \u0026#34;provider\u0026#34; Expected output:\n1 INFO Setting execution providers to [\u0026#39;CUDAExecutionProvider\u0026#39;, \u0026#39;CPUExecutionProvider\u0026#39;] Test Web Interface 1 curl -s -o /dev/null -w \u0026#34;%{http_code}\\n\u0026#34; http://127.0.0.1:2283 Expected: 200\nNow you can access Immich at http://localhost:2283 in your browser!\nKey Learnings CDI vs Device Mounts: Use CDI (--device nvidia.com/gpu=all) instead of explicit device mounts for cleaner GPU access.\nUserNS + CDI Incompatibility: Rootless pods with UserNS=keep-id cannot use CDI GPU devices. The workaround is to run GPU containers outside the pod.\nHost Networking for GPU: When GPU containers need to be outside the pod, use Network=host for simplicity.\nCross-Network Communication: Use host.containers.internal for pod containers to reach host network services. This hostname is stable across reboots.\nSELinux Considerations: For rootless containers with existing data volumes, use --security-opt label=disable instead of :Z flag.\nRedis Permissions: Valkey/Redis in rootless containers may need User=root and Group=root to access data directories.\nsystemd-reload Required: Always run systemctl --user daemon-reload after editing .container files.\nCPU Usage is Normal: ML containers will use significant CPU (20-30%) even with GPU acceleration, as CPU handles preprocessing, data transfer, and orchestration.\nTroubleshooting Commands 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Check service status systemctl --user status \u0026lt;service\u0026gt;.service # View logs journalctl --user -u \u0026lt;service\u0026gt;.service --since \u0026#34;5 minutes ago\u0026#34; # Direct container logs podman logs systemd-\u0026lt;container-name\u0026gt; # Test network connectivity from container podman exec \u0026lt;container-name\u0026gt; curl http://host.containers.internal:3003/ping # Check GPU device mounts podman inspect \u0026lt;container-name\u0026gt; --format \u0026#39;{{.HostConfig.Devices}}\u0026#39; # Monitor resource usage podman stats --no-stream References Nite07\u0026rsquo;s Podman Guide - Redis user/group solution Immich Documentation Podman Quadlet Documentation NVIDIA Container Device Interface Conclusion Running Immich with GPU acceleration on Podman requires careful consideration of network namespaces, user namespaces, and device access. The key insight is that GPU containers must run outside pods when using UserNS=keep-id, with cross-network communication handled via host.containers.internal.\nThe final setup provides:\nFull GPU acceleration for ML workloads Rootless containers for security Systemd integration for automatic startup Proper service dependencies Persistent configuration across reboots ","date":"2026-02-01T05:34:00Z","image":"https://blog.sparker0i.me/immich-gpu-acceleration-podman/immich_hu_5fd69f8afa43cbaf.jpg","permalink":"https://blog.sparker0i.me/immich-gpu-acceleration-podman/","title":"Running Immich with GPU Acceleration on Fedora: A Podman Quadlet Journey"},{"content":"Eighteen years. Eighteen long years of heartbreak, near misses and unwavering hope. As I sat on the stands at the NaMo stadium on that fine evening of June 3rd, 2025, I never imagined I would witness history in the making. RCB had finally done it - they had won their first ever IPL title.\nThe burden of the past For those of us who have followed RCB through the highs and lows, this moment felt surreal. Three times we had reached the final - 2009, 2011 and 2016 - and all the three times we had to watch our dreams slip away. The pain of those defeats, especially the heartbreaking loss in 2016 when Virat Kohli had played the greatest individual IPL season, still lingered in our hearts.\nBut it wasn\u0026rsquo;t just the final defeats that tested our faith. The dark years of 2017-19 were perhaps the most challenging period in RCB\u0026rsquo;s history. Finishing at the bottom in 2017 and scoring the league\u0026rsquo;s lowest total, sixth in 2018 and last again in 2019 where we missed out on qualification thanks to one no-ball - those were seasons that truly tested what it meant to be an RCB fan. Watching the team struggle to even qualify for the playoffs, seeing majority of the team underperform and enduring the constant criticism and memes about our franchise was heartbreaking.\nBut here we were again, in another final, this time against PBKS - another franchise part of the holy trinity desperately seeking their maiden title.\nA match for the ages The atmosphere outside and inside the Narendra Modi Stadium was electric from the moment I arrived. The entry points were a sea of red and gold, with RCB fans vastly outnumbering Punjab Kings supporters. Despite being technically a neutral venue, it felt like a home game for us. Every gate, every corridor, every section of the stadium was dominated by RCB jerseys, flags, and chants. The \u0026ldquo;RCB! RCB!\u0026rdquo; echoes were deafening even before the players took the field.\nWalking through the stadium concourses, you could see families who had traveled from across the country, groups of friends who had planned this trip for months, and die-hard fans who had followed RCB through every heartbreak. The energy was infectious – strangers were sharing stories of their RCB journey, kids were getting their faces painted in red and gold, and everyone seemed to carry that mix of nervous excitement and desperate hope that only comes with supporting a team that has broken your heart before.\nWhen Punjab won the toss and chose to field first, there was pindrop silence in the stadium, with all of us losing hope. The pressure was on our boys to set a defendable total on what appeared to be a tricky Ahmedabad surface.\nWatching Kohli walk out to bat, you could sense the weight of expectations on his shoulders. This was the man who had given his youth, prime and experience to RCB; who had stayed loyal when others might have moved on, who had scored mountains of runs but never lifted the trophy that mattered most to him and to us.\nThe innings unfolded like a masterclass in T20 batting under pressure. Kohli anchored the innings with a crucial 43 off 35 balls, playing the role he has perfected over the years - being the calm in the storm. But it was the collective effort that stood out. Livingstone\u0026rsquo;s quickfire 25, Jitesh\u0026rsquo;s explosive 24 off 10 balls, and crucially Romario\u0026rsquo;s late cameo of 17 off 9 pushed us to 190/9.\nFrom the stands, every wicket brought in silence, every run started feeling precious, every boundary was celebrated like a victory in itself. When Romario brought in the blitzkreig, the entire RCB fanbase erupted. 190 felt like a lost cause yet again, but in T20 cricket you never know when things turn to your side.\nThe defense If our batting had been about building pressure and seizing moments, our bowling was about holding our nerves and executing under ultimate pressure, something which we\u0026rsquo;ve consistently failed to do over the years after qualifying for playoffs. Punjab started well through Priyansh Arya and Prabhsimran Singh, and for a moment, that familiar feeling of dread began creeping in.\nBut then Hazlewood struck the first blow, and the game shifted. Watching Krunal Pandya bowl his heart out was poetry in motion - this was a man who had a habit of winning IPL titles with his clutch performances, and he brought all that experience to bear when it mattered the most. His figures of 2/17 in 4 overs were the backbone of our defense.\nThe tension in the stadium was palpable. Every dot ball was cheered, every wicket celebrated like we had already won. When Romario dismissed the opposition captain, you could sense the momentum shift decisively in our favour. Every wicket that fell after that strengthened our belief too.\nIn T20 cricket it\u0026rsquo;s never over until it\u0026rsquo;s over. Even with Punjab needing 29 off the final over, the match wasn\u0026rsquo;t dead yet. The monstrous Shashank Singh was still out there playing brilliantly. But Hazlewood started the final over with two crucial dot balls, which mathematically all but sealed the contest. All he had to do was not bowl a no-ball. Then Shashank exploded, hitting 6, 4, 6, 6 off the final four balls, but those magnificent strikes came just a touch too late.\nWhen the final ball was bowled and we had won by 6 runs, the noise in the stadium was deafening.\nThat Moment I\u0026rsquo;ll never forget the image of Virat Kohli falling to his knees, cupping his face in his hands, tears streaming down his cheeks. This was a man who had carried the hopes and dreams of millions of RCB fans on his shoulders for over a decade. To see him finally achieve what he had worked so tirelessly for was deeply emotional.\nThe celebrations on the field were pure euphoria. Players were running in all directions, some crying, others roaring with joy. The support staff who had worked tirelessly behind the scenes were finally getting their moment in the sun. Coaches, physiotherapists, analysts – everyone who had been part of this incredible journey was soaking in the magnitude of what they had just achieved.\nAfter the final ball was bowled and victory was confirmed, something magical happened in the stadium. Despite the late hour, the majority of RCB fans stayed rooted to their seats. Not a single person moved an inch. We all wanted to witness the trophy presentation with our own eyes, to see our heroes finally lift that golden prize we had dreamed about for so long. The collective refusal to leave was a testament to how much this moment meant to every single one of us.\nAround me in the stands, grown men were crying. Strangers were hugging each other. Flags were waving. The 18-year wait was over.\nWhat This Means This title changes the story of RCB. On paper the match reads like a thriller: Royal Challengers Bengaluru posted 190 for 9, and Punjab Kings finished on 184 for 7 - RCB winners by six runs in the final. That number - six runs - will live in RCB folklore forever. It rewrites the narrative from \u0026ldquo;nearly\u0026rdquo; to \u0026ldquo;champions\u0026rdquo;, gives Virat Kohli a chapter he’s sought for years, and gives the fans a memory to bring out on any gloomy evening. It’s a reminder of why we follow sport at all - for the small, ordinary build-ups that, once in a while, explode into something unforgettable.\nFor players like Krunal Pandya, Bhuvneshwar Kumar, and Yash Dayal, it was another title to add to their collection, but one that felt special because of the journey. For Rajat Patidar, it was the perfect start to his captaincy journey – lifting the IPL trophy in his very first season as skipper, a feat that will be remembered for years to come. His tactical acumen throughout the tournament and calm demeanor in pressure situations were crucial to this victory. To win the IPL in your debut season as captain is the stuff of dreams, and Patidar executed it perfectly.\nBut what made the celebration even more special was seeing the RCB legends who had joined us for this historic moment. Chris Gayle, the Universe Boss who had almost single-handedly carried us to the 2011 final with his explosive batting, was there on the field celebrating with the team. And AB de Villiers – Mr. 360, the man who had redefined T20 batting during his time with RCB – was also there to lift the trophy alongside the current squad. Seeing these icons, who had given their all for RCB during their playing days, finally get to hold that elusive IPL trophy was incredibly moving.\nFor us fans, it was the end of a long, emotional journey and the beginning of a new chapter. It\u0026rsquo;s redemption for a franchise that had been labeled \u0026ldquo;chokers\u0026rdquo; and \u0026ldquo;unlucky\u0026rdquo;. It\u0026rsquo;s proof that loyalty and persistence eventually pay off.\nA Celebration Turned Tragic But as we basked in the glory of our historic victory, tragedy struck the very next day. On June 4th, during RCB\u0026rsquo;s victory celebration and felicitation ceremony at the M. Chinnaswamy Stadium, a stampede occurred that claimed 11 lives and injured at least 50 others. What was meant to be a joyous homecoming for our champions turned into an unthinkable tragedy when almost 250k fans gathered outside the stadium, far exceeding the venue\u0026rsquo;s capacity.\nAs I sat in Ahmedabad airport waiting for my flight back to Bangalore, watching the news unfold on my phone, I was filled with a mixture of horror and a chilling realization – had I been in Bangalore that day, I would have undoubtedly joined those crowds outside Chinnaswamy, risking everything just to be part of the celebration. The thought of what could have happened sent shivers down my spine.\nThe scenes were heartbreaking – fans trying to climb boundary walls, exhaustion and trauma spreading through the crowds, emergency wards working frantically to save lives. RCB had announced compensation for the families of the deceased and pledged to cover medical expenses for the injured, but no amount of money could heal the pain of losing loved ones during what should have been a celebration.\nThis tragedy served as a stark reminder that our passion for cricket, while beautiful, must always be tempered with safety and proper planning. The joy of RCB\u0026rsquo;s victory will forever be tinged with sorrow for those families who lost their loved ones. Their memory will always be part of RCB\u0026rsquo;s story. RCB must learn from this tragedy and do something meaningful to ensure it never happens again. The fans who lost their lives were celebrating the very victory we had all waited 18 years for - they deserved better, and their families deserve more than just our prayers.\nLooking Back, Moving Forward As I walked out of the Narendra Modi Stadium that night, still in disbelief, I thought about all the RCB fans over the years who didn\u0026rsquo;t live to see this day. I thought about the heartbreaks we had endured together, the hope we had carried through season after season of disappointment.\nAnd then it hit me – my birthday was just two days away, on June 5th. What better birthday gift could I have asked for than witnessing RCB\u0026rsquo;s maiden IPL victory? After years of birthday wishes that always included \u0026ldquo;Please let RCB win the IPL this year,\u0026rdquo; the universe had finally delivered the ultimate present, two days early no less!\nBut mostly, I thought about how beautiful this game is – how it can break your heart and mend it again, how it can test your faith and then reward it in the most spectacular fashion.\nThe Royal Challengers Bengaluru are finally IPL champions. The wait is over. The dream has come true.\nAnd I was there to witness it all.\nEe Sala Cup Namdu – This Year, the Cup is now ours. Finally.\n","date":"2025-08-09T16:50:16Z","image":"https://blog.sparker0i.me/rcb-historic-ipl-2025-victory/rcb-trio_hu_12c9e14e559183c5.jpeg","permalink":"https://blog.sparker0i.me/rcb-historic-ipl-2025-victory/","title":"The wait is over: Witnessing RCB's Historic IPL 2025 Victory"},{"content":"Earlier this year, my gaming laptop started to show its signs of aging. Games constantly froze, laptop kept getting hotter on idle use and graphics and animations were slower than ever before. Thus I had decided to build a gaming PC for myself.\nBut I still needed a laptop for when I\u0026rsquo;d be traveling. Most of the latest laptops available in the Indian market were that of Intel. Given the spate of issues that have hit Intel chips in the last few years, I did not want to go for them. The ones having the AMD chip were all gaming laptops and were too expensive. Snapdragon X based laptops are not quite ready yet for the full development lifecycle.\nI then realized I could do all of my development online. Which then drove me to the iPad. The iPad Air with the M3 chip and 256GB storage was available at Rs. 80k and was far cheaper than the other laptops I\u0026rsquo;d been looking at. The iPad combined with a Magic Keyboard was all that I needed to properly code on the go.\nMy Coding Combination GitHub Codespaces became my primary development environment. The biggest gotcha? iPadOS keyboard shortcuts. They\u0026rsquo;re locked down tighter than a production database, so the usual Cmd+T for new tabs simply opens a new Safari tab instead of a terminal tab in VS Code.\nMy workaround: Open the codespace, then \u0026ldquo;Add to Home Screen\u0026rdquo; via Safari\u0026rsquo;s share menu. This creates a pseudo-native PWA app that bypasses some shortcut restrictions. You can finally remap Option-based shortcuts to something sensible like Option+W for closing tabs and Option+Tab for switching between them.\nThe muscle memory adjustment took about two weeks, but now I\u0026rsquo;m flying through code again.\nBeyond the basic setup, I\u0026rsquo;m running a 2-core, 8GB RAM codespace instance that costs roughly $0.36/hour. For most days, the free 120 hours covers everything, but intense project weeks can push me into paid territory.\nWhat Nobody Tells You About iPad Development The good stuff that surprised me The M3 chip is genuinely overkill for development work. I can have 20+ Safari tabs open, multiple codespace instances, Slack, Figma, and a Zoom call running simultaneously without any thermal throttling. My old Intel laptop would have sounded like a jet engine by now.\nBattery life is incredible. I coded for 6 hours straight during a recent flight to north India, watched a web series episode, and still had 20% charge left. Try that with any x86 laptop under Rs. 1.5 lakhs.\nThe painful realities Internet dependency is brutal. During a power outage that killed my Wi-Fi for 3 hours, the iPad became an expensive paperweight. No local Node.js, no Git, no nothing. Laptops with local dev environments would have kept me productive. I then had to rely on the hotspot from my iPhone, but that ended up draining the battery of both the devices.\nFile management feels terrible. Want to drag a CSV from Files into your web app for testing? Prepare for a 5-step dance involving the share sheet, temporary storage, and prayer. The Files app is a beautiful lie that promises desktop-like file handling but delivers mobile-first friction. However things might change with the upcoming iPadOS 26 release.\nReal Performance Testing I stress-tested this setup during a weekend trying to do multiple things:\nMigrate this blog from Hashnode to Hugo Learn about LLMs by building an app Build a webserver in Go from scratch All of the above were on three different codespaces on GitHub. Here\u0026rsquo;s what came out of that weekend:\nDay 1: Smooth sailing. Hot reloading worked perfectly on Hugo, additional go module installs were fast thanks to codespace\u0026rsquo;s fiber connection, and the iPad stayed cool despite 12 hours of continuous use. Day 2: Discovered the multi-tasking limitations. Switching between the codespaces, go documentation, and Discord required constant app switching. Unlike Windows or Mac, the switching between app animations takes ~2 seconds to complete. This definitely frustrated me. Cost Reality Check My iPad Air M3 (256GB) set me back ₹80,000 and the Magic Keyboard cost another ₹27,000, totalling ₹1,07,000. Compare this to a MacBook Air M3 at ₹1,15,000 or a decent Windows laptop at ₹80,000-1,20,000. Unfortunately the iPad falls flat in front of the MacBook Air thanks to its positioning in premium pricing territory while delivering close to no benefits over the MacBook.\nWhen iPad Development Actually Makes Sense You\u0026rsquo;re a good candidate if you satisfy some of these below points:\nYour codebase lives in GitHub with proper CI/CD setup through GitHub Actions You primarily work with web technologies (React, Vue, Node.js, Python web apps) You have reliable high-speed internet everywhere you code You value portability over raw performance You\u0026rsquo;re comfortable with cloud-first workflows If that doesn\u0026rsquo;t sound like you, then you should stick to a laptop if:\nYou do native iOS/Android development (yes, the irony) Your workflow involves Docker, VMs, or local databases You frequently work offline or with spotty internet You need multiple monitors for productivity You rely heavily on platform-specific tools Three months later: The honest verdict This experiment succeeded, but with important caveats. The iPad handles 80% of my development tasks beautifully. The remaining 20% – complex debugging, performance profiling, multi-service local development – still sends me back to my desktop.\nWhat I miss most: Terminal muscle memory. Web-based terminals are good, but they\u0026rsquo;re not native. Keyboard shortcuts feel slightly off, copy-paste behavior is inconsistent, and there\u0026rsquo;s always a tiny input lag that breaks flow state.\nWhat I appreciate most: Zero thermal throttling, all-day battery, and the ability to code comfortably in coffee shops without looking like I\u0026rsquo;m running a crypto mining operation.\nThe iPad won\u0026rsquo;t replace serious development machines yet, but it\u0026rsquo;s redefined what \u0026ldquo;portable coding\u0026rdquo; means for me. When Apple inevitably allows proper terminal emulators and fixes the multitasking limitations, this setup will become genuinely compelling for a broader range of developers.\nThe Bottom Line If you\u0026rsquo;re expecting a laptop replacement, you\u0026rsquo;ll be disappointed. If you\u0026rsquo;re looking for a capable secondary development device that happens to be the best tablet on the market, the iPad delivers.\nThe future of development is increasingly cloud-native, and the iPad offers a glimpse into that future today. It\u0026rsquo;s not perfect, but it\u0026rsquo;s far more capable than I expected when I started this experiment.\nWould I buy it again? Absolutely. But I\u0026rsquo;d also keep a proper development machine for the heavy lifting.\n","date":"2025-07-19T17:04:00Z","image":"https://blog.sparker0i.me/tried-ipad-as-development-machine/ipad-header_hu_cf21576118ed2bb9.jpeg","permalink":"https://blog.sparker0i.me/tried-ipad-as-development-machine/","title":"I tried the iPad as a development machine. Here's how that went"},{"content":"The Software Architecture landscape has often been dominated by two major choices: monolithic applications for simplicity vs microservices for scalability. This has forced teams to make major architectural calls, often sometimes forcing compromises that don\u0026rsquo;t fully address their needs. However, there may be another path that is gaining traction: Self-Contained Systems (SCS).\nSelf-Contained Systems represent an architectural pattern that combines the best aspects of both monoliths and microservices while addressing their key limitations. Unlike a typical microservices approach which focuses primarily on decomposing into multiple backend services, SCS embraces a full-stack philosophy where each system/team owns its complete user experience from start to finish.\nThis post will try to examine why SCS might be the sweet spot in your architecture, providing detailed comparison, practical frameworks and real-world insights to guide your architectural decisions.\nUnderstanding Self-Contained Systems Defining Self-Contained Systems A Self-Contained System is an autonomous, full-stack application that:\nOwns its complete stack: From the database schemas to the user interfaces\nImplements a specific business requirement: Focused on a bounded context\nOperates Independently: Can function without other systems\nCommunicates asynchronously: Primarily through events and data replication\nMaintains its own deployment lifecycle: Independent versioning and releases\nCore Principles of SCS Autonomy: Each SCS can be developed, tested, deployed and operated independently by a dedicated team. This autonomy extends beyond just the backend services to include the complete user experience.\nBusiness Alignment: Systems are organized around business capabilities rather than technical layers. For example, an e-commerce SCS might own everything related to Product catalog - the DB Schemas, business logic, APIs and web pages for browsing and searching through products.\nFull-Stack Ownership: Unlike microservices that often share frontend applications, each SCS owns its portion of the user interface. This eliminates coordination overhead and enables true end-to-end ownership.\nLoose Coupling: SCS instances communicate primarily through async mechanisms like events, message queues or data replication. Synchronous calls are minimized as much as possible and carefully managed.\nTechnology diversity: Teams can choose the most appropriate tech stack for their specific domain without being constrained by org wide standards.\nSCS vs Traditional Microservices While microservices focus on decomposing the backend into smaller services while often sharing the frontend, SCS takes a more holistic approach, decomposing the entire application stack - including UI - into independent systems.\nThis fundamental difference has profound implications for team structure, development process, and operational complexity.\nThe Monolith: Strengths and Limitations Monolith Strengths Simplicity: Monoliths offer simpler development, testing and deployment processes. Developers work within a single codebase with familiar patterns and tools.\nPerformance: In-process communication eliminates network latency. Database transactions can maintain ACID properties across the entire application.\nDebugging and Monitoring: Centralized logging, monitoring, and debugging provide clear visibility into an application\u0026rsquo;s behaviour.\nConsistency: Shared Libraries, coding standards and architectural patterns ensure consistency throughout the application.\nInitial Development Speed: Small teams can move quickly without the overhead of distributed system complexity.\nMonolithic Limitations Scalability constraints: The entire app must be scaled as a unit, which may lead to resource wasting and scaling bottlenecks if not utilized properly.\nTechnology Lock-In: Since the entire application becomes bound to a single technology stack, it may limit innovation opportunities.\nTeam Coordination Overhead: As teams grow, coordination becomes increasingly complex.\nDeployment Risk: Changes to any part of the application requires deploying the entire monolith, increasing the blast radius for any potential failures.\nLimited Fault Isolation: A failure in any single component may bring down the entire application.\nMicroservices: Promise and Pitfalls Microservices Strengths Independent Scalability: Services can be scaled independently based on their specific load patterns and resource requirements.\nTechnology Diversity: Teams can choose the most appropriate tech stack for each service\u0026rsquo;s requirements.\nFault Isolation: Failure in one service doesn\u0026rsquo;t necessarily cascade to others, thereby improving overall system resilience.\nTeam Autonomy: Small, focused teams can own and operate individual services with minimal coordination.\nCI/CD: Services can be deployed independently, enabling faster release cycles and reduced deployment risk.\nMicroservices Pitfalls Operational Complexity: Managing dozens or hundreds of services requires sophisticated tooling for deployment, monitoring, logging and debugging. This may end up being difficult to understand, maintain and coordinate.\nNetwork Complexity: Service-to-Service communication introduces latency, failure modes and the need for specific design patterns for microservices like circuit breakers and retry/resubmit mechanisms.\nFrontend Coordination: While backend systems are decomposed, frontends often end up remaining monolithic, creating coordination bottlenecks and deployment dependencies.\nTesting Complexity: Integration Testing across Multiple Services is significantly more complex than testing a single application.\nDistributed System Fallacies: Teams often underestimate the challenges of distributed systems leading to issues with network reliability, latency and security.\nDetailed Architectural Comparison Development Complexity Monolith: Low complexity for small teams and simple applications. Complexity increases dramatically as the codebase grows.\nMicroservices: High Complexity from day 1. Requires expertise in distributed systems, container orchestration and service discovery.\nSCS: Moderate Complexity. More complex than Monoliths but significantly less complex than fine-grained microservices. Complexity remains manageable as the system grows.\nOperational Overhead Monolith: Minimal operational overhead. Single deployment, monitoring and logging.\nMicroservices: High operational overhead. Requires sophisticated tooling for container orchestration, service mesh, distributed tracing and centralized logging.\nSCS: Moderate operational Overhead. Fewer systems to manage than microservices, while also providing better isolation than monoliths.\nTechnology Flexibility Monolith: Limited to a single stack across the entire application\nMicroservices: Maximum technology flexibility, but can lead to operational complexity and knowledge fragmentation.\nSCS: Good technology flexibility with natural boundaries.\nScalability Patterns Monolith: Scales the entire application as a unit. Simple, but potentially wasteful blocking up resources it doesn\u0026rsquo;t need.\nMicroservices: Fine grained scalability for individual services. Highly Optimized, but complex.\nSCS: Coarse grained scalability for business capabilities. Good balance between simplicity and optimization.\nDecision Matrix to pick between the styles Factor Monolith Microservices SCS Domain Complexity Simple Complex Moderate Scalability Requirements Low-Medium Extremely High Medium-High Time to Market Fast Slow Medium Technology Diversity None Maximum High Development Speed Initially Fast Slow Medium Testing/Debugging Complexity Low High Medium Deployment Complexity Low High Medium Fault Isolation Poor Excellent Good Learning Curve Low High Medium Real World Examples Zalando Zalando re-architected its e-commerce platform around autonomous teams and micro-frontends, stitching UI fragments together via their Project Mosaic framework. Each feature team deploys independent UI blocks backed by dedicated services and data stores, accelerating experiments and reducing cross-team impacts.\nLink 1, Link 2\nBreuninger Breuninger is a leading German fashion and lifestyle retailer with a strong online presence. The company needed to optimize its e-commerce platform to respond more quickly to market demands and customer expectations. Hence they adopted SCS architecture, which allowed independent and rapid development and deployment of new features tailored to the customer journey. This approach improved time-to-market for new features, increased platform stability, and enhanced customer satisfaction through a more optimized shopping experience.\nLink\nKühne+Nagel Kühne+Nagel is a global logistics company. The company needed to break down its monolithic software to improve agility and maintainability. Kühne+Nagel adopted SCS (within the context of logistics), comparing it to microservices and choosing SCS for its clearer boundaries and team ownership. The transition enabled more independent team operations and simplified the management of complex logistics workflows. SCS offered a practical alternative to both monoliths and microservices for large, distributed organizations.\nLink\nA Personal Anecdote In my previous role, I had the opportunity to work closely with a team that developed a comprehensive PaaS offering from scratch - a product that exemplified the SCS principles in action. The platform consisted of three very distinct Self-Contained Systems, each addressing a specific business capability. All three systems had their own tech stack for frontend (2x React, 1x Angular), backend (2x Go, 1x Python) and database (2x MongoDB, 1x DB2) integrated with a central authentication API (Go+MongoDB) that handled user login and session management. However, each of these SCS independently managed authorization within their respective areas using the principles of RBAC.\nConclusion Self-Contained Systems offer a compelling alternative to the traditional monolith versus microservices debate. By focusing on business-aligned, full-stack ownership, SCS provides many of the benefits of microservices while avoiding much of their complexity.\nThe key insights for architectural decision-making are:\nStart with Team Structure: Organize your architecture around how you want to organize your teams. SCS works best when teams can own complete business capabilities.\nEmbrace Business Alignment: Technical boundaries should reflect business boundaries. This makes systems easier to understand, maintain, and evolve.\nBalance Complexity: SCS provides a sweet spot between monolithic simplicity and microservices flexibility. Choose the approach that matches your organization\u0026rsquo;s current capabilities and growth trajectory.\nFocus on Autonomy: The primary benefit of SCS is team autonomy. Optimize for reducing coordination overhead while maintaining necessary integration points.\nEvolve Gradually: Architecture is not a destination but a journey. Start with what works for your current situation and evolve as your organization and requirements change.\nThe future of software architecture likely includes all three approaches - monoliths for simple applications, fine-grained microservices for high-scale optimization, and Self-Contained Systems for the vast middle ground where most applications live. Understanding when and how to apply each approach is the key to successful software architecture in the modern era.\nAs we continue to learn from industry experiences and evolve our understanding of distributed systems, Self-Contained Systems represent a mature approach to organizing software that aligns technical and business concerns while maintaining manageable complexity. For many organizations, SCS may indeed be the architectural pattern they\u0026rsquo;ve been searching for.\n","date":"2025-06-29T08:27:38.006Z","image":"https://blog.sparker0i.me/self-contained-systems-alternate-to-monolith-microservices/scs_hu_321faff4d8feece8.jpg","permalink":"https://blog.sparker0i.me/self-contained-systems-alternate-to-monolith-microservices/","title":"Self Contained Systems: Beyond the Monolith vs Microservices debate"},{"content":"Code is more than just instructions for a machine - it\u0026rsquo;s a form of communication with your future self and teammates. Yet all too often, codebases accumulate hidden \u0026ldquo;stinkers\u0026rdquo; that slow down development, introduce bugs, and frustrate newcomers. These are code smells: surface indicators that something deeper in the design or implementation needs attention. In this post, we\u0026rsquo;ll explore a comprehensive catalog of common smells and walk through concrete refactoring - complete with before/after snippets (in Go, but the logic can be applied to any language)- to help you keep your codebase clean, maintainable, and a joy to work with.\nWhy Code Smells matter? Readability \u0026amp; Onboarding: Long, tangled methods or duplicated logic force readers to mentally untangle intent from implementation. New team members spend hours deciphering what should\u0026rsquo;ve been clear. Bug Rate: Smelly code often has hidden dependencies or unexpected side effects. One small change can ripple out, breaking functionality in multiple places. Confidence to Change: When refactoring feels risky, teams delay improvements, leading to technical debt escalation. Over time, even minor enhancements require heroic effort. Personal Anecdote: During the early stages of my career I inherited a 5k line class handling everything from fetching data then transforming it and loading it somewhere. This class was growing exponentially with every sprint/iteration, and soon risked crossing 10k lines. Every change required a full regression suite and lots of manual testing. After identifying just three key smells - Long Method, Feature Envy, and Primitive Obsession - we applied targeted refactorings that reduced that class to under 1000 lines and we were easily able to extend our codebase.\nBasic Code Smells \u0026amp; Refactorings Long Method What it is: A method or function that spans many lines - often 200+ - and tries to do too much: input validation, business logic, data transformation, persistence, side-effects, and user interaction all in one place.\nWhy it matters\nCognitive Load: Readers must keep many moving parts and intents in mind at once. Hard to Test: You can\u0026rsquo;t isolate pieces easily for unit tests. Change Risk: A tweak for one concern can inadvertently break another. How to spot it\nLook for methods longer than a screen. Multiple comments like // validate, // compute, // persist in one block. Deep nesting of conditionals and loops. Refactoring: Use Extract Method\nIdentify a coherent chunk: it does one logical sub-task. Give it a descriptive name. Replace the chunk with a call to the new method. Example: Before\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func Checkout(cart Cart) { // 1. Validate if len(cart.Items) == 0 { log.Error(\u0026#34;empty cart\u0026#34;); return } // 2. Sum prices sum := 0.0 for _, i := range cart.Items { sum += i.Price * float64(i.Qty) } // 3. Apply discount if cart.Customer.IsVIP { sum *= 0.9 } // 4. Log \u0026amp; persist log.Printf(\u0026#34;Total: %.2f\u0026#34;, sum) db.Save(cart, sum) // 5. Send email emailService.Send(cart.Customer.Email, sum) } Example: After\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func Checkout(cart Cart) { if err := validate(cart); err != nil { log.Error(err); return } total := calculateTotal(cart.Items) total = applyDiscount(total, cart.Customer) finalizeOrder(cart, total) } func validate(cart Cart) error { if len(cart.Items) == 0 { return fmt.Errorf(\u0026#34;cart empty\u0026#34;) } return nil } func calculateTotal(items []Item) float64 { ... } func applyDiscount(total float64, c Customer) float64 { ... } func finalizeOrder(cart Cart, total float64) { log.Printf(\u0026#34;Total: %.2f\u0026#34;, total) db.Save(cart, total) emailService.Send(cart.Customer.Email, total) } Duplicated Code What it is: Slivers of nearly identical logic appear in two or more locations - copy-paste programming.\nWhy it matters\nMaintenance Hell: Fixing a bug requires updating every copy. Divergence: Over time, copies drift apart, hiding inconsistent behavior. How to spot it\nSearch for the same loop, conditional, or calculation in multiple files. In code reviews, ask \u0026ldquo;Have we done this before?\u0026rdquo; Refactoring Options\nPull shared logic into a single helper function. Replace all occurrences with calls to that helper Example: Before\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func CalculateTaxA(order Order) float64 { tax := 0.0 for _, item := range order.Items { tax += item.Price * float64(item.Quantity) * 0.08 } return tax } func CalculateTaxB(invoice Invoice) float64 { tax := 0.0 for _, line := range invoice.Lines { tax += line.UnitPrice * float64(line.Count) * 0.08 } return tax } Example: After\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func calculateTax(subtotal float64) float64 { return subtotal * 0.08 } func CalculateTaxA(order Order) float64 { subtotal := sumPrices(order.Items) return calculateTax(subtotal) } func CalculateTaxB(inv Invoice) float64 { subtotal := sumInvoiceLines(inv.Lines) return calculateTax(subtotal) } // Shared helpers func sumPrices(items []Item) float64 { total := 0.0 for _, i := range items { total += i.Price * float64(i.Quantity) } return total } func sumInvoiceLines(lines []Line) float64 { total := 0.0 for _, l := range lines { total += l.UnitPrice * float64(l.Count) } return total } Long Parameter List What it is: Methods that accept many parameters - often over five - making calls verbose and error-prone.\nWhy it matters\nHard to Remember Order: Callers mix up parameters. Low Cohesion: Signals multiple responsibilities or data clumps. How to spot it\nMethod signatures with more than 4 arguments. Frequent use of null or default values to skip parameters. Refactoring: Introduce Parameter Object\nIdentify parameters that form a logical group. Create a class/struct to hold them. Replace the parameter list with the new object. Example: Before\n1 2 3 func CreateUser(firstName, lastName, email, phone, role string, isActive bool) { // ... } After:\n1 2 3 4 5 6 7 8 9 10 11 12 type CreateUserRequest struct { FirstName string LastName string Email string Phone string Role string IsActive bool } func CreateUser(req CreateUserRequest) { // ... } Divergent Change What it is: A single class or module is edited for many unrelated reasons\u0026ndash;bug fixes, UI tweaks, business-rule updates\u0026ndash;indicating mixed responsibilities.\nWhy it matters\nFragile: Changes for one concern can break another. Violates SRP: Violates the Single Responsibility Principle. How to spot it\nVersion-control history shows one file modified by many tickets of different types. Code reviewers comment: \u0026ldquo;Why are we touching this here?\u0026rdquo; Refactoring\nIdentify distinct responsibilities and extract them into new types. Move methods closer to the data they operate on. Example: Before - one class handles both user validation and reporting logic:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type UserService struct{} func (s *UserService) Validate(u User) error { if u.Email == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;email required\u0026#34;) } // ... many more checks ... return nil } func (s *UserService) GenerateReport(u User) Report { // mixing data access and formatting... return Report{/* ... */} } Example: After - split responsibilities into two classes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type UserValidator struct{} func (v *UserValidator) Validate(u User) error { if u.Email == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;email required\u0026#34;) } // ... other checks ... return nil } type UserReportService struct{} func (r *UserReportService) Generate(u User) Report { report := Report{/* ... focused report generation... */} return report } Feature Envy What it is: A method in one type heavily accesses fields or methods of another type - more than its own - indicating misplaced behavior.\nWhy it matters\nTight Coupling: Class A becomes tightly coupled to B\u0026rsquo;s internals. Poor Encapsulation: Behavior isn\u0026rsquo;t located where the data resides. How to spot it\nA method\u0026rsquo;s code reads like b.getX(), b.getY(), b.computeZ() repeatedly. Refactoring\nMove Method: Shift the method into Class B. Extract Method: If only part of the code envies B, extract that fragment into a helper on B. Example: Before - a method in InvoiceService reaching into Order internals:\n1 2 3 4 5 6 7 8 9 10 type InvoiceService struct{} func (s *InvoiceService) TotalWithTax(o Order) float64 { sum := 0.0 for _, item := range o.Items { sum += item.Price * float64(item.Quantity) } rate := o.Customer.State.TaxRate return sum * (1 + rate) } Example: After - move the logic into Order:\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (o Order) TotalWithTax() float64 { sum := 0.0 for _, item := range o.Items { sum += item.Price * float64(item.Quantity) } return sum * (1 + o.Customer.State.TaxRate) } type InvoiceService struct{} func (s *InvoiceService) TotalWithTax(o Order) float64 { return o.TotalWithTax() } Shotgun Surgery What it is: A small change requires edits in many different places - scattered across classes or modules.\nWhy it matters\nError-Prone: Easy to miss one location. Discourages Change: Teams avoid improvements. How to spot it: Search-and-replace touches dozens of files for a single concept.\nRefactoring\nIntroduce Facade: Centralize calls behind one interface. Move Related Behavior: Co-locate methods and data so that a change is Example: Before\n1 2 3 4 5 func OnboardUser(userID string) { data := db.FetchUserData(userID) processed := processor.Process(data) notifier.SendWelcome(userID, processed.Summary) } Example: After\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type OnboardingFacade struct { DB Database Processor Processor Notifier Notifier } func (f *OnboardingFacade) Onboard(userID string) { data := f.DB.FetchUserData(userID) summary := f.Processor.Process(data).Summary f.Notifier.SendWelcome(userID, summary) } // Client: facade := OnboardingFacade{db, processor, notifier} facade.Onboard(\u0026#34;user123\u0026#34;) Primitive Obsession What it is: Overusing built-in types (String, int, bool) for domain concepts instead of small dedicated classes or value objects.\nWhy it matters\nScattering of Validation: Every user-input string is validated in ad-hoc ways. Duplication: Parsing and formatting logic repeated. How to spot it\nMethod signatures full of String parameters representing distinct concepts (e.g., email, phone, address). Refactoring\nReplace Primitive with Value Object: Create classes like EmailAddress, PhoneNumber that encapsulate format checks and domain logic. Example: Before\n1 2 3 4 func ConvertAmount(amount float64, fromCurrency, toCurrency string) float64 { rate := lookupRate(fromCurrency, toCurrency) return amount * rate } Example: After\n1 2 3 4 5 6 7 8 9 type Currency string func (c Currency) RateTo(other Currency) float64 { return lookupRate(string(c), string(other)) } func ConvertAmount(amount float64, from, to Currency) float64 { return amount * from.RateTo(to) } Replace Temp with Query What it is: Temporary variables hold intermediate calculation results, cluttering the method body.\nWhy it matters\nReadability: Readers must track variables and their transformations. Duplication: The same calculation might reappear elsewhere. How to spot it\nSequences like temp = expr; \u0026hellip; use temp; \u0026hellip; modify temp. Refactoring\nEncapsulate the expression in a well-named query method, then call it directly. Example: Before\n1 2 3 4 5 6 7 8 func FinalPrice(o Order) float64 { basePrice := float64(o.Quantity) * o.UnitPrice discount := 0.0 if basePrice \u0026gt; 1000 { discount = basePrice * 0.05 } return basePrice - discount } Example: After\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func FinalPrice(o Order) float64 { return calculateBase(o) - calculateDiscount(o) } func calculateBase(o Order) float64 { return float64(o.Quantity) * o.UnitPrice } func calculateDiscount(o Order) float64 { base := calculateBase(o) if base \u0026gt; 1000 { return base * 0.05 } return 0 } Replace Conditional with Polymorphism What it is: Extensive if-else or switch blocks that dispatch based on type codes or flags.\nWhy it matters\nOpen/Closed Violation: Every time you add a new type, you modify the conditional. Readability: Logic spread across a tangled conditional. How to spot it\nLarge switch(order.type) { case A:\u0026hellip;; case B:\u0026hellip; } constructs. Refactoring\nStrategy Pattern or Class Hierarchy: Define a common interface and let each subtype implement its behavior. Example: Before\n1 2 3 4 5 6 7 8 9 func SendNotification(u User, method string) { if method == \u0026#34;email\u0026#34; { sendEmail(u.Email, \u0026#34;Hello!\u0026#34;) } else if method == \u0026#34;sms\u0026#34; { sendSMS(u.Phone, \u0026#34;Hello!\u0026#34;) } else if method == \u0026#34;push\u0026#34; { sendPush(u.DeviceToken, \u0026#34;Hello!\u0026#34;) } } Example: After\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type Notifier interface { Notify(User) } type EmailNotifier struct{} func (EmailNotifier) Notify(u User) { sendEmail(u.Email, \u0026#34;Hello!\u0026#34;) } type SMSNotifier struct{} func (SMSNotifier) Notify(u User) { sendSMS(u.Phone, \u0026#34;Hello!\u0026#34;) } type PushNotifier struct{} func (PushNotifier) Notify(u User) { sendPush(u.DeviceToken, \u0026#34;Hello!\u0026#34;) } var notifierMap = map[string]Notifier{ \u0026#34;email\u0026#34;: EmailNotifier{}, \u0026#34;sms\u0026#34;: SMSNotifier{}, \u0026#34;push\u0026#34;: PushNotifier{}, } func SendNotification(u User, method string) { if n, ok := notifierMap[method]; ok { n.Notify(u) } } Data Clumps What it is: Groups of variables that always appear together\u0026ndash;e.g., x, y, z coordinates, street, city, zip.\nWhy it matters\nDuplication: Same parameter list repeated. Cohesion: Related data isn\u0026rsquo;t grouped. How to spot it\nMethod after method accepting the same set of parameters. Refactoring\nIntroduce Parameter Object or Data Class to bundle related fields into one type. Example: Before\n1 2 3 4 5 6 func DrawLine(x1, y1, x2, y2 float64, color string) { ctx.SetStrokeColor(color) ctx.MoveTo(x1, y1) ctx.LineTo(x2, y2) ctx.Stroke() } Example: After\n1 2 3 4 5 6 7 8 9 10 11 type Point struct{ X, Y float64 } func DrawLine(p1, p2 Point, color string) { ctx.SetStrokeColor(color) ctx.MoveTo(p1.X, p1.Y) ctx.LineTo(p2.X, p2.Y) ctx.Stroke() } // Usage DrawLine(Point{10, 20}, Point{30, 40}, \u0026#34;red\u0026#34;) General Refactoring Best Practices Test Coverage First: Ensure you have reliable unit and integration tests before refactoring. Tiny, Safe Steps: Change one thing at a time; run tests after each change to catch regressions early. IDE Assistance: Leverage built-in refactorings - Extract Method, Rename, Inline - to minimize manual edits. Code Reviews with Smell Checks: Add \u0026ldquo;smell spotting\u0026rdquo; to your review checklist to catch issues collaboratively. Continuous Refactoring: Make cleanup part of your regular workflow and Definition of Done, not a separate \u0026ldquo;cleanup sprint.\u0026rdquo; Conclusion Code smells are inevitable, but they need not become insurmountable technical debt. By recognizing these ten patterns and applying targeted refactorings, you\u0026rsquo;ll cultivate a codebase that\u0026rsquo;s more understandable, safer to change, and more enjoyable for your entire team. Start small, build momentum with quick wins like Extract Method, then tackle more advanced refactorings. Your future self (and teammates) will thank you.\n","date":"2025-05-12T07:43:20.58Z","image":"https://blog.sparker0i.me/identify-and-refactor-code-smells-for-better-code-quality/6821a6985006bd988f7f9344_hu_d55f4680dbcf248c.png","permalink":"https://blog.sparker0i.me/identify-and-refactor-code-smells-for-better-code-quality/","title":"How to Identify and Refactor Code Smells for Better Code Quality"},{"content":"Imagine you\u0026rsquo;re building furniture. You could design each piece from scratch, figuring out how legs connect to tabletops or how drawers slide in and out. Or, you could use tried-and-tested blueprints that woodworkers have refined over generations.\nIn software development, design patterns are these blueprints. They represent elegant solutions to common coding problems that developers have encountered repeatedly over decades. And whether you\u0026rsquo;re just starting your coding journey or you\u0026rsquo;re a seasoned developer, understanding these patterns can dramatically improve your programming skills.\nWhat Are Design Patterns, Really? At their core, design patterns are reusable solutions to problems that occur frequently in software design. Think of them as templates that can be applied to different situations, saving you from reinventing the wheel each time you face a familiar challenge.\nThe concept was popularized in 1994 when four authors (often called the \u0026ldquo;Gang of Four\u0026rdquo; or GoF) published the book \u0026ldquo;Design Patterns: Elements of Reusable Object-Oriented Software.\u0026rdquo; This landmark text identified 23 patterns that addressed common problems in object-oriented programming.\nThe Three Categories of Design Patterns Design patterns generally fall into three categories, each addressing a different aspect of software design:\n1. Creational Patterns These patterns deal with object creation mechanisms, trying to create objects in a manner suitable to the situation.\nExample: The Factory Pattern\nImagine you\u0026rsquo;re developing a game with different character types (warrior, mage, archer). Instead of writing code like this everywhere:\n1 2 3 4 5 6 7 8 Character character; if (type.equals(\u0026#34;warrior\u0026#34;)) { character = new Warrior(); } else if(type.equals(\u0026#34;mage\u0026#34;)) { character = new Mage(); } else if(type.equals(\u0026#34;archer\u0026#34;)) { character = new Archer(); } You could use a Factory pattern:\n1 Character character = CharacterFactory.createCharacter(type); The factory handles the complex creation logic, making your code cleaner and more maintainable. If you add a new character type later, you only need to modify the factory, not every place characters are created.\n2. Structural Patterns These patterns focus on how classes and objects are composed to form larger structures.\nExample: The Adapter Pattern\nImagine you have a new library that tracks user analytics, but its interface doesn\u0026rsquo;t match what your application expects:\n1 2 3 4 5 6 7 8 9 10 11 // Your application expects: interface OldAnalytics { void trackEvent(String name, Map\u0026lt;String, String\u0026gt; data); } // But the new library uses: class NewAnalyticsLibrary { void logActivity(String activityName, JSONObject attributes) { // Implementation } } Instead of changing your entire codebase, you can create an adapter:\n1 2 3 4 5 6 7 8 9 10 11 12 13 class AnalyticsAdapter implements OldAnalytics { private NewAnalyticsLibrary newAnalytics = new NewAnalyticsLibrary(); public void trackEvent(String name, Map\u0026lt;String, String\u0026gt; data) { JSONObject json = convertMapToJson(data); newAnalytics.logActivity(name, json); } private JSONObject convertMapToJson(Map\u0026lt;String, String\u0026gt; data) { // Conversion logic return new JSONObject(data); } } Now your existing code works with the new library without modifications!\n3. Behavioral Patterns These patterns are concerned with algorithms and the assignment of responsibilities between objects.\nExample: The Observer Pattern\nThink of how social media works: when someone posts an update, all their followers get notified. This is the Observer pattern in action!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Post { private List\u0026lt;User\u0026gt; followers = new ArrayList\u0026lt;\u0026gt;(); public void addFollower(User user) { followers.add(user); } public void createUpdate(String content) { // Create the update notifyFollowers(); } private void notifyFollowers() { for(User follower : followers) { follower.notify(\u0026#34;New post available!\u0026#34;); } } } This pattern creates a one-to-many dependency where multiple observers (followers) are notified when the subject (post creator) changes state.\nWhy Should Beginners Care About Design Patterns? If you\u0026rsquo;re new to programming, you might wonder if design patterns are relevant to you yet. The answer is a resounding yes, for several important reasons:\n1. They Teach You How to Think About Code Structure Learning design patterns early helps develop an architectural mindset. Instead of focusing solely on making your code work, you start considering how it\u0026rsquo;s organized and how different components interact.\n2. They Help You Write More Professional Code Faster When you recognize common problems, you can apply established solutions rather than struggling through trial and error. This accelerates your development process and results in more robust solutions.\n3. They Improve Collaboration Programming is rarely a solo activity. Using established patterns creates a shared vocabulary with other developers. When you mention using a \u0026ldquo;Factory Method\u0026rdquo; or \u0026ldquo;Observer Pattern,\u0026rdquo; other developers immediately understand your approach.\n4. They Prepare You for Framework Learning Modern frameworks like React, Angular, and Spring incorporate many design patterns. Understanding these patterns makes learning frameworks easier because you recognize the underlying concepts.\n5. They Prevent Common Mistakes Many design patterns evolved specifically to address problems that repeatedly caused bugs or maintenance headaches. Learning these patterns helps you avoid these pitfalls from the start.\nDesign Patterns in Real-World Applications Let\u0026rsquo;s look at some examples of how design patterns show up in technologies you might already be using:\nIn Web Development React\u0026rsquo;s Component Model: React uses the Composite pattern, allowing you to build complex UIs from simple components that can contain other components.\nEvent Handling in JavaScript: The Observer pattern is used extensively in event-driven programming, where event listeners \u0026ldquo;observe\u0026rdquo; elements and respond to user interactions.\nIn Mobile Apps iOS\u0026rsquo;s Delegation Pattern: This is a variation of the Observer pattern where objects delegate certain responsibilities to other objects.\nAndroid\u0026rsquo;s Adapter Views: These use the Adapter pattern to convert data sources into views that can be displayed in lists.\nIn Everyday Applications Word Processors: The Command pattern powers features like undo/redo, where each action is encapsulated as an object that can be executed, tracked, and reversed.\nVideo Games: The State pattern manages character behaviors, allowing characters to smoothly transition between actions like walking, running, or attacking.\nCommon Misunderstandings About Design Patterns As you learn about design patterns, be aware of these common misconceptions:\nMisunderstanding #1: \u0026ldquo;Design Patterns Are Too Complex for Beginners\u0026rdquo; Reality: While some patterns have complex implementations, the concepts behind them are accessible to beginners. Start with simpler patterns like Factory, Observer, and Strategy.\nMisunderstanding #2: \u0026ldquo;I Should Use Design Patterns Everywhere\u0026rdquo; Reality: Design patterns are tools, not rules. They should be applied when they solve a specific problem, not forced into every situation. Sometimes simple, straightforward code is better.\nMisunderstanding #3: \u0026ldquo;Design Patterns Are Outdated\u0026rdquo; Reality: While some specific implementations may become less relevant as languages evolve, the core principles behind design patterns remain valuable. Modern languages and frameworks may offer built-in solutions for some patterns, but understanding the patterns helps you use these features more effectively.\nHow to Start Learning Design Patterns Ready to dive into design patterns? Here\u0026rsquo;s a beginner-friendly approach:\n1. Start with Real Problems Rather than trying to memorize all 23 GoF patterns at once, focus on patterns that solve problems you\u0026rsquo;ve encountered. This makes the learning more relevant and memorable.\n2. Learn Through Examples Look for concrete examples of patterns in languages you\u0026rsquo;re familiar with. Many online resources provide code samples in different programming languages.\n3. Practice Implementation Try implementing a simple version of each pattern you learn. This hands-on approach helps solidify your understanding.\n4. Refactor Existing Code Take some code you\u0026rsquo;ve already written and refactor it using a design pattern. This exercise helps you see the benefits and trade-offs directly.\n5. Learn Patterns in Groups Study related patterns together to understand their similarities and differences. For example, learn Factory Method, Abstract Factory, and Builder together to understand different approaches to object creation.\nA Deeper Look at Five Essential Patterns for Beginners Let\u0026rsquo;s explore five patterns that are particularly valuable for beginners to understand:\n1. Singleton Pattern Problem it Solves: Sometimes you need exactly one instance of a class that is accessible globally, like a configuration manager or connection pool.\nHow it Works: The pattern ensures a class has only one instance and provides a global point to access it.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class DatabaseConnection { private static DatabaseConnection instance; // Private constructor prevents direct instantiation private DatabaseConnection() { // Initialize connection } public static synchronized DatabaseConnection getInstance() { if (instance == null) { instance = new DatabaseConnection(); } return instance; } public void query(String sql) { // Execute query using the single connection } } // Usage DatabaseConnection connection = DatabaseConnection.getInstance(); connection.query(\u0026#34;SELECT * FROM users\u0026#34;); When to Use It: When having multiple instances would cause problems (like multiple file writers trying to access the same file) or waste resources.\nWhen to Avoid It: When you need different instances with different configurations or when it introduces unnecessary global state.\n2. Strategy Pattern Problem it Solves: You need different variants of an algorithm, but don\u0026rsquo;t want to hardcode all the variants into a single class.\nHow it Works: Define a family of algorithms, encapsulate each one, and make them interchangeable.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Strategy interface interface PaymentStrategy { void pay(int amount); } // Concrete strategies class CreditCardPayment implements PaymentStrategy { private String cardNumber; public CreditCardPayment(String cardNumber) { this.cardNumber = cardNumber; } public void pay(int amount) { System.out.println(amount + \u0026#34; paid with credit card \u0026#34; + cardNumber); } } class PayPalPayment implements PaymentStrategy { private String email; public PayPalPayment(String email) { this.email = email; } public void pay(int amount) { System.out.println(amount + \u0026#34; paid using PayPal account \u0026#34; + email); } } // Context class ShoppingCart { private PaymentStrategy paymentStrategy; public void setPaymentStrategy(PaymentStrategy paymentStrategy) { this.paymentStrategy = paymentStrategy; } public void checkout(int amount) { paymentStrategy.pay(amount); } } // Usage ShoppingCart cart = new ShoppingCart(); cart.setPaymentStrategy(new CreditCardPayment(\u0026#34;1234-5678-9012-3456\u0026#34;)); cart.checkout(100); cart.setPaymentStrategy(new PayPalPayment(\u0026#34;user@example.com\u0026#34;)); cart.checkout(200); When to Use It: When you have multiple ways to perform an operation and need to switch between them dynamically.\nWhen to Avoid It: When there\u0026rsquo;s only one or two simple variants of an algorithm that aren\u0026rsquo;t likely to change.\n3. Observer Pattern Problem it Solves: You need many objects to receive updates when another object changes.\nHow it Works: Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 // Subject interface interface Subject { void addObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers(); } // Observer interface interface Observer { void update(String message); } // Concrete subject class NewsAgency implements Subject { private List\u0026lt;Observer\u0026gt; observers = new ArrayList\u0026lt;\u0026gt;(); private String news; public void addObserver(Observer observer) { observers.add(observer); } public void removeObserver(Observer observer) { observers.remove(observer); } public void notifyObservers() { for (Observer observer : observers) { observer.update(news); } } public void setNews(String news) { this.news = news; notifyObservers(); } } // Concrete observer class NewsChannel implements Observer { private String name; public NewsChannel(String name) { this.name = name; } public void update(String news) { System.out.println(name + \u0026#34; received news: \u0026#34; + news); } } // Usage NewsAgency agency = new NewsAgency(); NewsChannel channel1 = new NewsChannel(\u0026#34;Channel 1\u0026#34;); NewsChannel channel2 = new NewsChannel(\u0026#34;Channel 2\u0026#34;); agency.addObserver(channel1); agency.addObserver(channel2); agency.setNews(\u0026#34;Breaking news: Design patterns are awesome!\u0026#34;); When to Use It: When changes to one object may require changing other objects, and you don\u0026rsquo;t know how many objects need to change.\nWhen to Avoid It: When the notification logic becomes too complex or observers need to rely on notifications that might be missed (e.g., if they\u0026rsquo;re temporarily disconnected).\n4. Factory Method Pattern Problem it Solves: You need to create objects, but don\u0026rsquo;t know exactly what type of objects you\u0026rsquo;ll need to create until runtime.\nHow it Works: Define an interface for creating an object, but let subclasses decide which class to instantiate.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // Product interface interface Vehicle { void drive(); } // Concrete products class Car implements Vehicle { public void drive() { System.out.println(\u0026#34;Driving a car...\u0026#34;); } } class Motorcycle implements Vehicle { public void drive() { System.out.println(\u0026#34;Riding a motorcycle...\u0026#34;); } } // Creator abstract class abstract class VehicleFactory { public abstract Vehicle createVehicle(); public void deliverVehicle() { Vehicle vehicle = createVehicle(); System.out.println(\u0026#34;Delivering the vehicle...\u0026#34;); vehicle.drive(); } } // Concrete creators class CarFactory extends VehicleFactory { public Vehicle createVehicle() { return new Car(); } } class MotorcycleFactory extends VehicleFactory { public Vehicle createVehicle() { return new Motorcycle(); } } // Usage VehicleFactory factory = new CarFactory(); factory.deliverVehicle(); factory = new MotorcycleFactory(); factory.deliverVehicle(); When to Use It: When a class can\u0026rsquo;t anticipate the type of objects it must create, or when a class wants its subclasses to specify the objects it creates.\nWhen to Avoid It: When adding new products requires changing the factory interface, which violates the Open/Closed Principle.\n5. Decorator Pattern Problem it Solves: You need to add responsibilities to objects dynamically without affecting other objects of the same class.\nHow it Works: Attach additional responsibilities to an object dynamically by placing it inside special wrapper objects.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 // Component interface interface Coffee { String getDescription(); double cost(); } // Concrete component class SimpleCoffee implements Coffee { public String getDescription() { return \u0026#34;Simple coffee\u0026#34;; } public double cost() { return 1.0; } } // Decorator abstract class abstract class CoffeeDecorator implements Coffee { protected Coffee decoratedCoffee; public CoffeeDecorator(Coffee coffee) { this.decoratedCoffee = coffee; } public String getDescription() { return decoratedCoffee.getDescription(); } public double cost() { return decoratedCoffee.cost(); } } // Concrete decorators class MilkDecorator extends CoffeeDecorator { public MilkDecorator(Coffee coffee) { super(coffee); } public String getDescription() { return decoratedCoffee.getDescription() + \u0026#34;, milk\u0026#34;; } public double cost() { return decoratedCoffee.cost() + 0.5; } } class SugarDecorator extends CoffeeDecorator { public SugarDecorator(Coffee coffee) { super(coffee); } public String getDescription() { return decoratedCoffee.getDescription() + \u0026#34;, sugar\u0026#34;; } public double cost() { return decoratedCoffee.cost() + 0.2; } } // Usage Coffee coffee = new SimpleCoffee(); System.out.println(coffee.getDescription() + \u0026#34;: $\u0026#34; + coffee.cost()); coffee = new MilkDecorator(coffee); System.out.println(coffee.getDescription() + \u0026#34;: $\u0026#34; + coffee.cost()); coffee = new SugarDecorator(coffee); System.out.println(coffee.getDescription() + \u0026#34;: $\u0026#34; + coffee.cost()); When to Use It: When you need to add responsibilities to objects dynamically and transparently, without affecting other objects.\nWhen to Avoid It: When the component hierarchy becomes too complex with many layers of decorators.\nDesign Patterns and Code Quality Beyond solving specific problems, design patterns contribute to overall code quality in several ways:\nThey Promote the SOLID Principles Many design patterns naturally align with the SOLID principles of object-oriented design:\nSingle Responsibility Principle: Patterns like Decorator and Strategy help separate different responsibilities. Open/Closed Principle: Patterns like Factory Method allow for extension without modification. Liskov Substitution Principle: Patterns ensure that subclasses can be used in place of their parent classes. Interface Segregation Principle: Patterns like Adapter help create focused interfaces. Dependency Inversion Principle: Patterns like Dependency Injection promote depending on abstractions. They Reduce Code Duplication By providing standard solutions to common problems, patterns help avoid reinventing solutions, reducing duplicated code across projects.\nThey Improve Code Maintainability Well-implemented patterns make code more modular and easier to understand, which simplifies maintenance and updates.\nThe Journey From Pattern User to Pattern Creator As you grow as a developer, your relationship with design patterns will evolve:\nPattern Recognizer: First, you\u0026rsquo;ll learn to identify when existing patterns apply to your problems. Pattern Implementer: Then, you\u0026rsquo;ll become comfortable implementing standard patterns in your code. Pattern Adapter: Next, you\u0026rsquo;ll adapt patterns to fit your specific needs, combining and modifying them as necessary. Pattern Creator: Eventually, you might even develop your own patterns to address unique challenges in your domain. Patterns as a Growth Investment Learning design patterns is one of the best investments you can make in your development career. They provide immediate benefits in code quality and long-term benefits in thinking about software architecture.\nStart small, focus on understanding the problems each pattern solves, and gradually incorporate them into your projects. Over time, you\u0026rsquo;ll find yourself naturally reaching for the right pattern when faced with a familiar challenge.\nRemember, the goal isn\u0026rsquo;t to use patterns for their own sake, but to solve problems efficiently and create maintainable code. When used appropriately, design patterns become powerful tools that elevate your work from merely functional to truly professional.\nHappy pattern hunting!\nAdditional Resources for Learning Design Patterns Books:\n\u0026ldquo;Head First Design Patterns\u0026rdquo; by Eric Freeman and Elisabeth Robson (beginner-friendly) \u0026ldquo;Design Patterns Explained\u0026rdquo; by Alan Shalloway and James Trott The original \u0026ldquo;Design Patterns\u0026rdquo; by the Gang of Four (more advanced) Online Resources:\nRefactoring.Guru (visual explanations with examples in multiple languages) SourceMaking.com (pattern descriptions with real-world examples) Design Patterns in the Java Tutorials (Oracle\u0026rsquo;s official examples) Practice Projects:\nTry implementing a simple text editor with undo/redo using the Command pattern Build a notification system using the Observer pattern Create a document converter that can output different formats using the Strategy pattern ","date":"2025-03-08T08:02:38.006Z","image":"https://blog.sparker0i.me/why-every-developer-should-care-about-design-patterns/67cbf99efca581965cde6fca_hu_e445c83256850238.png","permalink":"https://blog.sparker0i.me/why-every-developer-should-care-about-design-patterns/","title":"Why Every Developer Should Care About Design Patterns: A Deep Dive"},{"content":"We\u0026rsquo;ve been developing an AI-enabled data engineering product at our company, using Scala as the core programming language. We also utilize Scala-related frameworks like Spark and Play to power various components. Recently, we conducted a vulnerability scan on our applications and discovered numerous security issues. The root cause was that we hadn\u0026rsquo;t updated our libraries in over three years.\nDuring this period, Scala underwent a major release and introduced its first Long-Term Support (LTS) version. Alongside these updates, our codebase had accumulated significant technical debt, providing us with an opportunity to address longstanding inefficiencies.\nBackground We had been using the scalaj-http library for making HTTP calls in Scala. However, this library had reached its end-of-life (EOL) status over two years ago and was no longer receiving updates.\nThis is a small snippet of our class which defines the methods making HTTP calls:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def makePostCall(url: String, body: String, authToken: String): (Int, JsValue) = { println(\u0026#34;url: \u0026#34;+url) println(\u0026#34;body: \u0026#34;+body) val res = Http(url) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .header(\u0026#34;Authorization\u0026#34;, s\u0026#34;Bearer $authToken\u0026#34;) .header(\u0026#34;Charset\u0026#34;, \u0026#34;UTF-8\u0026#34;) .option(HttpOptions.readTimeout(100000000)) .option(HttpOptions.connTimeout(100000000)) .postData(body) .asString val code = res.code val response = Json.parse(res.body) (code, response) } def makeGetCall(url: String, authToken: String): (Int, JsValue) = { println(\u0026#34;url: \u0026#34;+url) val res = Http(url) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .header(\u0026#34;Authorization\u0026#34;, s\u0026#34;Bearer $authToken\u0026#34;) .header(\u0026#34;Charset\u0026#34;, \u0026#34;UTF-8\u0026#34;) .option(HttpOptions.readTimeout(100000000)) .option(HttpOptions.connTimeout(100000000)) .asString val code = res.code val response = Json.parse(res.body) (code, response) } // More methods for more HTTP types. With our existing code base, there was no scope of any modification. It had a lot of limitations:\nHTTP call functions were hardcoded. Headers were not modifiable Authentication was limited to Bearer tokens The request body format was restricted to JSON HTTP methods could not be easily altered. These limitations affected not only our HTTP class but also other classes that relied on it. With the EOL of scalaj-http, we needed to refactor our codebase significantly.\nInspiration Many software developers and testers use REST API clients to test API endpoints. Various REST API clients exist, and the one you see above is Bruno - One of the only REST clients that meet my org\u0026rsquo;s stringent IT Security Standards.\nThe flexibility of API clients - allowing for different HTTP methods, body types, and authentication mechanisms - is crucial. This adaptability is possible because these clients are not hardcoded, unlike our previous implementation. Therefore, in our refactoring, we aimed to make our code modular and adhere to known design patterns and SOLID principles.\nBreakdown In our previous implementation, constructing the HTTP request and making the call were done within a single file. This approach meant that any modification required changes across multiple classes, leading to tight coupling.\nTo resolve this, we decided to separate our API package into three subpackages:\nrequest client apps The request and client components are designed to be loosely coupled. This separation ensures that changes in one part, such as switching to a different client library, require minimal modifications elsewhere in the code. Ideally, only a single line should need updating to change the client library used throughout the program. Classes inside apps would use the client to execute business processes by making the HTTP calls.\nRequest Package To construct an HTTP request, there are five fundamental components:\nURL Headers HTTP Method Body Authentication It\u0026rsquo;s a widely accepted convention that the URL is a String, while headers are represented as a HashMap with String keys and values.\nThe elements that can vary between HTTP requests are the Method, Body, and Authentication.\nMethod In this initial implementation of the Request package, I\u0026rsquo;ve chosen to support the most commonly used HTTP methods:\nGET POST PUT DELETE 1 2 3 4 5 6 trait ApiMethod // trait is the equivalent for interface in OOP langugaes case object DELETE extends ApiMethod case object GET extends ApiMethod case object POST extends ApiMethod case object PUT extends ApiMethod To implement these, I use a Scala-specific paradigm called Case Object, which aligns with the Singleton design pattern. This approach is similar to enums in other programming languages.\nBody For the body of HTTP requests, I will support four types:\nJSON XML Binary FormURLEncoded To manage these, I created an interface (ApiBody) that specifies two methods: contentType and content. Each respective class will have a default constructor that includes a variable representing the content we intend to send.\nThe contentType method will return the MIME type, such as application/json or application/xml. The content method will convert the body content into a byte array. The choice of a byte array is deliberate; while JSON and XML content can often be represented as a string, binary data cannot. By converting all types into a byte array, we provide a consistent interface for later client implementations.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 trait ApiBody { def contentType: String def content: Array[Byte] } case class JsonBody(json: String) extends ApiBody { override def contentType: String = \u0026#34;application/json\u0026#34; override def content: Array[Byte] = json.getBytes(\u0026#34;UTF-8\u0026#34;) } case class XmlBody(xml: String) extends ApiBody { override def contentType: String = \u0026#34;application/xml\u0026#34; override def content: Array[Byte] = xml.getBytes(\u0026#34;UTF-8\u0026#34;) } case class FormUrlEncodedBody(formData: Map[String, String]) extends ApiBody { override def contentType: String = \u0026#34;application/x-www-form-urlencoded\u0026#34; override def content: Array[Byte] = formData .map(e =\u0026gt; java.net.URLEncoder.encode(e._1, \u0026#34;UTF-8\u0026#34;) + \u0026#34;=\u0026#34; + java.net.URLEncoder.encode(e._2, \u0026#34;UTF-8\u0026#34;)) .mkString(\u0026#34;\u0026amp;\u0026#34;) .getBytes(\u0026#34;UTF-8\u0026#34;) } case class BinaryBody(data: Array[Byte]) extends ApiBody { override def contentType: String = \u0026#34;application/octet-stream\u0026#34; override def content: Array[Byte] = data } Authentication We will implement two common types of authentication:\nToken based authentication Basic authentication (user/password) The plan was to use the same approach as with Body and Method, by defining an interface and corresponding classes. However, there’s a challenge: different client libraries may handle authentication in unique ways. Since the Request package must remain unaware of the Client package\u0026rsquo;s specifics, a direct implementation isn’t feasible.\nOne potential solution is to use headers since they can be represented as a key-value map. This method works for basic and token-based authentication but fails for more complex mechanisms, such as GitHub API or AWS S3 SDK with v4 signature. These mechanisms often require access to the request body to generate appropriate headers.\nIn my implementation, I have kept Body and Authentication separate, and I intend to maintain that separation. The solution involves creating an interface and corresponding classes for authentication types, along with a wrapper that encapsulates all authentication mechanisms. This wrapper will then be implemented by the Client to execute the HTTP call.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 trait AuthRequestProvider { def addBearerToken(token: String): Unit def addBasicAuth(username: String, password: String): Unit } trait ApiAuth { def applyTo(request: AuthRequestProvider): Unit } case class BasicAuth(username: String, password: String) extends ApiAuth { override def applyTo(request: AuthRequestProvider): Unit = { request.addBasicAuth(username, password) } } case class TokenAuth(token: String) extends ApiAuth { override def applyTo(request: AuthRequestProvider): Unit = { request.addBearerToken(token) } } The implementation of the AuthRequestProvider in the Client package will be discussed in a subsequent section.\nAPI Request With the basics of building an HTTP Request covered, the next step is to create a class representing our ApiRequest. This class will be used by the client to execute the actual HTTP call.\n1 2 3 4 5 6 7 protected[api] case class ApiRequest( method: ApiMethod, url: String, body: Option[ApiBody] = None, headers: Map[String, String] = Map.empty, authentication: Option[ApiAuth] = None ) In this class, the Method and URL are mandatory components for making an HTTP call. While headers are optional, they are represented using a Map data type, so no dedicated class is necessary. The Body and Authentication components are also optional and are encapsulated using the Option wrapper. If a component is absent, it will be represented as None; if present, it will be enclosed within a Some object.\nThe protected[api] modifier on the case class restricts its instantiation to within the api package, which includes both the request and client sub-packages. This ensures that downstream classes cannot create substandard ApiRequest instances, maintaining the integrity of our API implementation.\nAPI Request Builder Downstream classes should use the ApiRequestBuilder to create an ApiRequest object, which they can then pass to the client for making the HTTP call. This implementation follows the Builder design pattern.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 case class ApiRequestBuilder( private val method: Option[ApiMethod] = Some(GET), private val url: String = \u0026#34;\u0026#34;, private val body: Option[ApiBody] = None, private val headers: Map[String, String] = Map(), private val authentication: Option[ApiAuth] = None ) { def withMethod(method: ApiMethod): ApiRequestBuilder = this.copy(method = Some(method)) def withUrl(url: String): ApiRequestBuilder = this.copy(url = url) def withBody(body: ApiBody): ApiRequestBuilder = this.copy(body = Some(body)) def withHeaders(headers: Map[String, String]): ApiRequestBuilder = this.copy(headers = headers) def addHeader(key: String, value: String): ApiRequestBuilder = this.copy(headers = headers + (key -\u0026gt; value)) def withAuthentication(auth: ApiAuth): ApiRequestBuilder = this.copy(authentication = Some(auth)) def build(): ApiRequest = { if (method.isEmpty || url.isEmpty) throw new IllegalStateException(\u0026#34;Method and URL cannot be empty\u0026#34;) ApiRequest(method.get, url, body, headers, authentication) } } The usage is straightforward: methods like withUrl, withHeaders and others allow you to configure your request. The build() method then produces the ApiRequest object, which can be passed to the client from downstream classes.\nAPI Response When an HTTP request is made, a response is received, which can influence decisions within the application. The APIResponse case class encapsulates this response, including the HTTP response code, response body, and response headers.\n1 case class ApiResponse(code: Int, body: ApiBody, headers: Map[String, String] = Map()) For the response body, we utilize the same ApiBody interface as for the request body, maintaining consistency across request and response handling.\nClient With our Request package established, we now focus on the client responsible for making the actual HTTP call. The client will be defined as an interface with a single method, send, which accepts an instance of ApiRequest.\n1 2 3 trait ApiClient { def send(request: ApiRequest): Either[ApiResponse, ApiResponse] } Client Library For our client implementation, we use the Scala STTP library, which will consume ApiRequest and generate an ApiResponse. Before implementing the client, we need to address two key components:\nAuthentication Wrapper: An implementation to handle various authentication mechanisms. Body Parser: To convert the response body into an appropriate ApiBody implementation. Authentication Wrapper The authentication wrapper for the STTP library will accept an STTP Request as a constructor argument and provide the necessary authentication logic.\n1 2 3 4 5 6 7 8 9 class SttpRequestProviderWrapper(var request: sttp.client4.Request[Either[String, String]]) extends AuthRequestProvider { override def addBearerToken(token: String): Unit = { request = request.auth.bearer(token) } override def addBasicAuth(username: String, password: String): Unit = { request = request.auth.basic(username, password) } } This wrapper can be extended to support additional authentication mechanisms as needed. The request (sttp.client4.Request[Either[String, String]]) will include the request body, allowing the wrapper to generate custom HTTP headers based on the body content.\nBody Parser To transform the response body into one of our ApiBody implementations, we create a BodyParser interface. This interface will take the response content and headers, returning the appropriate body type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 trait BodyParser { def parseBody(): ApiBody } case class BinaryParser(content: Array[Byte], contentType: String) extends BodyParser { override def parseBody(): ApiBody = BinaryBody(content, contentType) } case class JsonParser(content: Array[Byte]) extends BodyParser { override def parseBody(): ApiBody = JsonBody(new String(content, \u0026#34;UTF-8\u0026#34;)) } case class XmlParser(content: Array[Byte]) extends BodyParser { override def parseBody(): ApiBody = XmlBody(new String(content, \u0026#34;UTF-8\u0026#34;)) } case class StringParser(content: Array[Byte]) extends BodyParser { override def parseBody(): ApiBody = StringBody(new String(content, \u0026#34;UTF-8\u0026#34;)) } case class FormUrlEncodedParser(content: Array[Byte]) extends BodyParser { override def parseBody(): ApiBody = { val decodedContent = new String(content, \u0026#34;UTF-8\u0026#34;) val formData = decodedContent.split(\u0026#34;\u0026amp;\u0026#34;).map { pair =\u0026gt; val Array(key, value) = pair.split(\u0026#34;=\u0026#34;) java.net.URLDecoder.decode(key, \u0026#34;UTF-8\u0026#34;) -\u0026gt; java.net.URLDecoder.decode(value, \u0026#34;UTF-8\u0026#34;) }.toMap FormUrlEncodedBody(formData) } } A Factory design pattern will be used to instantiate the appropriate BodyParser:\n1 2 3 4 5 6 7 8 9 10 11 object BodyParser { def getParser(content: Array[Byte], headers: Map[String, String]): Option[BodyParser] = headers.get(\u0026#34;content-type\u0026#34;) match { case Some(value) =\u0026gt; value match { case \u0026#34;application/json\u0026#34; =\u0026gt; Some(JsonParser(content)) case \u0026#34;application/xml\u0026#34; =\u0026gt; Some(XmlParser(content)) case \u0026#34;application/x-www-form-urlencoded\u0026#34; =\u0026gt; Some(FormUrlEncodedParser(content)) case _ =\u0026gt; Some(BinaryParser(content)) } case None =\u0026gt; None } } Client Implementation With the building blocks in place, we implement the client. The client will use the ApiRequest properties (method, URL, headers) to construct the initial request, apply the body if present, and then handle authentication if required.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 protected[api] class SttpClient extends ApiClient { private val backend = HttpURLConnectionBackend() override def send(request: ApiRequest): Either[ApiResponse, ApiResponse] = { try { new URI(request.url).toURL } catch { case e: URISyntaxException =\u0026gt; return Left(ApiResponse(500, Some(JsonBody(ujson.Obj(\u0026#34;message\u0026#34; -\u0026gt; e.getMessage).toString())), Map())) case e: IllegalArgumentException =\u0026gt; return Left(ApiResponse(500, Some(JsonBody(ujson.Obj(\u0026#34;message\u0026#34; -\u0026gt; e.getMessage).toString())), Map())) } var sttpRequest = sttp.client4.basicRequest.method(sttp.model.Method(request.method.toString), uri\u0026#34;${request.url}\u0026#34;) request.headers.foreach { case (key, value) =\u0026gt; sttpRequest = sttpRequest.header(key, value) } request.body.foreach { body =\u0026gt; sttpRequest = sttpRequest.body(body.content).contentType(body.contentType) } val authRequest = new SttpRequestProviderWrapper(sttpRequest) request.authentication.foreach(_.applyTo(authRequest)) sttpRequest = authRequest.request val response = sttpRequest.response(asByteArray).send(backend) val headers = response.headers.map(h =\u0026gt; h.name -\u0026gt; h.value).toMap val responseEntity = ApiResponse( response.code.code, response.body match { case Left(error) =\u0026gt; BodyParser.getParser(error.getBytes(\u0026#34;UTF-8\u0026#34;), headers) match { case Some(value) =\u0026gt; Some(value.parseBody()) case None =\u0026gt; None } case Right(value) =\u0026gt; Some(BodyParser.getParser(value, headers).get.parseBody()) }, headers ) if (response.code.isClientError || response.code.isServerError) Left(responseEntity) else Right(responseEntity) } } Ensure stability of API package A primary motivation for this rewrite is to minimize the impact of a library going end-of-life (EOL) in the future. If the STTP library becomes unsupported, we should only need to make a single line change in our code, ensuring the rest of the classes remain unaffected.\nIn the SttpClient implementation, the access modifier is set to protected[api]. This restricts access to within the api package, preventing direct usage outside. To facilitate client access, we define an ApiClient object in the same file as the ApiClient trait. This object includes an apply() method that creates a new instance of SttpClient, leveraging its position within the api subpackage.\n1 2 3 object ApiClient { def apply() = new SttpClient() } In the event that the STTP library goes EOL, we would need to:\nImplement a new authentication wrapper for the replacement library. Implement the new library\u0026rsquo;s client (similar to SttpClient). Update the apply() method in ApiClient to return an instance of the new client. This single-line change ensures that no other classes are impacted by the transition. Using the client With the foundational components in place, we can now use the ApiClient to make an HTTP call. Here\u0026rsquo;s an example of how to perform an API call using the implemented code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 val request = ApiRequestBuilder() .withUrl(url) .withMethod(POST) // Refers to the case object POST created above .withAuthentication(TokenAuth(token)) .withBody(JsonBody(\u0026#34;\u0026#34;\u0026#34;{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}\u0026#34;\u0026#34;\u0026#34;)) .build() ApiClient().send(request) match { case Left(value) =\u0026gt; throw new Exception(value.body.get) case Right(ApiResponse(_, body, _)) =\u0026gt; println(body.get) println(\u0026#34;API Call successful\u0026#34;) } Note the use of ApiClient() to initiate the HTTP call. In Scala, ApiClient() is syntactic sugar that the compiler interprets as a call to the ApiClient.apply() method.\nAdherence to SOLID Principles SOLID is a set of five principles of Object-Oriented class design. They are a set of rules and best practices to follow while designing a class structure. These five principles help us understand the need for certain design patterns and software architecture in general. I always strive to adhere to these principles whenever I\u0026rsquo;m developing software, and we will now have a look at how the code above adheres to the SOLID Principles.\nSingle Responsibility Principle (SRP) A class should have one, and only one, reason to change. This principle promotes the separation of concerns within a system:\nOur HttpRequestBuilder is solely responsible for building HttpRequest objects. It encapsulates the construction logic and provides an interface for setting request properties. The BodyParser isolates the logic for parsing different types of response bodies based on the content type, keeping this concern separate from other parts of the system Our SttpClient implementation handles the responsibility of sending HTTP requests and processing responses. It doesn\u0026rsquo;t concern itself with the construction of requests or parsing of response bodies, which are handled by other classes. ApiResponse encapsulates the response data, separating it from the request logic and the client implementation. ApiAuth and its various subclasses have a single responsibility to apply a specific authentication mechanism. Open/Closed Principle (OCP) Software entities should be open for extension but closed for modification. This means you can extend the behavior of a class without modifying its source code:\nThe Body trait and its subclasses allow for easy addition of new body types without altering existing code. For example, adding a new CsvBody would involve creating a new case class that extends Body. New implementations of ApiClient can be created (e.g., JavaApiClient, Http4sApiClient) without modifying the existing SttpClient. Each new client implementation can provide its own logic for sending requests. New authentication methods can be added by creating new classes that implement the Authentication trait. Liskov Substitution Principle (LSP) Objects of a superclass should be replaceable with objects of a subclass without affecting the correctness of the program:\nAnywhere a Body is expected, any of its subclasses (JsonBody, XmlBody, etc.) can be used interchangeably. The system can use TokenAuthentication or BasicAuthentication interchangeably wherever Authentication is expected, ensuring the applyTo method works correctly for any subclass. Interface Segregation Principle (ISP) No client should be forced to depend on methods it does not use. This principle advocates for smaller, more specific interfaces:\nApiAuth Interface: This interface is specifically for adding authentication to a request and does not include other unrelated methods. It ensures that only relevant methods are exposed to classes that implement this interface. ApiClient Interface: Defines a minimal interface for sending HTTP requests. It does not force implementing classes to expose unnecessary methods. Dependency Inversion Principle (DIP) High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions:\nApiClient Interface: The high-level code (such as a service using the HTTP client) depends on the ApiClient abstraction, not on specific implementations like SttpClient. Dependency on Abstractions: The system uses the ApiAuth, ApiBody, and ApiClient abstractions rather than concrete implementations, making it easy to swap out implementations as needed. Conclusion The goal was to modernize our HTTP client classes, moving from a rigid, outdated pieces to a flexible, modular architecture. By focusing on a clean separation of concerns and employing design patterns like Builder, Factory and Singleton, we created a robust framework that enhances maintainability and scalability.\nKey features include a modular request builder, an extensible authentication wrapper, and a consistent handling of API responses. The new design ensures easy adaptability to future changes, such as switching out libraries or updating security protocols, with minimal impact on the overall system.\nThis streamlined, adaptable architecture not only addresses previous limitations but also positions our product for future growth and technological advancements.\nCode used inside this post is also available inside my GitHub repo.\n","date":"2024-08-06T22:01:48.09Z","image":"https://blog.sparker0i.me/refactor-a-tightly-coupled-codebase-by-following-solid-principles/66b29d4c63629cc69724f054_hu_74842450f0ce5e8d.png","permalink":"https://blog.sparker0i.me/refactor-a-tightly-coupled-codebase-by-following-solid-principles/","title":"Refactor a tightly coupled codebase by following SOLID Principles"},{"content":"UPDATE 25th June 2024: Modified the article to include podman alongside colima.\nApps are a significant part of our lives today. There are various apps you might be using today. On a smartphone, you would be using WhatsApp, Snapchat, Instagram, YouTube and various other apps. On a PC/laptop, you would be using a browser, game launchers to start your favorite games and IDEs to develop applications.\nMany websites you know and love are apps themselves. As an example, the Facebook website is written using the React framework and packaged as a Web application to run in a browser. YouTube and various other websites by Google are written in the Angular framework and packaged as web apps too.\nTo develop any kind of major applications, you would need a PC or a laptop and an IDE installed. There are various kinds of IDEs available based on the programming language and the kind of application you are developing. You would also need various libraries to create your application - lest write the code yourself. Which leads to the problem I\u0026rsquo;ll be tackling in today\u0026rsquo;s post.\nBackground PCs and laptops sold today run on x86 architecture CPUs made by Intel and AMD. However in recent times, we have started to see a lot of laptops being sold with the CPUs using ARM architecture, which until recently was found only in mobile phones. Not only are these CPUs way more battery efficient, they also allow to wake a laptop from sleep a lot quicker than x86 based laptops. Most notable ARM based laptops are manufactured by Apple, which use the Apple Silicon chips - M1, M2, M3 etc.\nLike I\u0026rsquo;ve explained before when you are looking to build an application, you\u0026rsquo;d need various libraries to write code. Most libraries in the various programming languages are universal, ie. they are compatible to run on the architecture of your machine\u0026rsquo;s CPU. However, there are some libraries which do not run (yet) on an ARM machine.\nThe most notable culprit for this is the ibm_db library on Node. If you try to install that package on your Apple Silicon mac, you will see this error:\nError installing ibm_db directly on the Apple Silicon MacBook\nYup, it suggests to install the x64 version of NodeJS and then use the package. I had so many other NodeJS applications on my machine without ibm_db which were working pretty well, so I did not want to install an inefficient version of NodeJS for my machine. But I also had to work on this important project on the M1 mac for my org. I was in a dilemma. Enter devcontainers:\nDevcontainers From its website, A devcontainer allows you to use a container as a full-featured development environment. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase, and to aid in continuous integration and testing.\nThis is very similar to how Python\u0026rsquo;s venv (Virtual Environments) work. Usually, all Python developers need that to do any basic development. But one key difference with devcontainers is that it opens your project folder inside a Docker container, and then any packages you install in the devcontainer remains inside that and does not cross over to your host machine.\nWhile the devcontainer spec is Open source and available independently, Visual Studio Code provides an easy way (UI) of doing stuff with it. Using devcontainers, I\u0026rsquo;ll be trying to run my NodeJS app with the ibm_db dependency on my MacBook with Apple Silicon.\nCreate the Virtual Machine Docker - or for that matter any of the open source containerization software - cannot run as is on a machine without a Linux Kernel. You\u0026rsquo;ll need a Linux virtual machine that acts as the place where all your containers will be run. The simplest solution to do this is to create a VM using Colima or Podman.\nHere is the command to create a machine using Colima:\n1 colima start --cpu 2 --memory 4 --disk 50 --arch aarch64 --vm-type=vz --mount-type=virtiofs --vz-rosetta --very-verbose The CPU, Memory and Disk parameters should be very obvious here. --very-verbose is to see more detailed logging while the VM starts. It is useful to debug if anything is going wrong or not. --arch aarch64 tells the Lima CLI to create the VM with the ARM64 architecture. You cannot directly create an x86 machine on an ARM MacBook just like that. The next two options will help enable what I want to do. --vm-type=vz will use the new MacOS Virtualization API to create the VMs. --vz-rosetta will use Rosetta translation layer when interacting with the VM. --mount-type=virtiofs creates a VM with the virtiofs volume driver. This allows you to share files from your host machine inside the Container. Using the VZ APIs along with the virtiofs mount enables better performance running the VM.\nAs of Podman 5.1, Rosetta translation layer is supported by default. If you have been using Podman for your other projects but holding back to the emulation issues, you may start using it instead of colima. Here is the command to create and start a similar VM for Podman:\n1 2 podman machine init --cpus=2 --memory=2048 --disk-size=50 podman machine start It doesn\u0026rsquo;t matter what option you choose, any further steps from hereon are independent of your choice of Containerization Framework.\nCreate base image for the Container Devcontainer runs your code inside a Docker container. The basic principle of running a Docker Container requires you to have a base image on top of which any operations can be performed.\nIn this case I want to develop a NodeJS app which utilizes the ibm_db library. So I will need a base image with NodeJS installed. Thankfully, Microsoft provides base Docker images that work well with devcontainers inside VS Code. I\u0026rsquo;ll be using a base image which comes with NodeJS 20 installed.\nTo create the base image I will use in my application, I will need to SSH into the Virtual Machine using the command and then create the base image from there. If you try to create the base image from your host terminal, the image gets created with the ARM architecture, which is not helpful for us as we want the image with an x86 architecture.\nThis is the Dockerfile I will be using:\n1 2 FROM --platform=linux/amd64 mcr.microsoft.com/devcontainers/typescript-node:20-bookworm RUN uname -a Dockerfile contents\nTo create the image, run the following commands:\ncolima ssh\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\nCommand to build image:\ndocker build --no-cache --platform linux/amd64 --progress plain -t node20-amd64-localhost:latest . (For Colima) podman build --no-cache --platform linux/amd64 --progress plain -t node20-amd64-localhost:latest . (For Podman) exit\nOnce you build this image, you should see the output like below for the uname command. If you don\u0026rsquo;t see the x86_64 like I\u0026rsquo;ve highlighted, then you might have not followed the guide properly:\nPS: If you are using Podman and had a Podman VM created before installing v5.1, you may have to delete that VM and create another one in its place if you don\u0026rsquo;t see x86_64 as highlighted above. That will ensure that any new VMs created will have Rosetta translation layer by default.\nRun your project inside a Devcontainer I won\u0026rsquo;t be using any sample project for this article, as you may use any of your x86 based projects you wish to emulate inside a Devcontainer. To do that, you will need to create a folder called .devcontainer, inside which you need to have two files: devcontainer.json and Dockerfile.\nThe contents of the Dockerfile is a single line which uses the base image that we had built in one of the previous sections:\n1 FROM --platform=linux/amd64 node20-amd64-localhost:latest The devcontainer.json would contain the following contents (Please do not copy the comments as is, it is only meant to explain what each line does:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;name\u0026#34;: \u0026#34;NodeJS with Typescript installed to build angular apps using x86-only libraries\u0026#34;, \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, \u0026#34;runArgs\u0026#34;: [\u0026#34;-v\u0026#34;, \u0026#34;${localWorkspaceFolder}:/workspace:cached\u0026#34;], \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;extensions\u0026#34;: [ \u0026#34;ms-azuretools.vscode-docker\u0026#34; ] } }, \u0026#34;remoteUser\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;forwardPorts\u0026#34;: [3000] } In line 4, ${localWorkspaceFolder} refers to your project\u0026rsquo;s location on the host machine, /workspace is where your project files be mounted inside the devcontainer and the :cached option is used to improve performance in Docker when mounting volumes.\nI want to use the user root inside my devcontainer so that I don\u0026rsquo;t need to do a sudo everytime to install an npm package inside the container. I\u0026rsquo;m forwarding the port 3000 from my container to my host machine as my NodeJS express app uses that port to listen onto requests.\nOnce you are done with these two files, open your project inside VSCode, then install the Devcontainers extension, Reload the window, press Cmd+Shift+P and then type Reopen in Container and click on that option. This will build your Container image, mount your project and make it available inside /workspace in the container and then you should be able to emulate projects using x86 libraries inside the devcontainer on your machine.\nTo test that it works, I will try to install the ibm_db package from within the container, and here\u0026rsquo;s how that went:\nThat went nicely. Now I need to run my app and see whether it is able to connect to my Database using this library or not:\nYup it did connect well.\nWhy am I using Colima Colima has support for emulating x86 based VMs using the Rosetta 2 translation layer on Apple Silicon Macs. This is important as we needed ibm_db to work. I also think that building x86 images on ARM platforms will become common, as soon as ARM based laptops from Apple and others start becoming mainstream.\nMoreover, I\u0026rsquo;ve not yet found another easier way to create and run a Docker machine using CLI commands. Of course there\u0026rsquo;s Docker Desktop which gives a nice GUI, but its license change in 2022 wrecked havoc on many companies. Our org had to ban the installs of Docker Desktop completely. I have had to migrate to Podman in the past due to this. While it was fun, it didn\u0026rsquo;t help me solve my problem. Which brings us to: Podman v5.1 now supports the Rosetta translation layer.\nWhy not QEMU? I did try using QEMU based emulation by typing colima start --arch x86_64 -p qemu but, While that worked okay, as in it started the container and I was able to run my app, I discovered that for my NodeJS based application it wasn\u0026rsquo;t really as efficient. Also, my M1 MacBook was heating up like it hadn\u0026rsquo;t done ever before. What is the point of having an M1 Mac if it\u0026rsquo;s going to behave the same as the Intel ones. Thus I felt using Rosetta based emulation was better for me.\nColima vs Podman I\u0026rsquo;ve updated the blog post in June 2024 with the instructions for Podman as well. Thus a performance comparison between both options was also needed. In real world performance on your Apple Silicon based machine, you will not notice any difference between using your projects with a Docker VM (through Colima) or a Podman VM.\nIf you have a keen eye though, you\u0026rsquo;d notice that operations with Podman did finish 1-2s faster than on Colima. Even building the first node20-amd64 image on Podman took 2s lesser than that on Colima, despite using the same internet for both. If I were to make a guess, it could be due to one peculiar output I noticed when creating the first image on the Podman VM.\nHere I can see two things: SMP and PREEMPT_DYNAMIC, which were not available when I created the VM inside Colima. With a simple Google Search, I found this website which helped explain both these things. In a nutshell, a combination of both those strings allows you to utilize resources better while providing better responsiveness.\nHow about Windows? While ARM based Windows laptops are set to make a debut later in 2024, I don\u0026rsquo;t believe there will be too much to be done to get it to work. There\u0026rsquo;s WSL which exists already, and one has to watch out how the x86 emulation plays out on these ARM machines using the Snapdragon X chips. If that ends up like how Rosetta has played out so far, all we\u0026rsquo;ll need is for WSL and its distros to support doing the same as well. Things are not yet clear on that front, and I will try to update my article as soon as the picture is clear.\nConclusion Right now I\u0026rsquo;ve just shown one example where I had to run an NodeJS app with x86 libraries on an M1 based Mac without spinning up a full fledged VM like inside VirtualBox or VMWare. You can also extend this concept to various other languages having x86-only libraries like Python etc.\n","date":"2024-04-09T15:51:58Z","image":"https://blog.sparker0i.me/running-vs-code-devcontainers-with-x86-runtime-apple-silicon/661c07fb9bb70e9dac2bfedf_hu_ab4f0d0ce6d4bd5.png","permalink":"https://blog.sparker0i.me/running-vs-code-devcontainers-with-x86-runtime-apple-silicon/","title":"How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac"},{"content":"UPDATE Ever since I wrote this article, the team behind Podman has released and improved their Podman Desktop offering. This product improves upon Podman Desktop Companion and provides integration with other features from RedHat like Podman AI Lab, Code Ready Containers (OpenShift) within the app. I\u0026rsquo;d highly recommend to check that out instead.\nContainers and Docker have always been synonymous to the ears since eternity. For most of us, Docker has been the go-to tool for containerization - a way to pack a software application and all dependencies into an image that can be run anywhere. This allows application to remain separate from the infrastructure so that the app works regardless of where it runs.\nWhile working on multiple applications in my company, I had been using Docker Desktop as my containerization platform of choice, on my company provided MacBooks to build and debug application images. But we were dealt with a blow a few months ago when it was announced that effective January 31st 2022, Docker Desktop will require a license for enterprises with more than 250 employees. Unfortunately, my company IBM fell into Docker\u0026rsquo;s categorization of an enterprise.\nNaturally, I started looking at all available alternatives, given that Docker desktop annual subscription per user costs $250. Unfortunately there was no clear winner which could replicate everything that Docker Desktop could do - GUI support, Built in Kubernetes, Environments, Integration with IDEs etc. But all I needed was something with which I could build images locally in a secure manner and run containers locally, with an optional support of a GUI.\nEnter:\nPodman Podman is a CLI tool that provides a Docker-compatible API. It is open source and published by Red Hat. The biggest advantage over Docker is that it is a Daemonless container engine which runs the containers in a rootless state by default. This brings in additional security layer, because even if the container engine, runtime or the orchestrator is compromized, the attacker won\u0026rsquo;t gain any root privileges on the host - which is a flaw in Docker\u0026rsquo;s architecture. You can read this article to understand the finer points of difference between Docker and Podman.\nInstallation of Podman 4 is fairly simple on MacOS (using brew). Unfortunately for Linux based Operating Systems, only Fedora has an unofficial COPR that allows you to install Podman 4, while for other Operating Systems you have to build from source code in order to install podman 4.\nOnce you install the podman binary, all you need to do is execute the below two commands for MacOS:\n1 2 podman machine init podman machine start This will start a Fedora CoreOS based VM in the background having podman installed. Once you have started this Podman VM, Almost all Docker CLI commands are compatible with Podman as well - run, exec, push etc. It brings zero impact to the developers that operate on CLI - how you build images and whatever you do with that image using the Docker CLI remains the same. You can also add alias docker=podman to ~/.zshrc (MacOS) or ~/.bash_profile (Linux) so that you don\u0026rsquo;t need to keep reminding yourself to use podman instead of Docker.\nThe CLI route is easy for users who have been working with Docker CLI. However what about those users who have been primarily using the Docker Desktop GUI for their workflows? Enter:\nPodman Desktop Companion Podman Desktop Companion is a third-party app, which is an almost adequate drop-in replacement for the Docker Desktop GUI. Since this is not an official app, there are a few features this app lacks, most notably the absence of Kubernetes - though this won\u0026rsquo;t be a big deal for those who only want the containerization features of Podman. Here\u0026rsquo;s a screenshot of the app running on my MacBook:\nPodman Desktop Companion makes you feel right at home with the look and feel of Docker Desktop\nFeels familiar, doesn\u0026rsquo;t it? Unfortunately, the process to install this app is not as straightforward. For a MacBook, you will need to install lima-vm, an app that launches Linux virtual machines with automatic file sharing and port forwarding - very similar to what WSL2 does, just that it is mostly for MacOS but can also be used on various Linux distros as well. Unfortunately if you want to proceed to the next step, you will have to stop the podman machine you had created earlier.\nLima offers the ability to create VMs using their sample YAML templates or by supplying user written YAML. When you create a VM with the Podman Template YAML, the VM will be running Podman v3. This lacks some key features from Podman 4 including Volume and Device mounts, as well as a vastly improved Network stack. Thus you will have to use a custom YAML if you want to install Podman 4.\nYou need to ensure that the name of this VM is podman. This is necessary for the Desktop Companion (This dependency on a specific VM name is a bad programming practice on the part of the Desktop Companion creator, but since this is a Beta version we can forgive them for now until they release a stable version). You can do this by running limactl start --name=podman /location/of/the/yaml/from/below. To make things easier for you, I\u0026rsquo;ve written a YAML that worked well for me. You need to save this to a location and use the location of this YAML in the command above.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 images: - location: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/35/Cloud/x86_64/images/Fedora-Cloud-Base-35-1.2.x86_64.qcow2\u0026#34; arch: \u0026#34;x86_64\u0026#34; digest: \u0026#34;sha256:fe84502779b3477284a8d4c86731f642ca10dd3984d2b5eccdf82630a9ca2de6\u0026#34; - location: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/35/Cloud/aarch64/images/Fedora-Cloud-Base-35-1.2.aarch64.qcow2\u0026#34; arch: \u0026#34;aarch64\u0026#34; digest: \u0026#34;sha256:c71f2e6ce75b516d565e2c297ea9994c69b946cb3eaa0a4bbea400dbd6f59ae6\u0026#34; cpus: 4 memory: 8 GiB disk: 50 GiB mounts: - location: \u0026#34;~\u0026#34; - location: \u0026#34;/tmp/lima\u0026#34; writable: true containerd: system: false user: false provision: - mode: system script: | #!/bin/bash dnf copr enable rhcontainerbot/podman4 -y dnf update dnf install -y podman crun - mode: user script: | #!/bin/bash set -eux -o pipefail systemctl --user enable --now podman.socket probes: - script: | #!/bin/bash set -eux -o pipefail if ! timeout 30s bash -c \u0026#34;until command -v podman \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; do sleep 3; done\u0026#34;; then echo \u0026gt;\u0026amp;2 \u0026#34;podman is not installed yet\u0026#34; exit 1 fi hint: See \u0026#34;/var/log/cloud-init-output.log\u0026#34;. in the guest portForwards: - guestSocket: \u0026#34;/run/user/{{.UID}}/podman/podman.sock\u0026#34; hostSocket: \u0026#34;{{.Dir}}/sock/podman.sock\u0026#34; message: | To run `podman` on the host (assumes podman-remote is installed), run the following commands: ------ export CONTAINER_HOST=$(limactl list podman --format \u0026#39;unix://{{.Dir}}/sock/podman.sock\u0026#39;) podman system connection add lima \u0026#34;unix://{{.Dir}}/sock/podman.sock\u0026#34; podman system connection default lima podman{{if eq .HostOS \u0026#34;linux\u0026#34;}} --remote{{end}} run quay.io/podman/hello ------ I\u0026rsquo;m using a Fedora Base VM Image because there is a custom COPR available which can install Podman v4 in the VM image. Then once I\u0026rsquo;m done installing Podman and crun on the VM, I\u0026rsquo;m forwarding the VM\u0026rsquo;s socket to a socket on the host machine. This is needed so for the Desktop companion to establish connection and verify services running on the guest VM.\nOnce the VM is installed and started, you also need 4 other commands on the host to notify the podman remote CLI to connect to the guest VM rather than listening on the host. Thus once you are done installing Lima, Podman Remote CLI on Host, Podman on Lima VM and the Desktop Companion, you will feel right at home, without missing Docker Desktop.\n","date":"2022-04-03T11:31:44Z","image":"https://blog.sparker0i.me/podman-best-docker-alternative/661c08015ae500253ce7927c_hu_5d6393cd80d74ac5.png","permalink":"https://blog.sparker0i.me/podman-best-docker-alternative/","title":"Podman with Desktop Companion: The best alternative to Docker Desktop"},{"content":"Crypto-currencies are a hot subject today. From seeing multiple bull-runs, to governments trying to destroy the movement of sovereign independence, to countries adopting Bitcoin as a legal tender, cryptocurrencies have come a long way. I have been holding crypto-currencies since 2019, but it was only at the start of this year that I actually started trading and buying them.\nNow, if you ask anyone online about which crypto to buy, you will be frequently met with the term #DYOR - Do Your Own Research. Same was with me as well, whenever I\u0026rsquo;ve asked people on tips, I was told to #DYOR, but there were not many tips online about what to do in DYOR. Like many in the beginning, I\u0026rsquo;ve had my shares of success and failures in crypto hodling and trading. Here are a few tips I follow when looking to buy tokens.\nInitial Research and Filtering Bad Assets Before I hodl or trade any tokens, I make sure to do some initial research with a few pointers. I\u0026rsquo;d feel great about those tokens that meet this initial criteria, while others become a big no-go for the rest of my life (unless something drastically changes in that particular crypto\u0026rsquo;s tech, but would be unlikely).\nThe first thing I do is visit CoinMarketCap website, search for the particular crypto I\u0026rsquo;m looking to hodl or invest. Every major crypto asset listed on CoinMarketCap will have its own website, and that is the first thing I always look at. Within the website, the first thing you should read is the Whitepaper of the token. When you read through the various pages on a particular crypto\u0026rsquo;s website, I am able to mostly detect any non-sense or BS. If it uses any terms or protocols that you find it hard to comprehend - like it \u0026ldquo;promises\u0026rdquo; a huge payout or claims to have some tech that seems to good to be true (like self-mining) - immediately avoid it.\nYou should also look for some more things like Roadmap of features for the particular project and the track record on how it has delivered so far on all those features. If a project has consistently failed or lagged behind to deliver new features, you should treat it as another yellow flag. I\u0026rsquo;d also look at how the crypto\u0026rsquo;s tech functions and the value a token is trying to create. If there are no unique use-cases or if the crypto does not require/benefit from the use of blockchain fundamentals, I\u0026rsquo;d give it a red flag and move on.\nAnother thing that you should really look at is the Blockchain Explorer for a given crypto. First thing I\u0026rsquo;d check here is the max supply for the given crypto. Needless to say, stablecoins are a no-go here. If there\u0026rsquo;s no max supply for a token, it is usually a Red Flag from my side, although there are a very few exceptions for this - like Ethereum. If you still want to go ahead and deal with cryptos without a max supply, you should do it at your own risk and must usually look to own it only for the short-term and not hodl them.\nNext thing you should look at is the distribution of tokens across all wallet holders of the given token. If you see that one wallet holds more than 15-20% of the total current supply, I\u0026rsquo;d raise a red flag here. The moment a big whale dumps (sells) all those coins, the entire crypto will start reeling. Best example would be Vitalik Buterin dumping 45% of the entire Shiba Inu coins in supply, causing a massive drop in the price of the Shiba Inu coin. (More surprising in the case of Shiba Inu is that the Whitepaper of the coin still refers Buterin as a visionary - \u0026ldquo;like its anonymous founder\u0026rdquo; - even after he dumped the project\u0026rsquo;s worth)\nSome other pointers that you can consider to research would be browsing the source code of a given asset on GitHub and the amount of activity in its repos in the recent past. As an alternate means, try going through the blogs and subreddit of the given token and look for any threads that look skeptical to you.\nAvoid Scams When you start off dealing with cryptos, you would be looking at ways to maximize money instantly. While I\u0026rsquo;d say it is not impossible, but I would advise you to not do that. Lured by the opportunity of making quick money, people might fall into the trap of crypto scams.\nMost of these scams happen on Telegram within groups which usually advertise quick money making schemes in the form of Token Airdrops. They will mostly look to lure you with \u0026ldquo;verified\u0026rdquo; token deposit screenshots from customers. These \u0026ldquo;customers\u0026rdquo; are either running the scam along with the admins of the group, or are accounts managed by the admins themselves. If you look to participate in such airdrops, you will end up losing all your life savings.\nHowever, I\u0026rsquo;m not saying all airdrops are bad. Some of them that have happened - like the Stellar Airdrops in 2017 and 2019. But that was done by a verified entity - the Stellar Foundation. Here we are talking about Telegram groups that are run by Anonymous Users. Such groups are always Bait and Scam, and I would recommend you to avoid them.\nFill out an Investment Checklist Once I\u0026rsquo;m done filtering out the coins I invest in, I look to create an investment checklist where I have noted certain questions that I ask before I invest in a coin. Here are the details of the questions:\nWhat is the problem that a crypto is trying to solve? Like I mentioned in the previous section, if a token does not serve a purpose or does not solve a problem - not just with blockchains or the financial sector, but any sector, say Education or Supply Chain - I believe there is no reason for that coin to exist. Such coins are only hype driven and only good for short-term trading. I will call such hype-driven coins as Shitcoins from now. Hodling them for a long term will leave you reeling. What is the Dev Team like? What is their track record? How are they funded, organized? I will look for is the history of some of the Lead members on a project and see what they have worked on previously - via information they post online (say on LinkedIn). If they are anonymous members (exception: Satoshi Nakamoto, the creator of Bitcoin), I\u0026rsquo;ll automatically raise a Red flag on that crypto and move on. Then, I will also look at how they are funded. Whether they have partnerships with Enterprises or they have solved a critical problem for any pillar of the society - say government or law - or any other major piece of news regarding any advancement that was made possible because of a crypto. Who is their competition and how big is the market they\u0026rsquo;re targeting? What is the roadmap they created? Like I\u0026rsquo;ve mentioned in the previous section, you should always have a look at the roadmap for a given token. No roadmap -\u0026gt; No progress -\u0026gt; Red Flag. Also you need to look at the other coins being offered in the market and how do they compete with the token you\u0026rsquo;re looking to buy. Because healthy competition always leads to success, unless you\u0026rsquo;re looking for a shitcoin, or the competitors of a given token are all shitcoins. Is there a staking mechanism or is it transactional? There are cryptos which rely on various types of consensus mechanisms - Proof of Work (PoW), Proof of Stake (PoS), Proof of Time and Space etc. How does the token/coin actually derive value for the holder? What are the weaknesses or problems with this crypto? Of course there can never be some shortcoming associated with a given crypto. If there are none, it sounds too good to be true. (Even for Bitcoin, there\u0026rsquo;s only one disadvantage that I can think about - Electricity consumption. But that too is definitely lesser than electricity used for mining actual Gold, or by the entire banking system around the world) Create a Valuation Framework for a token With the boom that crypto trading has seen since November 2020, you must always be sure of what coins you want to invest in. I\u0026rsquo;ve been a part of various crypto groups on multiple sites like Facebook and Reddit. In it, I\u0026rsquo;ve seen a lot of people complain about the losses they\u0026rsquo;ve had to made while dealing with crypto, because they either bought high and then the market went bearish, or they invested in the pump-and-dump shitcoins.\nTo avoid becoming one among such people, you need to have a certain valuation model for a given crypto. Even if you create a basic one, you\u0026rsquo;ll go miles ahead of your peers/other people. Here are some simple things you can consider doing:\nLook at the total market cap of the coin and compare it with the size of the market it is trying to address. Market cap here includes not only the circulating supply, but also the max supply possible for the crypto. Check for the total number of users of a given token. I believe that the value of a given crypto would be proportional to the number of users using the crypto. The best known crypto out there is Bitcoin and there are almost 100 Million people already using it. Compounded with its scarcity, its scale and community is why Bitcoin carries a huge value. These were 2 very basic checks that I do for any crypto. I might add some other checks in the evaluation framework as well. Once you have a model setup, you can then evaluate one crypto against another, and also see why you would want to own this coin rather than do short-term trading. Doing this will lead you to think long term about dealing with a given crypto and think more about the \u0026ldquo;value\u0026rdquo; it provides.\nOnce you do this, you will have more mental peace and have good confidence for your decision making. You would also not panic when there are any short-term price dips.\nPortfolio Allocation This is one of the crucial aspects of your journey with crypto. You should think on how much fiat you can use to buy crypto, as well as how do you want to allocate your crypto portfolio between \u0026ldquo;safe\u0026rdquo; and \u0026ldquo;rigged\u0026rdquo; cryptos. (planned pump-and-dump, shitcoins etc.). If you\u0026rsquo;re just starting out, I would recommend you to not go with shitcoins to begin with. I have seen a few of my friends who started their crypto journeys because of celebrities who rig coins like Elon Musk and co. The peak of this came during Elon\u0026rsquo;s SNL appearance. Despite my advice against it, most of my friends bought Dogecoin only because of Elon\u0026rsquo;s appearance and thinking it\u0026rsquo;s price will keep going up if they bought then. Little did they know that it was the starting point for dumping Dogecoin, whose effects can still be felt today. Many of my friends felt cheated, and some of them have left the crypto space despite my assurances that not all cryptos are bad.\nSo if you\u0026rsquo;re just starting out, I\u0026rsquo;d recommend having 80-90% of your portfolio filled with safe coins. Once you become more informed and confident, you can start dealing with Shitcoins and vary your portfolio allocation. You should also think in terms of crypto categories as well as the percentage of your portfolio in each of these segments. For starters, I categorise crypto into following categories: (You may disagree with how I\u0026rsquo;m grouping the cryptos, but we can agree to disagree)\nCore Holdings: Bitcoin (BTC), Litecoin (LTC) Smart Contracts: Ethereum (ETH), Cardano (ADA), Polkadot (DOT), ALGO, Solana etc. Crypto with Privacy?: Monero (XMR), ZCash Intermediary with fiat dealings/Bank Settlement: Stellar (XLM), Ripple (XRP) Enterprise Solutions: VeChain etc. Promising: NANO, IOTA, RVN, Algo etc. Coins without intrinsic value (Shitcoin): DOGE + the army of dog coins (Shiba, Kishu) etc. You should also have a fair idea about the market conditions as there is a lot of uncertainty today, mostly due to policies+governments and others due to rigging shitcoins. Due to this uncertainty, you should best stick to the core holdings and then pick up coins in segments you are knowledgeable enough about. Example, if you don\u0026rsquo;t know about Smart Contracts, how can you be sure that ALGO is a game changer?\nThis is where portfolio diversification and allocation comes into place. You should look to diversify, but in the ever-changing world of crypto you shouldn\u0026rsquo;t look to diversify way too much. It would be difficult for you to keep up with all the changes that have occurred across cryptos. I wouldn\u0026rsquo;t recommend you to have more than 10-12 cryptos. If you have more, I would recommend you to consolidate to the few segments you are well knowledgeable about.\nLearn Everyday If you aren\u0026rsquo;t doing this already, read a bit daily on cryptocurrencies. There are decent YouTubers that talk about the market movements in the crypto space. You should also learn more about the underlying principles in the crypto space. More specifically, if you are not aware of basics like Proof-of-Work (PoW), Proof-of-Space (PoS), learn about it first.\nIf you invest in stocks, one factor you would look for is what are the core offerings of a given company and how it performs against other peers. If you don\u0026rsquo;t know about how the underlying technology of a crypto works, learn about it as well. That will increase your belief if you want to buy that crypto. If you don\u0026rsquo;t care about the underlying technology or find reading about it very tedious, I believe you shouldn\u0026rsquo;t be in this crypto space at all and shouldn\u0026rsquo;t look to invest here, otherwise you will feel betrayed after your coins\u0026rsquo; value gets dumped.\n","date":"2021-08-06T04:20:59Z","image":"https://blog.sparker0i.me/factors-to-look-dealing-crypto/661c08055610fb416dbc1885_hu_5bd6a168b1acfbc0.png","permalink":"https://blog.sparker0i.me/factors-to-look-dealing-crypto/","title":"Factors to look at while dealing with Cryptocurrencies"},{"content":"Advertisements are everywhere - You see ads on your newspaper, your family sees ads on TV channels, there are ads on billboards seen by people in and around an area, and you too see ads on almost any website you visit.\nBut, internet advertising industry is broken. It\u0026rsquo;s failing the users and the advertisers themselves. Internet advertising majorly uses targeted/personalized ads to go after you and cater products you would most likely want. This ends up collecting as much data about you as it can.\nIn this blog post, I will argue why the current model of internet advertising is bad, and how can you save yourself from being a victim of personalized ad targeting.\nA Regular User\u0026rsquo;s Perspective on Advertising If you ask me what are the problems with the current model of advertising online, here are a few things I might suggest:\nAds violate privacy: There\u0026rsquo;s no secret to this, internet advertising scoops up as much information about you from the services you visit and use. Ads are disruptive: As an example, sometimes you click on a blank area in a website, you then see a new pop-up window with an ad, despite your browser blocking pop-ups Ads are annoying: Sometimes you visit an interesting website, and the very next moment when you open a social media website, you start seeing ads about this almost everytime. Malicious Ads: A vast majority of the people of my generation shouldn\u0026rsquo;t be worried about Malicious ads, as our generation is aware about websites and malware, and we\u0026rsquo;d only visit website after due diligence. But our previous generations are not so well-informed about websites, they would click on any website and that site may end up spreading malware. What I\u0026rsquo;m worried about is the first point - Digital Advertising violating our privacy. While our previous generations are not much aware about their Data Privacy, our current generation does worry about it.\nWhile we sit here, worrying about our privacy, IMF has \u0026ldquo;suggested\u0026rdquo; that a person\u0026rsquo;s credit history should be based on their Web History. That\u0026rsquo;s how ridiculous things may get in the future if things are not corrected today. A 2018 study by researchers at Data and Society concluded that “today’s digital advertising infrastructure creates disturbing new opportunities for political manipulation and other forms of antidemocratic strategic communication.”\nEven without concerns of privacy violations, there have been many efforts to try and fix some of the issues:\nCoalition for Better Ads Acceptable Ads Brave These only fix some of the problems with Advertising in general, not the entire problem.\nAttention Economy Multiple advertisements craving for your attention\nHerbert Simon argued in the 70s that when information becomes abundant, attention becomes the scarce resource. In today\u0026rsquo;s modern age, we’re living through the pendulum swing of that reversal—yet we consistently overlook its implications.\nWhen you visit any website or browse through any app, you take some time out of your life - or your attention to visit that resource. You are also using some of your attention to read this post (for which I\u0026rsquo;m eternally grateful). Now, you normally wouldn\u0026rsquo;t take out some of this attention to visit a website that you haven\u0026rsquo;t clicked by yourself. Thus, such websites/apps allow to advertise within itself (mostly as a small banner) via publishers, so that you don\u0026rsquo;t have to take out extra time off your life just to visit the standalone ad. Then, such publishers use your \u0026ldquo;actions\u0026rdquo;, and use such individualized data about you - collected via algorithms - and sell slots to advertisers to display ads.\nWikipedia has a perfect definition for this - Attention Economy.\nWho wouldn\u0026rsquo;t love to have an individualized feed of news. Sadly, individualization that happens today is neither for our own benefit nor for getting a peace of mind. Rather such a system only invites more outrage from us - users.\nNow, an incentive driven advertising system is good for the publishers, who earn good money; and advertisers, who get to sell lot more of their products. But this is bad for the user. There are literally billions of dollars being spent to figure out how to persuade you to look at a certain thing over a competitor\u0026rsquo;s; to care about one thing over another. This is the way most of the information in this world is being monetized today.\nWe can all see the effects of Attention Economy - Social Media websites like Twitter and Facebook never want people of different groups to stop fighting - be it over religion, sports or their political bias. This is because these networks are optimized in a way to promote fighting, or spread fake news/propaganda very quickly. You may argue that companies are investing to stop fake news, but I believe that it is still relatively minor as they don\u0026rsquo;t really want their revenue streams to go down.\nMy take on digital ads I don’t want any relevant ads, or any ad-driven products, ever. Other people might feel differently, but collecting the data is only half the problem. That data can be abused, even if only client-side - for example targeting could exploit you and persuade to buy stuff you don’t need. The whole point of ads is to change your behavior, if only slightly, with advertisers literally bidding for your attention, and then playing mind tricks, which work because of how our critical thinking often fails us a majority of the times.\nI believe great products have often been successful via word of mouth referrals. A great example would be Cred. Only a few credit card users in India were aware of the Cred before September 2020. That month, a delayed IPL 2020 started, and Cred ads - which a majority of the Indians never understood - was everywhere on TV, getting people\u0026rsquo;s interest in knowing about the product, and eventually end up using it. There are tactics that don’t involve tricking people into changing their behavior. Conventional advertising is better than the targeted ones, because it’s not individualized. At least everybody else is seeing the same shit that you’re seeing.\nThat brings us to the most important question - What should companies do? Honestly speaking, I don\u0026rsquo;t think that\u0026rsquo;s our problem. I don\u0026rsquo;t think any company should be entitled to our data or our attention. When was the last time you ever saw an ad that made you feel this is the real deal. I haven\u0026rsquo;t, have you?\nYou know how the unethical behavior of companies is always excused via the requirement of company stakeholders to make money? Well, this goes both ways. We, the consumers, aren’t running any charity. I strongly feel it isn’t our job to save such dying business models.\nFirst Party Ads One solution to the problem could be the content providers (websites/apps) serving ads based on content that is being viewed right now. This might ensure there is no need to keep user\u0026rsquo;s profile and history. Your data will no longer be shared to third-parties and hence the quality of ads shown will get better - which also means no scam ads and no malvertising.\nThis too has a problem. Even if there is a whitelist for “acceptable ads”, the problem is those ads are still downloaded Javascript code, with no way to review what they do. Further, third-party requests could get disguised as first-party ones. In such cases, you have to trust the publishers, which I\u0026rsquo;d usually find a hard time to do.\nThen, Wouldn\u0026rsquo;t Ad-Blocking affect the Internet? Given a choice between receiving ads, and no ads, sane people would obviously choose no ads. Nobody likes ads. If its forced, we have to gulp it down our throat, because either there\u0026rsquo;s no clear alternative (Facebook), or the price of no-ads option is way too high (Youtube, Spotify, Apple Music, Hotstar et al.).\nAlmost no browser ships with an aggressive ad-blocker enabled by default. This is because somewhere or the other, their business model is also dependent on ads, even if they don\u0026rsquo;t serve ads directly. It will only make the end user\u0026rsquo;s experience much better. Many browsers that claim to do aggressive ad-blocking like Microsoft Edge, Brave, Samsung, Vivaldi et al. earn a lot of money from advertising.\nNow, wouldn\u0026rsquo;t ad-blocking affect the Internet\u0026rsquo;s health? Yes, it will. Negatively, on the advertisers and publishers\u0026rsquo; wallets; Positively on the rest of the internet, because it can only get better from there on.\nSolutions Pay for the things you use A wise man once said,\n\u0026ldquo;If you are not paying for a product, you are the product\u0026rdquo;.\nIf I really enjoy something, I don\u0026rsquo;t mind paying for it. For example, I\u0026rsquo;ve been using Hey email since July. It did not catch my attention because of all the amazing features it brings, but rather because of its infamous feud with Apple. In short, Hey is a radical re-thinking of what an Email client should be. As one Twitter user puts it:\nIn positive news, https://t.co/lwKSOQNcFe seems to have finally solved email (!!!). Been using it several weeks and no longer dealing with spam, long lists of “unread” messages, or sorting out annoying but important docs. The relief is so real 🙌🏼\n— Dr. Darya Rose 🇺🇸 (@summertomato) June 10, 2020\nI\u0026rsquo;ve been using it so far without any major problems. All advertising mails are now screened out, so that they never take my attention again. I\u0026rsquo;ve changed emails for all my online accounts to reflect to my hey.com email address. Now there are certain services like stock and mutual fund brokers, whose email change process has to be offline. Once all of my services are moved over to Hey, I\u0026rsquo;m deleting all my non Hey mail accounts (except primary Google Play Store and Microsoft Store accounts, because all my app/in-app purchase history is sitting in those 2 accounts).\nSimilarly, I\u0026rsquo;m also using Spotify Premium because I don\u0026rsquo;t want myself to be exposed to their ads right after one song.\nI also pay for Xbox Game Pass for PC. It has more than 100 AAA PC games that I can easily play on my Lenovo Legion Y740 laptop. There are no ads in the games that are played via XGP. I regularly play Age of Empires 2: DE, Flight Simulator, Doom Eternal, Moto GP20 and The Outer Worlds. Buying all those games individually would\u0026rsquo;ve cost me a fortune, but at Rs 489/month (577 if you include taxes) it is a killer deal, specially for gamers like me. I am also using Kindle Unlimited for Reading as many Books as I can.\nWith all the information I\u0026rsquo;m unearthing every single day about the shady practices of all developers, I\u0026rsquo;m removing all ads-enabled apps from my mobiles. Moreover, if I find that any app is sharing data that is unwanted, I promptly uninstall that app. If at all I really need to use the app\u0026rsquo;s services which provide ads, I would find a browser-based alternative for it - Eg. Youtube, Twitter, Facebook, IG, Amazon + Entertainment apps.\nBlock All Ads Even if you may think that some ads are fine, but do you know whether those are targeted specifically to you or not? Even if there\u0026rsquo;s a whitelist of \u0026ldquo;acceptable ads\u0026rdquo;, it\u0026rsquo;s still Javascript code downloaded from the internet, which I have no idea about its functionality. It\u0026rsquo;s still intrusive, and drives you to buy things you don\u0026rsquo;t really want.\nI really think it is wrong that Publishers think they can run code on our devices. If publishers think otherwise, they can feel free to block me from using their services. If I see any website that serves me ads (and gets caught by my ad blockers), I\u0026rsquo;d really start looking for alternatives.\nBlocking for all devices DNS blocking has worked well so far. That is until apps/sites starting using DNS-over-HTTPS over their own servers. Still there are techniques to circumvent this too.\nPiHole: I had a Raspberry Pi 3B sitting idle at home for the last 1 year or so. When I got to know that I could run a service like Pihole on it, I wasted no time to set it up. Ever since Christmas eve, I\u0026rsquo;ve been using Pihole to block ads for all the devices my family uses. And the stats are great too:\nPihole stats since December 23, 2020\nThough the only downside to this is that I\u0026rsquo;ve had to change my router to use static IP address assignment. There was some initial hassle in setting up my router and the devices, but once that was done, ads were a thing of the past (except Youtube ads, for which there is no working solution in 2021. Screw you, Google).\nHosted services: 3 months before trying out Pihole, I tried using NextDNS. While the results were incredible, I felt that giving away Rs. 159 every month (187 including taxes) was a bit too much for me. I\u0026rsquo;ve also heard about Adguard DNS, but I haven\u0026rsquo;t used it so far.\nIndividual Devices Desktop/Laptop + Android devices Even before I\u0026rsquo;ve been using PiHole, I had experimented with ad blocking via browser extensions. The best combination I\u0026rsquo;ve found is Firefox + uBlock Origin + Privacy Badger. It simply works.\nuBlock Origin is by far the best ad blocker I\u0026rsquo;ve ever used. Ever since installing it, I\u0026rsquo;ve noticed far better loading speeds, because there are lesser resources to be loaded. It also protects against CNAME cloaking, where 3rd party requests masquerade as 1st party ones and you might not even notice it.\nSadly, this won\u0026rsquo;t work with Google Chrome and other Chromium forked browsers in the near future. This is because Manifest v3 for Chrome extensions will be deployed in Google Chrome sometime in January (with the release of v88). Simply put, Manifest is something any extension developer will have to follow while developing an extension. This particular Manifest v3 changes the way ad-blocking extensions function. This negatively impacts ad-blockers and uBlock is one of them.\nMy take on this will be to ditch Chrome/Chromium forked browsers ASAP. Switch to Firefox while you still can, because Mozilla\u0026rsquo;s financials aren\u0026rsquo;t great, and the browser could go kaput anytime.\nThe same combination also works well for Android as well, just that you will have to update the Firefox browser to whichever latest version is available.\nWhat about Brave Browser? Brave browser displaying ads from Brave Network via OS notifications\nBrave is yet another Chromium fork which focuses on user privacy by blocking trackers, scripts and ads by default. So inside some pages where ads don\u0026rsquo;t load properly, you might be forced to turn down Brave Shields, which will then open up the website to show you ads.\nBy the time you\u0026rsquo;ve reached here, you must\u0026rsquo;ve realised that internet is up and running, all thanks to advertisements. The content you might be seeing for free is able to remain free because of advertising. This is profitable for both advertisers and publishers, but not the users.\nBrave attempts to change the advertising model by flipping the pyramid upside down. What I mean is, users get paid for getting advertisements on their devices. It allows users to opt-in to its own Brave rewards system (in other words, another advertising system). Brave serves ads after blocking ads from third party providers on websites, and then start showing ads from their own advertising platform via OS level notifications.\nOnce you\u0026rsquo;ve opted-in, Brave will display you \u0026lsquo;privacy-respecting\u0026rsquo; ads (it\u0026rsquo;s still an advertisement btw 😂) and if you receive an ad, you will be rewarded with a BAT cryptocurrency (BAT = Basic Attention Token), which you can use to support Brave verified creators (You can\u0026rsquo;t withdraw those tokens to your crypto wallet yet, that\u0026rsquo;s coming soon though).\nBrave intends to serve ads as per user interests, in an anonymised way. This could be either via client-side profiling, or simply piggybacking what a user is currently viewing. While rewards and ads maybe opt-in for users, they are not for publishers, leaving us in a very awkward position. If a publisher wants a cut of the rewards, they need to be a part of the Brave Partner program.\nWith this behaviour, I feel that Brave is yet another middleman between the user and the advertiser. Other companies also have the same model of running their business. You are trusting an advertising company (Brave) - just like Google - to preserve your privacy. What an irony.\nConclusion I want the targeted advertisement system to die. I will go to all lengths to block all ads. I feel companies are not entitled to my data or my time (\u0026lsquo;attention\u0026rsquo;).\nRead More:\nThe cowardice of Brave Ten arguments for deleting your Social Media account right now Brave taking Cryptocurrency donations \u0026ldquo;for me\u0026rdquo; without my consent ","date":"2021-01-23T03:33:00Z","image":"https://blog.sparker0i.me/health-targeted-digital-advertising-today/661c080c8442574e70467574_hu_cccafdd555627c6e.png","permalink":"https://blog.sparker0i.me/health-targeted-digital-advertising-today/","title":"Is the state of targeted digital advertising broken?"},{"content":"In one of my previous blog posts, I\u0026rsquo;d mentioned that Spark 3.0 is coming with Native GPU support. A few days after that, Spark 3.0 released on 18th June 2020. While it did release, there were no mentions of how to run your Spark 3.0 code on a GPU anywhere on the internet. It changes now.\nIn this post, you\u0026rsquo;ll see the prerequisites for running Spark on GPU on a local machine, as well as all installation instructions.\nPrerequisites To run Spark applications on your GPU, it is recommended that you have an Nvidia GPU of Pascal Architecture or better. This means that you will need an Nvidia Geforce GTX 1050 or better. Other requirements are the same as Apache Spark requirements.\n(PS. I don\u0026rsquo;t have an AMD GPU, so can\u0026rsquo;t really test and confirm whether this will work with it or not, but chances are very slim as you need a tool called nvidia-smi, which works only with Nvidia GPUs)\nYou will also need to install Apache Spark 3.0, Nvidia CUDA on your machine.\nOther than these, you will also need 2 JARs: Rapids Accelerator and NVIDIA CUDF (for CUDA 11).\nYou will also need a Linux system to run your jobs. This won\u0026rsquo;t work on Windows as CUDF isn\u0026rsquo;t supported on that platform. However, the CUDF team says they will support CUDA Running on WSL 2.0. To get CUDA Running with WSL, you\u0026rsquo;ll need to be a part of the Windows Insider Program.\nYou will also need a GPU Discovery script which tells the program the addresses of GPUs available on your system. Fortunately, the Spark repo has a GPU discovery script handy which can be readily used.\nRunning For Spark 3.0 to recognize that you will be running your jobs on a GPU, you need to pass a few parameters as Spark confs:\nspark.rapids.sql.enabled as true spark.plugins as com.nvidia.spark.SQLPlugin spark.driver.resource.gpu.discoveryScript as You can either run this with spark-shell or you can create your own JAR and run it using spark-submit and then pass these configurations.\nPerformance In order to illustrate the performance difference between running your Spark program on a CPU vs GPU, I will be using a very simple program which is very much self explanatory:\n1 2 3 4 5 6 7 val values: List[Int] = List(100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000, 10000000, 50000000, 100000000, 500000000, 1000000000) for (upperBound \u0026lt;- values) { val df = sc.makeRDD(1 to upperBound).toDF(\u0026#34;a\u0026#34;) val df2 = sc.makeRDD(1 to upperBound).toDF(\u0026#34;b\u0026#34;) println(df.join(df2, $\u0026#34;a\u0026#34; === $\u0026#34;b\u0026#34; / 2).count()) } Spark program for testing performance: CPU vs GPU\nFurther, in order to level the playing field between the 2 runs, I\u0026rsquo;m setting certain common configs:\nspark.locality.wait = 0s spark.driver.memory = 10G spark.sql.files.maxPartitionBytes = 512 * 1024 * 1024 spark.sql.shuffle.partitions = 10 Here are the specs of the laptop which I used to perform this test:\n6-core Intel Core i7-8750H 16GB DDR4 RAM, 256GB NVME SSD 8GB Nvidia Geforce RTX 2080 Graphics Card Here are two plots showing the upperBound against time taken:\nchart created with amCharts | amChartschart created with amCharts | amCharts\nAs you can see from the graphs above, for very less records - with sizes within a few Megabytes - it is faster on the CPU than on the GPU because of the less time taken to propagate the results.\nBut things change for the better, when a high volume of records have to start processing. For very high records, you can see a difference of almost 3x.\nMoreover, for 1000000000 records (the last one), my Spark program crashed when run against the CPU. So the 13 minutes that you see above was until when it was successfully running.\nConclusion To confirm whether your program is running against the GPU or not, you can go to the SQL tab, select your job, and then you will see something like GpuRowToColumnar, indicating that the job is running against the GPU.\nSpark running on GPU\nSo if you\u0026rsquo;ve got heavy workloads, try and offload them to the GPU as much as you can :)\n","date":"2020-09-19T03:44:00Z","image":"https://blog.sparker0i.me/run-spark-3-applications-on-gpu/661c08139bb70e9dac2bfee1_hu_95497076141ddc8b.png","permalink":"https://blog.sparker0i.me/run-spark-3-applications-on-gpu/","title":"How to run Spark 3.0 applications on your GPU"},{"content":"Note: This is the second article of the series: Cool Spark ML. The other parts can be found below:\nPart 1: K Nearest Neighbours Part 2: Preprocessing of Data (current) People who have been performing Machine Learning for quite a long time know that Data Preprocessing is a key step before running any algorithms on the data. In a majority of datasets, you might always find null, or incomplete values. The data would also be inconsistent across columns, which directly affects algorithms using distance measures.\nThis is where Data Preprocessing comes in. It is a crucial step which involves cleaning and organizing the data to make it suitable for building models. In other words, if you don\u0026rsquo;t perform Preprocessing, your models may not be accurate.\nWhile there are quite a lot of articles online about Data Preprocessing in Python, there aren\u0026rsquo;t a lot of them in Spark, or even Scala. In this post, I will be dealing with the ways you can perform Data Preprocessing in Spark on Scala.\nPS. You might be asking why I\u0026rsquo;m dealing with this now when I have actually written KNN in Spark before. The truth is, KNN isn\u0026rsquo;t officially supported inside Spark ML module. What I wrote in the previous article was a top-to-bottom version of KNN performed using Spark. You can also say that I\u0026rsquo;m doing a complete reset of this series 😅\nJust like always, the codes for all posts in this series will be available on my GitHub repo.\nTypes of Preprocessing in Spark There are two types of preprocessing:\nNumeric Data Text Data Numeric Data There are three ways you can preprocess numeric data in Spark:\nNormalize Standardize Bucketize To illustrate Normalize and Standardize, I\u0026rsquo;ll be using some Scala magic which will generate my points as a Vector. Each vector represents a point in a 3-Dimensional Space.\n1 2 3 4 5 6 7 8 9 val points = for (i \u0026lt;- 1 to 1000) yield (i, Vectors.dense( Array( (math.random * (10 - 1)) * i + 1.0, (math.random * (10000 - 1000)) + 1000.0, math.random * i ) )) val featuresDf = points.toDF(\u0026#34;id\u0026#34;, \u0026#34;features\u0026#34;) Doing the above results in the following DataFrame:\nEach element inside Features column represents a point in a 3-D space.\nNormalize Normalization is the process of mapping numeric data from their original range into a range of 0 to 1. The lowest value of the original range gets value of 0, and the highest gets the value 1. All the other values in the original range will fall between these two.\nThis is important because there may be multiple attributes with different ranges. E.g. Salary values may range between 3 and 8+ digit numbers, years in company will be between 1- and 2-digit numbers. The reason we want to normalize those attributes in a [0,1] range is so that when algorithms that use distance as a measure, they don\u0026rsquo;t weigh some attributes like salary more heavily than others.\nThe formula to convert values in an un-normalized column to a normalized form is given by:\nNormalization Formula\nWhere:\nx is the value inside a column to be normalized, x(new) is the normalized value, x(min) is the minimum value of that column, and x(max) is the maximum value of that column Working on the featuresDf created above, we will import MinMaxScaler from the org.apache.spark.ml.feature package. We now have to create an instance of the MinMaxScaler. It will take two parameters: Input column name, and an Output Column name. This object will transform the contents of the input column vectors into a scaled version, and save it into the output column.\nIn our case, we will be using our features column inside featuresDf as the input column, and our output column will be named sFeatures. We create the instance in this manner:\n1 2 3 val featureScaler = new MinMaxScaler() .setInputCol(\u0026#34;features\u0026#34;) .setOutputCol(\u0026#34;sfeatures\u0026#34;) Next, we have to fit the data present in our featuresDf inside this featureScaler and later transform to create the scaled data. This is done using the code below:\n1 2 val scaledDf = featureScaler.fit(featuresDf) .transform(featuresDf) Transforming original values into normalized ones\nNow, if we have a look at our transformed data:\nNormalized DataFrame\nYou can then use this new sFeatures to calculate distances among points.\nStandardize Now, we may have data whose values can be mapped to a bell-shaped curve, or normally distributed but maybe not exactly. With standardization, we map our data and transform it, which has a variance of 1 and/or a mean value of 0. This is done because some machine learning algorithms, like SVM, work better this way.\nThus, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. The formula to convert values in a non-standardized column to a standardized form is given by:\nStandardization Formula\nWhere:\nx is the value to be standardized x(new) is the standardized value μ is the mean of the column σ is the standard deviation of the column. Again, we will be using the featuresDf created above. We will import StandardScaler from the org.apache.spark.ml.feature package. Just like MinMaxScaler, an instance of StandardScaler will require an input column and an output column. In our case, we will still continue with features and sFeatures. We will then fit the data inside the scaler and later transform the data. I\u0026rsquo;ve combined both these steps into a single code snippet:\n1 2 3 4 5 6 7 8 val featureStandardScaler = new StandardScaler() .setInputCol(\u0026#34;features\u0026#34;) .setOutputCol(\u0026#34;sfeatures\u0026#34;) .setWithStd(true) .setWithMean(true) val standardizedDf = featureStandardScaler.fit(featuresDf) .transform(featuresDf) Now if we have a look at our transformed data:\nStandardized Numeric Data\nWait, weren\u0026rsquo;t the values supposed to be scaled within the range of [-1, 1]? Well, that\u0026rsquo;s the surprise associated with the StandardScaler. It uses the unbiased sample standard deviation instead of the population standard deviation.\nIn other words, while the standard deviation will be 1 (or very close to 1), the mean may not be necessarily 0. To scale your data in a way that the range of numbers is between [-1,1] and the standard deviation is 1 and mean 0, you will have to follow this accepted StackOverflow answer. Even otherwise with this process, the data has been standardized.\nBucketize Bucketization is done when we have to organize continuous ranges of data into different buckets. Bucketizer allows us to group data based on boundaries, so a list of boundaries has to be provided. I will call it splits with the domain of all buckets when added looks like: {(-∞, -500.0) ⋃ [-500.0, -100.0) ⋃ [-100.0, -10.0) ⋃ [-10.0, 0.0) ⋃ [0.0, 10.0) ⋃ [10.0, 100.0) ⋃ [100.0, 500.0) ⋃ [500.0, ∞)}.\nThen I\u0026rsquo;ll generate 1000 random points that fall in the range of [-10000.0, 10000.0] and save it in a DataFrame with column name as features. This is done using the below code:\n1 2 3 4 val splits = Array(Float.NegativeInfinity, -500.0, -100.0, -10.0, 0.0, 10.0, 100.0, 500.0, Float.PositiveInfinity) val bucketData = (for (i \u0026lt;- 0 to 10000) yield math.random * 10000.0 * (if (math.random \u0026lt; 0.5) -1 else 1)) val bucketDf = bucketData.toDF(\u0026#34;features\u0026#34;) Now, our Bucketizer needs three inputs: the splits, input column name, and output column name. Then I\u0026rsquo;ll transform that data which would then give me the element and which bucket it belongs to:\n1 2 3 4 5 6 val bucketizer = new Bucketizer() .setSplits(splits) .setInputCol(\u0026#34;features\u0026#34;) .setOutputCol(\u0026#34;bfeatures\u0026#34;) val bucketedDf = bucketizer.transform(bucketDf) Notice that I didn\u0026rsquo;t have to do a fit operation before doing a transform. This is because Bucketizing is fairly simple and you only need to find which bucket a number belongs to. Thus, there are no operations like scaling which happened in the other 2 sections, and hence you don\u0026rsquo;t need to fit your data. Now if we have a look at the created DataFrame:\nBucketized DataFrame\nNow you might also want to know how many numbers are there in a particular bucket. So, I will do a groupBy on bFeatures column and retrieve the count of occurrences. The following code does that and displays my generated data:\nFairly easy, isn\u0026rsquo;t it?\nText There are two ways in which you can preprocess text-based data in Spark:\nTokenize TF-IDF To illustrate both of them, I will be using sentencesDf created using this code:\n1 2 3 4 5 val sentencesDf = Seq( (1, \u0026#34;This is an introduction to Spark ML\u0026#34;), (2, \u0026#34;MLLib includes libraries for classification and regression\u0026#34;), (3, \u0026#34;It also contains supporting tools for pipelines\u0026#34;) ).toDF(\u0026#34;id\u0026#34;, \u0026#34;sentence\u0026#34;) Tokenize In tokenization, you map your string containing a sentence into a set of tokens, or words. As an Example, the sentence \u0026ldquo;This is an introduction to Spark ML\u0026rdquo; can be mapped into a list of 7 words - {This, is, an, introduction, to, Spark, ML}.\nWe will first import Tokenizer from the org.apache.spark.ml.feature package. Now an instance of this will need two parameters - input column and output column. Our input will be sentence and the output will be words, because that is what the Tokenizer will produce. Then we will apply transform on the sentences above.\nNow, just like bucketing, we are not fitting any data here. Tokenizer already knows its job - Split strings into the separate words. The above process is illustrated in the code below:\n1 2 3 4 5 val sentenceToken = new Tokenizer() .setInputCol(\u0026#34;sentence\u0026#34;) .setOutputCol(\u0026#34;words\u0026#34;) val sentenceTokenizedDf = sentenceToken.transform(sentencesDf) Now, if we have a look at our data:\nThe words column contains lists of words that have been broken up in the ways you would expect a regular expression pattern matching to break up a sentence into words - based on white space, punctuation, etc.\nEasy, isn\u0026rsquo;t it?\nTerm Frequency-Inverse Document Frequency (TF-IDF) Here we map text from a single, typically long string, to a vector, indicating the frequency of each word in a text relative to a group of texts such as a corpus. This transformation is widely used in text classification.\nTF-IDF captures the intuition that infrequently used words are more useful for distinguishing categories of text than frequently used words. Considering the above figure as an example, Normalizing appears only once, to appears twice and so on. Like this, we go through all the documents in our corpus, which is nothing but a collection of documents. Then we count up how often a term appears across all of the documents. In this example normalizing is a very rare word. Whereas other words like maps, data and to show up more frequently. We use these two sets of counts and feed those two into the term frequency-inverse document frequency calculation. And that gives us our TF-IDF measures.\nI will use the same sentenceTokenizedDf created above for this exercise as well. Just like other processes mentioned above, we will need to import a few things from org.apache.spark.ml.feature package - HashingTF (for hashing Term Frequency), IDF (for Inverse Document Frequency), Tokenizer.\nFirst, I will create a HashingTF instance - which takes an input column (words), an output column (rawFeatures) and the number of features to keep track of (20) as the parameters. Now we apply our transformation on this and get a new DataFrame:\n1 2 3 4 5 6 7 val hashingTF = new HashingTF() .setInputCol(\u0026#34;words\u0026#34;) .setOutputCol(\u0026#34;rawFeatures\u0026#34;) .setNumFeatures(20) val sentenceHashingFunctionTermFrequencyDf = hashingTF.transform(sentenceTokenizedDf) sentenceHashingFunctionTermFrequencyDf.show() Now if we have a look at our data, it has added an extra column which is of Vector type. It has mapped each word to an index, so for example, this maps to 1, is maps to 4, an -\u0026gt; 5, and so on.\nNow we\u0026rsquo;re going to scale the rawFeatures vector values and we\u0026rsquo;re going to scale them based on how often the words appear in the entire collection of sentences. To do this we\u0026rsquo;re going to create an IDF instance. Again, we have to specify an input column (rawFeatures) and an output column (idfFeatures) as parameters.\nLet\u0026rsquo;s use the term frequency data we just calculated to fit the inverse document frequency model. And to do that I\u0026rsquo;m going to create an idfModel, and we\u0026rsquo;re going to call the idf object I just created, and I\u0026rsquo;m going to fit it using our term frequency data. Then we apply the IDF transformation to create a new DataFrame that has both the term frequency and the inverse document frequency transformations applied.\n1 2 3 4 5 6 val idf = new IDF() .setInputCol(\u0026#34;rawFeatures\u0026#34;) .setOutputCol(\u0026#34;idfFeatures\u0026#34;) val idfModel = idf.fit(sentenceHashingFunctionTermFrequencyDf) val tfIdfDf = idfModel.transform(sentenceHashingFunctionTermFrequencyDf) Now if we have a look at our data (I\u0026rsquo;m selecting only the rawFeatures and idfFeatures columns to fit in the screen):\nNow we have a new column which contains the inverse document frequency features. These are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences.\nCONCLUSION Preprocessing is indeed a tough challenge where you will have to know what kinds of data you might get and what kinds of processing you want to apply on your data. If not done properly, your machine learning models might not be of much use.\n","date":"2020-06-06T08:02:00Z","image":"https://blog.sparker0i.me/spark-ml-data-preprocessing/661c082397510b44fe969e47_hu_2fe79d72bf6c23ab.png","permalink":"https://blog.sparker0i.me/spark-ml-data-preprocessing/","title":"Cool Spark ML - Part 2: Preprocessing of Data"},{"content":"\u0026ldquo;Going Viral\u0026rdquo; and \u0026ldquo;Trending\u0026rdquo; are probably two words which are a big deal in today\u0026rsquo;s world and use networking platforms to connect and grow.\nIt\u0026rsquo;s an even bigger deal for a software developer like me who just started a career almost 1.5 years back. I would have never thought that one of my posts on LinkedIn would ever be trending.\nBut it has happened, and here I am putting out my story.\nHow did I perform on LinkedIn earlier? Before May 2020 started, I used to have around 200 connections. Whenever I used to share something on my LinkedIn profile, it would mostly go unnoticed.\nViews would never cross the double-digit mark. Likes were usually single digit numbers, if not 0. 0 Comments. Period.\nWhereas, the people I was following, had a lot of likes and comments on their posts. When I had a look at their posts, their content was very well structured. Their posts were written as if they were telling a story. Then I realized they used to like and comment on these posts because they were able to relate themselves to it.\nLater, when I had a look at my posts on LinkedIn, they never had content. They used to be mostly reshares of someone else\u0026rsquo;s content. You can see a few examples below, along with the view count of that post:\nWhat triggered a change? Around March 2020, I saw an amazing article on a website which gave a glimpse on the new features coming to Spark 3.0 (You can find that post here). This time around, while sharing this post on LinkedIn, I decided to put in my thoughts on what I felt about the release of Spark 3. Find this post below:\nThis was the first time the View count on any of my posts stepped into the triple-figure mark. This was also the first time the Reaction count on my posts touched 5 (the previous best at this point was 3, as shown in the screenshot above).\nBecause there\u0026rsquo;s always a first time for everything ;)\nThis was the time I realized that if I put my thoughts on what I felt about something, I would surely gain more views and likes. I definitely wanted to grow more on LinkedIn and realized that I had to put my content in a better way that people don\u0026rsquo;t find crowded. (That embedded post above had only 1 paragraph which felt cluttered).\nThe Guru Around a month later, I saw one of my connections, Vaibhav Sisinty commenting on people\u0026rsquo;s posts about his 5-Day LinkedIn workshop. That\u0026rsquo;s not just once. I saw his comments on multiple people\u0026rsquo;s posts.\nWhile reading the posts, I actually felt that this isn\u0026rsquo;t promotion going on. People are telling their experiences out of the workshop. People were telling the steps they executed and were also showing the results they achieved.\nThe cost of the workshop was a measly Rs. 500. It isn\u0026rsquo;t a huge amount where we would have to empty our pockets to grow. I actually contemplated on whether to join his workshop or not a lot. I was in a dilemma because I was just another (inexperienced) software developer on LinkedIn, amongst 1000s of others. I also had other thoughts on my mind - whether this will workshop will help me as a software developer or not.\nThen I took a leap of faith\u0026hellip; and joined his 5-Day LinkedIn Workshop.\nThe workshop I was a part of Batch 3 of his workshop, which ran from 5th-11th May. In short, the content of the workshop was divided into a few parts, each taken over a period of 5 days (actually 6, because he gave one extra day off to complete one of the tasks):\nUnderstanding Target Audience Optimizing LinkedIn profile Connect with the target audience Create good content The content for each of those points in the workshop was well defined. He had recorded sessions for each of those 5 days, and at 9PM he used to have a Live Q\u0026amp;A session of that day\u0026rsquo;s topic. Despite all of the above, execution is key.\n\u0026ldquo;No Execution, No Results\u0026rdquo;.\nThis is one of the golden words Vaibhav used to frequently tell inside his videos and the Facebook and WhatsApp groups {Yes, he used to engage with us on WhatsApp as well ;) }. He would also send reminder emails (I guess he sent around 50-70 such mails) to me (and I guess each participant).\nThe content boom Ever since the workshop, I posted few contents, whose views and Reaction count were significantly higher than any of my previous posts. Have a look at the two screenshots below:\nViews and Reaction count on my posts after executing the contents of the workshop\nAt least 3x the usual reaction counts and 5-8x views of my posts. I was definitely improving in my LinkedIn game.\nProfile views and search appearances were up as well:\nProfile views after executing contents of the workshop\nThen came the blockbuster. On May 15th 2020 at around 10:15pm, I put up a post on LinkedIn regarding native GPU support for Spark. This was a recent news, and Spark was something I\u0026rsquo;m working on at my workplace. Posting my thoughts on that news definitely made sense. Have a look at that post below:\nThis was the post where I\u0026rsquo;ve got the best results on inside LinkedIn. Look at the counts below. These numbers are insane for a software developer who had just started out his career:\nView and Reaction count for the above post\nOh yes, and the \u0026ldquo;Trending on LinkedIn\u0026rdquo; happened as well, 2.5 days after putting up that post:\nThe Trending part\nCouldn\u0026rsquo;t expect more for myself.\nConclusion While I did start to see great numbers after executing contents in the workshop, complacency isn\u0026rsquo;t allowed. The workshop contents are supposed to be executed for long (unless LinkedIn detects you are a Super User and stops you from executing for the rest of the day).\nVaibhav\u0026rsquo;s workshop is a great place to get started whoever you are - from an entry level Software Developer, to a Digital Marketer, to someone finding the next job - this workshop is for everyone who wants to improve their LinkedIn profile.\nDo visit Vaibhav\u0026rsquo;s profile on LinkedIn.\nUntil another post, ciao.\n(PS. Not a promotional post. These are definitely my experiences out of this workshop. You can see the results for yourselves)\n","date":"2020-05-18T19:02:45Z","image":"https://blog.sparker0i.me/journey-10k-post-views-linkedin/661c08335ae500253ce7927e_hu_d62e824fe849482.png","permalink":"https://blog.sparker0i.me/journey-10k-post-views-linkedin/","title":"My journey to 10k post views on LinkedIn"},{"content":"You can soon run your Apache Spark programs natively on your GPU. This became possible thanks to collaboration between Nvidia and Databricks. At the GPU Technology Conference, both the companies have presented a solution that brings GPU Acceleration to Spark 3.0 without major code changes.\nHow things were before? GPU based solutions have existed for Spark for a long time, so what has changed?\nSuch GPU integrations into Spark were provided by either third party libraries in Java/Scala, or you had to depend on Cloud Providers which would provide such an infrastructure to run Spark on GPU. Also, programs would usually be restricted to applications based on Spark ML, thus they generally couldn\u0026rsquo;t be applied to other Big Data uses on Scale.\nWhen it comes to Spark/Python, you had to use custom tools like Horovod, which would also end up using popular Python based libraries like Numpy and Tensorflow. Thus, this approach severely limits the performance of the Spark Programs due to the nature of Python, where programs are dynamically interpreted.\nDon\u0026rsquo;t get me wrong, Python has its very own unique use-cases which Scala doesn\u0026rsquo;t provide (yet), but because Spark was built to do Big Data operations effectively, Python severely restricts the performance.\nWhat happened now? With the release of Spark 3.0, native GPU based acceleration will be provided within Spark. This acceleration is based on the open source RAPIDS suite of software libraries, Nvidia built on CUDA-X AI. This will allow developers to run Spark code without any modifications on GPUs - thereby alleviating load off the CPU.\nThis also benefits Spark SQL and DataFrame operations, thereby making the GPU acceleration benefits available for non-Machine Learning workloads as well. This will also bring capabilities where we don\u0026rsquo;t have to provision a dedicated Spark Cluster for AI and ML based jobs.\nIn an advanced briefing for members of the press, NVidia CEO Jensen Huang explained that users of Spark clusters on Azure Machine Learning or Amazon SageMaker can benefit from the GPU acceleration as well. This means that the infrastructure is already in place, it is now upon other cloud providers to provide the necessary infrastructure, and upon developers to adopt and build their workloads to the new changes.\nAdobe + Spark GPU Acceleration Adobe and Nvidia had signed a deal in 2018 where they will utilize Nvidia\u0026rsquo;s AI capabilities for their solutions. Building upon this deal, Adobe has been an early adopter for this new GPU Acceleration on Spark, and they have shown a 7x improvement in performance of their workloads, while saving up to 90% of the costs.\nThese are serious numbers. Imagine, if a company as huge as Adobe is able to bring down costs while improving performance, other companies too can follow suit and we could see Profits and Performance for everyone. Period.\nConclusion Imagine how game changing this can prove to be for the Big Data community overall. No longer will we have to wait for operations to complete when we can utilize the GPU, we have on our local Gaming PCs and laptops. We will also be able to utilize GPU servers on Cloud for Spark without doing major changes.\nThis can also encourage many people to start using Scala for AI and Machine Learning instead of Python. While I do realize that there are no major visualization libraries supporting Spark available in Scala, an encouragement to do machine learning with Spark shall bring more enthusiasm for Scala, due to the disadvantages I mentioned for Python above. This in turn will lead to a growth in the Scala community, which will further result in availability of more and more libraries.\nFor now, there is a Scala visualization library that supports Spark, in active development, which when released to MVN Repository could be a game changer. Head over to SwiftViz2\u0026rsquo;s GitHub repo for more info. You can place safe bets on this one :)\nIn short, this is a win-win situation for everyone involved in this ecosystem.\nUntil another blog post, Ciao.\nSOURCES ZdNet, Nvidia Newsroom\n","date":"2020-05-16T18:01:34Z","image":"https://blog.sparker0i.me/spark-3-native-gpu-integration/661c08375610fb416dbc1887_hu_daf8a55eb564e7bc.png","permalink":"https://blog.sparker0i.me/spark-3-native-gpu-integration/","title":"Spark 3.0 adds native GPU integration: Why that matters?"},{"content":"In the below code, I have written a recursive function that multiplies all the natural numbers up to the number passed as a parameter to the function. As you might have guessed, this is nothing but computing the factorial of a particular number.\n1 2 3 4 5 6 def recursiveProd(x: Int): BigInt = { if (x \u0026lt;= 1) return 1 else return x * recursiveProd(x-1) } Recursive Factorial Program\nLet us see how this function is being executed as a whole assuming we executed recursiveProd(5):\n1 2 3 4 5 6 7 recursiveProd(5) 5 * recursiveProd(4) (4 * recursiveProd(3)) (3 * recursiveProd(2)) (2 * recursiveProd(1)) 1 120 From above, each recursive call has to be completed first before the actual work of calculating the product begins. Each recursive call saves the current state, and proceeds to call the next recursive function. This happens repeatedly until the base case is reached. In between, you might also encounter the Stack Overflow error.\nSo, in each step you execute 2 steps, retrieve the current value and the value from the next stage (as a recursive call), and then multiply them. Subsequent recursive calls will do the same. If you can visualize this correctly, you will notice this recursive call was completed in 14 computations (4 multiplications, 5 recursive calls, 5 returning values), with computations happening in each step.\nTail Recursion Now let’s consider Tail Recursion. In Tail Recursion, all the processing related to the recursive function must finish before the recursive call takes place. This means that if a function is tail-recursive, the last action is a call to itself.\n1 2 3 4 5 6 def tailRecursiveProd(x: Int, currentTotal: BigInt): BigInt = { if (x \u0026lt;= 1) return currentTotal else return tailRecursiveProd(x - 1, currentTotal * x) } In this scenario, despite there being a multiplication operation, it happens when the argument is passed to the next recursive call. In short, we send the current state of the recursive call to the next state, and the same process will be repeated until the base case is reached. Let us see how this is executed:\n1 2 3 4 5 6 recursiveProd(5,1) recursiveProd(4,5) recursiveProd(3,20) recursiveProd(2,60) recursiveProd(1,120) 120 In this way, we can save up additional stack memory which would\u0026rsquo;ve otherwise be wasted to compute the multiplications at every return step. Thus, this implementation only takes 10 computations (5 recursive calls, 5 returning values). This is equivalent of you using a loop to process the factorial.\nThus, you should always try and convert your recursive function into a tail recursive function wherever possible.\nTail Recursion in Scala One good thing about Scala is that it automatically recognizes two types of tail-recursive methods automatically and optimizes them. These types are:\nMethods within an object Methods defined as final Sadly, if you write a non-final tail-recursive function inside a class, or even a case class, it will not be automatically optimized by the Scala Compiler because a class can be extended and these methods can be overriden. Consider my code given below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 object Bm { def nTailRecursion(n: Int): Int = { if (n == 0) 1 else nTailRecursion(n - 1) } } case class Bm() { def tailRecursion(n: Int): Int = { if (n == 0) 1 else tailRecursion(n - 1) } final def tailsRecursion(n: Int): Int = { if (n == 0) 1 else tailsRecursion(n - 1) } } You can see that all these functions are doing the same task. Now:\nStart a Scala REPL (Install Scala on your machine, then type scala on your command line/terminal and press Enter) Type :paste and press Enter Paste the code snippet above Press Ctrl-D to exit the paste mode Then, try running Bm.nTailRecursion(60000) and Bm().tailsRecursion(60000). I\u0026rsquo;ve tried that on my current laptop with an Intel i7-8750H processor and 16GB RAM, and both of them worked fine. Now, when you try running Bm().tailRecursion(60000), you see a familiar java.lang.StackOverflowError which usually occurs with recursive function:\nSure, you could play around with the JVM memory limits and possibly execute this function properly. You must always remember that memory is an intensive resource, and non-availability of memory might crash other programs, as well as your current program.\nFortunately, Scala provides the @tailrec annotation to denote that a method is actually tail-recursive. First you will have to import scala.annotation.tailrec and place that annotation before the function you want to mark as tail-recursive. Place this annotation before tailRecursion() inside the case class and now copy-paste inside the REPL and try again. This time it won\u0026rsquo;t throw the dreaded java.lang.StackOverflowError Exception.\nConvert a recursive function to a tail-recursive function In some cases, you might want to retain the original method\u0026rsquo;s signature (eg. Factorial). This can be done using the following steps:\nCreate a second function Within the recursiveProd as defined in the first code piece above, we now define another method, cumulativeRecursion with two parameters: n, our number and res, the result of recursion. We retain the algorithm of the first method as is. At this point our new method looks like:\n1 2 3 4 5 6 def recursiveProd(n: Int): Int = { def cumulativeRecursion(n: Int, res: Int): Int = { if (n \u0026lt;= 1) 1 else n * recursiveProd(n - 1) } } Modify the second method\u0026rsquo;s algorithm We will now utilize the accumulator we\u0026rsquo;ve just created, res and modify the function such that the base case returns the accumulated value and the other case recursively calls the new method again:\n1 2 3 4 5 6 def recursiveProd(n: Int): Int = { def cumulativeRecursion(n: Int, res: Int): Int = { if (n \u0026lt;= 1) res else cumulativeRecursion(n - 1, res * n) } } Annotate the second method and call the new method We will now annotate our new method with @tailrec as shown earlier and we will now call this method from our original method:\n1 2 3 4 5 6 7 def recursiveProd(n: Int): Int = { @tailrec def cumulativeRecursion(n: Int, res: Int): Int = { if (n \u0026lt;= 1) res else cumulativeRecursion(n - 1, res * n) } cumulativeRecursion(n, 1) } Hence, you retain your method\u0026rsquo;s original signature, as well as converted it into a tail-recursive call (Though this will add 1 extra stack call to the new function).\nCONCLUSION In this post, I have:\nDefined Tail Recursion Introduced @tailrec annotation Shown a formula to convert a recursive function into a tail-recursive one. Hope you have enjoyed this post. Do follow my profiles on LinkedIn, GitHub and Twitter.\nCiao, until the next post.\nReference: Tail Recursive Algorithms\n","date":"2020-05-07T19:45:00Z","image":"https://blog.sparker0i.me/tail-recursion-scala-why-how-to/661c083cdbf837e5981954ca_hu_f9fcb7a67567ebfa.png","permalink":"https://blog.sparker0i.me/tail-recursion-scala-why-how-to/","title":"Tail Recursion: Why and How-to Use in Scala"},{"content":"Note: This article is the first of the Series: Cool Spark ML. Other parts are coming soon.\nI had taken up a few machine learning courses in my college throughout 2018. Most of the problems there were solved using Python and the necessary libraries - NumPy, Pandas, Scikit-Learn and Matplotlib. With my daily work at IBM now requiring me to use Scala and Spark, I decided to use my free time during the lockdown to try out Spark ML.\nNote: All the codes in the Cool Spark ML Series will be available on my GitHub repo\nIntro to Spark ML As the name suggests, Spark ML is the Machine Learning library consisting of common Machine learning algorithms - classification, regression, clustering etc.\nWhy Spark ML? Pandas - a Python library - won’t work every time. It is a single machine tool, so it\u0026rsquo;s constrained by the machine\u0026rsquo;s limits. Moreover, pandas doesn’t have any parallelism built in, which means it uses only one CPU core. You may hit a dead-end on datasets of the size of a few gigabytes. Pandas won\u0026rsquo;t help if you want to work on very big datasets.\nWe are now in the Big Data era, where gigabytes of data are generated every few seconds. Such datasets will require powerful systems to run even the basic machine learning algorithms. The cost of getting such a powerful system will be huge, as well as the costs to scale them up. With distributed computers, such calculations can be sent to multiple low-end machines, which prevents the cost of getting a single high-end machine.\nThis is where Spark kicks in. Spark has the concept of DataFrame (now deprecated in favor of Datasets), which behaves very similar to how a Pandas DataFrame would do, including having very similar APIs too. The advantage of using Spark DataFrame is that it was designed from ground-up to support Big Data. Spark can also distribute such DataFrames across multiple machines and collect the calculated results.\nKNN: K-Nearest Neighbors The process in KNN is pretty simple. You load your entire dataset first, each of which will have input columns and one output column. This is then split into a training set and a testing set. You then use your training set to train your model, and then use the testing set to predict the output column value by testing it against the model. You then compare the actual and the predicted target values and calculate the accuracy of your model.\nProblem Definition We are going to train a model to predict the famous Iris dataset. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\nIt is a multiclass classification problem. The number of observations for each class is the same. The dataset is small in size with only 150 rows with 4 input variables and 1 output variable.\nThe 4 features are described as follows:\nSepal-Length, in cm Sepal-Width, in cm Petal-Length, in cm Petal-Width, in cm Prerequisites Create a Scala project in IntelliJ IDEA based on SBT Select Scala version 2.11.12 Include spark-core, spark-sql and spark-ml 2.4.5 as library dependencies in your build.sbt KNN Steps In this blog post, I will be developing KNN algorithm from scratch. The process to perform KNN can be broken down into 3 easy steps:\nCalculate Euclidean Distance Get Nearest Neighbors Make Predictions Step 1: Calculate Euclidean Distance The first step will be to calculate the distance between two rows in a Dataset. Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line.\nEuclidean Distance is calculated as the square root of the sum of the squared differences between the two vectors, as given in the image below:\nWhere x1 is the first row of data, x2 is the second row of data, and i is a specific index for a column as we sum across all columns. Smaller the value, more similar will be the two rows.\nSince we will be reading our data and transforming it using Spark, to compute distances between two Rows in a DataFrame, we write the function below in Scala:\n1 2 3 4 5 6 7 def computeEuclideanDistance(row1: Row, row2: Row): Double = { var distance = 0.0 for (i \u0026lt;- 0 until row1.length - 1) { distance += math.pow(row1.getDouble(i) - row2.getDouble(i), 2) } math.sqrt(distance) } You can see that the function assumes that the last column in each row is an output value which is ignored from the distance calculation.\nStep 2: Get Nearest Neighbors Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our distance measure. To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above.\nWe can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance, and then retrieve the neighbors. The below function does this job in Scala:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def getNeighbours(trainSet: Array[Row], testRow: Row, k: Int): List[Row] = { var distances = mutable.MutableList[(Row, Double)]() trainSet.foreach{trainRow =\u0026gt; val dist = computeEuclideanDistance(trainRow, testRow) val x = (trainRow, dist) distances += x } distances = distances.sortBy(_._2) var neighbours = mutable.MutableList[Row]() for (i \u0026lt;- 1 to k) { neighbours += distances(i)._1 } neighbours.toList } Step 3: Make Predictions The most similar neighbors collected from the training dataset can be used to make predictions. In the case of classification, we can return the most represented output value (Class) among the neighbors.\nWe would first map the class values to the number of times it appears among the neighbors, then sort the counts in descending order and get the most appeared class value. The below function does exactly that in Scala:\n1 2 3 4 5 6 7 8 9 10 def predictClassification(trainSet: Array[Row], testRow: Row, k: Int): String = { val neighbours = getNeighbours(trainSet, testRow, k) val outputValues = for (row \u0026lt;- neighbours) yield row.getString(trainSet(0).length - 1) outputValues.groupBy(identity) .mapValues(_.size) .toSeq .sortWith(_._2 \u0026gt; _._2) .head._1 } Apply the above concepts to Iris Dataset We will now apply the concepts above to perform KNN on the Iris Dataset.\nFirst, we have to load the dataset into the program. This is done using the readCsv function I\u0026rsquo;ve written below:\n1 2 3 4 5 6 7 8 def readCsv(fileName: String, header: Boolean): DataFrame = { spark.read .format(\u0026#34;csv\u0026#34;) .option(\u0026#34;header\u0026#34;, header) .option(\u0026#34;inferSchema\u0026#34;, header) .load(fileName) .repartition($\u0026#34;Class\u0026#34;) } We also have to normalize the data we have. This is because KNN is based on distance between records. Unless data is normalized distance will be incorrectly calculated, because different attributes will not contribute to the distance in a uniform way. Attributes having a larger value range will have an unduly large influence on the distance, because they make greater contribution to the distance. If the dataset requires that some columns be given a greater preference over others, then normalization isn\u0026rsquo;t recommended, but this is not true in the case of the Iris dataset.\nWe use the Z Score Normalization technique. With this, we subtract the mean of the respective column from each cell, and divide that with the standard deviation of that column. This article describes Data Normalization in good detail.\nThe following function does our job:\n1 2 3 4 5 6 7 8 9 10 def normalizeData(): Unit = { df.columns.filterNot(e =\u0026gt; e == \u0026#34;Class\u0026#34;).foreach{col =\u0026gt; val (mean_col, stddev_col) = df.select(mean(col), stddev(col)) .as[(Double, Double)] .first() df = df.withColumn(s\u0026#34;$col.norm\u0026#34;, ($\u0026#34;$col\u0026#34; - mean_col) / stddev_col) .drop(col) .withColumnRenamed(s\u0026#34;$col.norm\u0026#34;, col) } } As you can see above, we are filtering out the class value, because we will not be using this value to compute the distance. There\u0026rsquo;s one problem with our approach though, our KNN functions written above assume that the class value will be the last column. In the way we\u0026rsquo;ve normalized the data, we are dropping the original column, and adding the normalized column in place. This will push the Class column to the beginning. So, I\u0026rsquo;ve written another function which will move the column back to where it should actually be:\n1 2 3 4 def moveClassToEnd(): Unit = { val cols = df.columns.filterNot(_ == \u0026#34;Class\u0026#34;) ++ Array(\u0026#34;Class\u0026#34;) df = df.select(cols.head, cols.tail: _*) } We will evaluate our algorithm using K-fold cross-validation with 5 folds. This means that we will have 150/5 = 30 rows per fold. We will use helper functions evaluateAlgorithm() and accuracyMetric() to evaluate the algorithm for cross-validation and calculate the accuracy of our predictions respectively.\nSince Spark does not allow any of its operations inside a Spark transformation, we will have to perform a collect() on the Train set and Test set DataFrames every time before passing it to any function. A sample run with k = 3 produces the following output:\nLet\u0026rsquo;s go one step further and run our program over different values of k. I\u0026rsquo;m running it for k from 1 to 10, and here are some results (this may not be the same everytime):\nKNN accuracy for a variety of k values\nYou can find the entire code below:\nCONCLUSION While Spark ideally shouldn\u0026rsquo;t be used smaller datasets like this, you could apply the same thought process and transform this code to use for some larger datasets, and there you will see the magic of Spark over Pandas.\nInspired heavily from this great article.\n","date":"2020-04-19T05:41:03Z","image":"https://blog.sparker0i.me/spark-machine-learning-knn/661c0843dbf837e5981954cc_hu_c97642255bfdd5cd.png","permalink":"https://blog.sparker0i.me/spark-machine-learning-knn/","title":"Cool Spark ML: K Nearest Neighbors"},{"content":"Background A Spark DataFrame has a better advantage over a Pandas DataFrame when it comes to the ability to scale and process it. I\u0026rsquo;m writing more on this in another blog post which will arrive shortly after this one.\nFunctionally, both Spark and Pandas have an almost same set of functionalities, and their APIs are not so different either. There\u0026rsquo;s one function which is used extensively in the data science community with Pandas - shape(). This function returns you the return the row and column count coupled inside a Tuple. Sadly, this functionality isn\u0026rsquo;t available with Spark DataFrame (and won\u0026rsquo;t come either).\nImplicit classes in Scala Fortunately, we have Implicit classes in Scala for our rescue. Implicit classes enable us to add some new functionality on top of an existing class\u0026rsquo; functionalities. To know more about Implicit Classes, you can read this article for diving deep.\nFirst, we need to define a new implicit class with the method we want to add. In this case, I want to add the shape() function on top of the Spark DataFrame class.\n1 2 3 implicit class DataFramePlus(df: DataFrame) { def shape(): (Long, Int) = (df.count(), df.columns.length) } Then all you need to do is print the shape of the DataFrame:\n1 2 df = spark.read.format(\u0026#34;\u0026lt;something\u0026gt;\u0026#34;).load(\u0026#34;\u0026lt;Filename\u0026gt;\u0026#34;) println(df.shape()) This solved a major pain point for me without having to extend an existing class.\nBest Practice While writing these codes inside the Scala REPL (Scala/Spark Shell on Terminal) might seem a little easier to implement, openly exposing your code for everyone to use isn\u0026rsquo;t a great idea.\nInstead, you could implement the implicit class in a package object like this:\n1 2 3 4 5 6 7 8 9 package me.sparker0i import org.apache.spark.sql.DataFrame package object machinelearning { implicit class DataFramePlus(df: DataFrame) { def shape(): (Long, Int) = (df.count(), df.columns.length) } } Then you\u0026rsquo;ll need to add the proper import statement in your class, after which you can use the shape method with any DataFrame:\n1 2 3 4 5 6 7 8 9 10 package me.sparker0i.machinelearning.regression import org.apache.spark.sql.DataFrame import me.sparker0i.machinelearning._ class LinearRegression { def function(df: DataFrame): Unit = { println(df.shape()) } } CONCLUSION With this approach of using implicit classes in Scala, we no longer have to extend any existing class just to add additional functionality to it. You define the behavior you want, and then add that behavior to existing class instances after adding the proper import statements.\nInspired heavily from Alvin Alexander\u0026rsquo;s article\n","date":"2020-04-10T18:09:42Z","image":"https://blog.sparker0i.me/scala-add-new-functions-to-existing-class/661c08479bb70e9dac2bfee3_hu_2ccf8bbd4d2bd693.png","permalink":"https://blog.sparker0i.me/scala-add-new-functions-to-existing-class/","title":"Add new functions to existing classes the Scala way"},{"content":"Everyone reading my blog knows that I am a huge RCB fan (Shame on you if you don\u0026rsquo;t). I\u0026rsquo;ve been showing my love, affection and madness for this franchise across all social media platforms.\nI got invited by the RCB management - along with a few more mad fans like me - to cheer for RCB whenever we bought a player during the IPL auction. Imagine how it feels when you get recognized for showing madness for a team by them. It just happened to me, and this will go down as one of the best things that has ever happened in my life.\nWe were to do a livestream on Instagram, Facebook and the official RCB app on the day. The host of the day was - RCB\u0026rsquo;s 12th Man - Navneeth Krishna. I even got lucky as I got to talk with him, and took a selfie as well:\nTo start it off, we had a good brunch - dosa and chutney - to warm up before we cheered like there was no tomorrow. Then we were given some special tips on how to reply when a question was asked during the livestream.\nThe first hour, we bought two big players - Aaron Finch and Chris Morris, both would help in solving two critical problems RCB has had to face over the years - Solid Opener (so that Virat and ABD can be destructive in the middle order) and a Death Bowler. While all of us there felt that they were a little too pricey, but still all of us believed that these 2 players will be the key for RCB to win their first trophy in 2020.\nDuring the first break, I also got to meet another person who is special for RCB. Guess who: Sugumar Kumar, RCB\u0026rsquo;s biggest fan ever.\nWhen the auction ended, I also got a match ball signed by the famous spinner - Yuzvendra Chahal (PS. He wasn\u0026rsquo;t there but I got this from RCB). This will be among the prized possessions in my life:\nSigned ball by Yuzvendra Chahal\nIt was my dream come true when I met these special persons for RCB. Never did I expect in life that I will get recognized for my madness for this team. Now here\u0026rsquo;s to hoping I get to show more madness like this at the M Chinnaswamy Stadium, when the IPL actually starts.\nBefore I sign off, here are a few other RCB fans I met during the day:\nView this post on Instagram Lucky indeed.\n","date":"2019-12-22T09:55:23Z","image":"https://blog.sparker0i.me/ipl-2020-auction-experience-with-rcb/rcb_hu_77d24efce5a41b31.jpg","permalink":"https://blog.sparker0i.me/ipl-2020-auction-experience-with-rcb/","title":"IPL 2020 auction experience with RCB"},{"content":"I’ve never liked to make changes to my laptop by providing my admin credentials to install various tools, like MySQL because I love doing things at a user level. Programs like VSCode and Atom are good examples of that when all you do is drag the application icon to the Applications folder. Homebrew is also a good example to install command line applications at a user level.\nIf you were to install using the official installer, you’d have to give administrator rights. You will also have to make changes to your bash profile to access mysql in terminal. All these problems can be alleviated by installing a MySQL container inside Docker.\nINSTALL DOCKER COMMUNITY EDITION Download Docker Desktop for Mac by visiting this link. Then you need to install Docker on your Mac by following the installation steps. It is recommended not to change any defaults if prompted.\nOnce you are done with that, we will now proceed with the installation of MySQL container inside Docker. You can either do it inside a terminal, or use a tool like Kitematic using which you can manage multiple containers in your system once you create them.\nFire up a terminal, and write this command\n1 docker run --name=mysql -d -p 3306:3306 -e MYSQL_USER=developer -e MYSQL_PASSWORD=mydbpwd -e MYSQL_DATABASE=mydb mysql/mysql-server Here we will use the mysql/mysql-server image, and our Username, password and Database are developer, mydbpwd and mydb respectively. Then do the port mapping between the container and the host. We bind Container’s port 3306 to the Mac’s port 3306.\nIf everything goes fine, you should see a combination of alphabets and numbers as an output. This is the container ID. Type this command:\n1 docker exec -it mysql bash -c \u0026#34;mysql -u developer -p\u0026#34; Then enter the password you entered while creating the container (In this case mydbpwd). Then you will have an instance up and running inside Docker.\nCHECK MYSQL CONNECTIVITY INSIDE MYSQL WORKBENCH First up install MySQL Workbench, either using the official installer or using the brew command brew cask install mysqlworkbench. Once you open up the MySQLWorkbench, click on the add connection button, then enter as following:\nCreating a new connection inside MySQL Workbench\nThen click the Test Connection button. If everything goes alright, you will get this popup:\nConnection established with MySQL Container in Docker\nCONCLUSION Installing MySQL in Docker on your PC is a safer approach to installing MySQL than providing Admin credentials to install using the official installer. If anything goes wrong, all you have to do is delete the container and create a new one in Docker.\nREFERENCES Run MySQL in a Docker Container – Medium\n","date":"2019-03-28T19:25:50Z","image":"https://blog.sparker0i.me/mysql-installation-inside-docker-on-mac/661c084eec6af0386c750e4a_hu_63e8ec78a2525c3.png","permalink":"https://blog.sparker0i.me/mysql-installation-inside-docker-on-mac/","title":"MySQL installation inside Docker on Mac"},{"content":"Hey all, today I will write about my visit to a wood cutting factory situated near my home. This visit was back in December 2017, so I’m sorry for such a long delay because I got busy with academics and other important tasks.\nCUTTING THE WOOD IN FACTORY The process starts with cutting the wood logs in the factory. This factory uses the logs of the Neem tree to obtain the final product because they are of the best quality.\nThese wood logs are first passed into a machine, from where you will get wood sheets as output. Here is a video that illustrates this better. The image below the video will show you the output of that machine.\nProcessing of a wood log into a wood sheet\nWood sheets from the machine\nThe next step is to send these wood sheets to cutting, which gives us the wood sheets of required dimensions. Therefore we obtain a combination of good wood and bad wood sheets. The bad wood is sent to a burning plant in Tamilnadu to generate electricity.\nBad wood obtained after cutting\nThe good wood goes on to become plywood. Here’s the good wood.\nGood wood obtained after cutting\nSimilarly, there’s also another process of cutting the bad wood into a sheet of dimensions. The video below helps describe this better\nDRYING Once we get the sheets of good wood, these sheets dry under the sun. Usually, this finishes in around 4-5 hours, but it may vary. Here is a picture illustrating drying of good sheets.\nGood sheets kept for drying\nOnly 1 lakh sq. Km of sheets can dry at a time due to space restrictions. With a drying machine, drying wood sheets of 1 lakh + 4 lakh km sq = 5 lakh sq. km is not an issue.\nThese sheets sell in huge numbers to plywood factories, from where we obtain plywood. Each sheet will have crests and troughs. The reason is that the sheets dry under the sun. With a drying machine, this would never happen, because we get a straight sheet of wood.\nPLYWOOD This is a plywood made from the sheets supplied by this factory.\nPlywood made from the wood sheets of this factory\nLikewise, you can see stripes between the plywood on its lateral side. Many dried wood sheets are glued together to create this, apart from the other factory processes.\nLateral side of the plywood\nA PLYWOOD MACHINE PERHAPS? Ever since this factory trip took place, a drying machine was installed here. Also, there are plans on having the plywood machine as well.\nMoreover, adding a plywood making machine alongside will give four-fold benefits:\nGood quality of dried sheets Good quality of plywood Increased employment, due to increasing machinery and scalability requirements Increased production, giving profit to owners SIGNING-OFF Most importantly, I can’t sign off before thanking those people, because of whom I could have a hands-on tour of the factory. Here’s a pic of the factory owners (Pro tip: The one on the right is @theamritanair‘s uncle)\nThe factory owners\nUntil another post, ciao.\n","date":"2018-08-26T02:24:58Z","image":"https://blog.sparker0i.me/factory-visit-wood-cutting-factory/neem2sheet_hu_b8c9e7dba5e4b088.jpg","permalink":"https://blog.sparker0i.me/factory-visit-wood-cutting-factory/","title":"Factory Visit - Episode of a visit to a Wood Cutting Factory"}]